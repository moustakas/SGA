#!/usr/bin/env python
"""
Parse Tractor fitting logs in parallel.

Usage:
    python parse_tractor_logs_parallel.py <pattern> --csv results.csv --nproc 8
    python parse_tractor_logs_parallel.py '/path/to/logs/*-coadds.log' --csv results.csv

Examples:
    # Process all logs in a directory
    python parse_tractor_logs_parallel.py '/pscratch/.../dr11-south/*/*/*-coadds.log' --csv tractor_stats.csv

    # Process from file list
    find /pscratch/.../dr11-south -name '*-coadds.log' | python parse_tractor_logs_parallel.py --from-stdin --csv results.csv

"""

import re
import sys
import csv
import glob as globmodule
from pathlib import Path
from multiprocessing import Pool


def count_attempts(logfile):
    """
    Count number of fitting attempts by finding rotated log files.

    Pattern: base-coadds.log with rotations as base-coadds_log.0, base-coadds_log.1, etc.
    The .0 file is the first/original attempt.

    Returns
    -------
    int
        Number of attempts (1 if only base log exists, 2+ if rotated logs exist)

    """
    logfile = Path(logfile)
    logstr = str(logfile)

    # Handle rotation pattern: -coadds.log â†’ -coadds_log.*
    if logstr.endswith('-coadds.log'):
        base_pattern = logstr.replace('-coadds.log', '-coadds_log.*')
    elif logstr.endswith('-coadds_log'):
        base_pattern = logstr + '.*'
    else:
        base_pattern = logstr + '.*'

    # Find all rotated versions
    rotated = globmodule.glob(base_pattern)

    # Total attempts = main log + rotated logs
    return 1 + len(rotated)


def parse_tractor_log(logfile):
    """
    Extract key metrics from a Tractor log file.

    If some fields are missing from the main log (e.g., due to checkpoint recovery),
    falls back to reading the first rotated log (-coadds_log.0).

    Parameters
    ----------
    logfile : str or Path
        Path to log file (or base path without log file)

    Returns
    -------
    dict
        Parsed metrics: group_name, width, nccd, nblob, nsources, runtime, nattempts, ncheckpoint

    """
    logfile = Path(logfile)

    result = {
        'group_name': None,
        'width': None,
        'nccd': None,
        'nblob': None,
        'nsources': None,
        'runtime': None,
        'nattempts': None,
        'ncheckpoint': None
    }

    # Handle case where logfile doesn't exist
    if not logfile.exists():
        match = re.search(r'/(\d+[pm]\d+)/', str(logfile))
        if match:
            result['group_name'] = match.group(1)
        result['width'] = 0
        result['nccd'] = 0
        result['nblob'] = 0
        result['nsources'] = 0
        result['runtime'] = 0.0
        result['nattempts'] = 0
        result['ncheckpoint'] = 0
        return result

    # Count attempts
    result['nattempts'] = count_attempts(logfile)

    # Parse main log
    try:
        with open(logfile) as f:
            for line in f:
                if 'outdir=' in line and result['group_name'] is None:
                    m = re.search(r'--outdir=.+?/(\d+[pm]\d+)', line)
                    if m:
                        result['group_name'] = m.group(1)

                if '--width=' in line and result['width'] is None:
                    m = re.search(r'--width=(\d+)', line)
                    if m:
                        result['width'] = int(m.group(1))

                if 'Keeping' in line and 'CCDs' in line:
                    m = re.search(r'Keeping (\d+) CCDs', line)
                    if m:
                        result['nccd'] = int(m.group(1))

                if 'Keeping' in line and 'checkpointed results' in line:
                    m = re.search(r'Keeping (\d+) of \d+ checkpointed results', line)
                    if m:
                        result['ncheckpoint'] = int(m.group(1))

                if 'Sources detected:' in line:
                    m = re.search(r'Sources detected: (\d+) in (\d+) blobs', line)
                    if m:
                        result['nsources'] = int(m.group(1))
                        result['nblob'] = int(m.group(2))

                if line.startswith('Total runtime:'):
                    m = re.search(r'Total runtime: ([\d.]+)', line)
                    if m:
                        runtime_sec = float(m.group(1))
                        result['runtime'] = runtime_sec / 60.0

        # If checkpoint recovery, some fields may be missing - check rotated logs
        if result['ncheckpoint'] is not None and result['ncheckpoint'] > 0:
            # Try all rotated logs in order: _log.0, _log.1, _log.2, etc.
            for i in range(10):  # Check up to 10 rotations
                fallback_log = Path(str(logfile).replace('-coadds.log', f'-coadds_log.{i}'))
                if not fallback_log.exists():
                    break

                with open(fallback_log) as f:
                    for line in f:
                        if result['nccd'] is None and 'Keeping' in line and 'CCDs' in line:
                            m = re.search(r'Keeping (\d+) CCDs', line)
                            if m:
                                result['nccd'] = int(m.group(1))

                        if result['nsources'] is None and 'Sources detected:' in line:
                            m = re.search(r'Sources detected: (\d+) in (\d+) blobs', line)
                            if m:
                                result['nsources'] = int(m.group(1))
                                result['nblob'] = int(m.group(2))

                # Stop if we found everything
                if result['nccd'] is not None and result['nsources'] is not None:
                    break

    except Exception as e:
        print(f"Error reading {logfile}: {e}", file=sys.stderr)

    return result


def main():
    import argparse

    parser = argparse.ArgumentParser(description='Parse Tractor fitting logs in parallel')
    parser.add_argument('pattern', nargs='?', help='Glob pattern for log files')
    parser.add_argument('--from-stdin', action='store_true', help='Read file list from stdin')
    parser.add_argument('--csv', required=True, help='Output CSV file')
    parser.add_argument('--nproc', type=int, default=8, help='Number of parallel processes')
    args = parser.parse_args()

    if args.from_stdin:
        logfiles = [line.strip() for line in sys.stdin if line.strip()]
    elif args.pattern:
        logfiles = globmodule.glob(args.pattern)
    else:
        parser.error('Must provide pattern or --from-stdin')

    if not logfiles:
        print("No log files found", file=sys.stderr)
        sys.exit(1)

    print(f"Processing {len(logfiles)} log files with {args.nproc} processes...")

    with Pool(args.nproc) as pool:
        results = pool.map(parse_tractor_log, logfiles)

    with open(args.csv, 'w', newline='') as f:
        fieldnames = ['group_name', 'width', 'nccd', 'nblob', 'nsources', 'runtime', 'nattempts', 'ncheckpoint']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)

    print(f"Wrote {len(results)} results to {args.csv}")


if __name__ == '__main__':
    main()
