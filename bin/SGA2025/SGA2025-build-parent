#!/usr/bin/env python

"""Code to build the parent SGA2025 sample based on a combination of internal and external catalogs.

salloc -N 1 -C cpu -A m3592 -t 04:00:00 --qos interactive
SGA2025-shifter
source /global/homes/i/ioannis/code/SGA/bin/SGA2025/SGA2025-env

or

shifter --image legacysurvey/sga:0.3 bash
source $CFS/desicollab/users/ioannis/SGA/2025/scripts/SGA2025-shifter-env.sh

SGA2025-build-parent --build-parent-nocuts
SGA2025-build-parent --build-parent-vicuts
SGA2025-build-parent --build-parent-archive
SGA2025-build-parent --in-footprint --region=dr9-north
SGA2025-build-parent --in-footprint --region=dr11-south
SGA2025-build-parent --build-parent
SGA2025-build-parent --qa-parent

salloc -N 1 -C cpu -A m3592 -t 04:00:00 --qos interactive --image=docker:legacysurvey/sga:0.3
srun --ntasks=32 shifter --env-file=$CFS/desicollab/users/ioannis/SGA/2025/scripts/SGA2025-shifter-env.sh \
  SGA2025-build-parent --in-footprint --region=dr9-north


"""
import os, pdb
import numpy as np

#import os, sys, time, pdb
#from importlib import resources
#from collections import Counter
#import numpy.ma as ma
#import fitsio
#from astropy.table import Table, vstack
#import matplotlib.pyplot as plt
#
#from astrometry.util.starutil_numpy import arcsec_between, degrees_between
#from astrometry.libkd.spherematch import match_radec
#
#from SGA.coadds import PIXSCALE, BANDS
#from SGA.qa import qa_skypatch, multipage_skypatch
#from SGA.util import match, match_to, choose_primary, resolve_close
#from SGA.io import sga_dir, parent_version
#from SGA.parent import drop_by_prefix, remove_by_prefix, resolve_crossid_errors

from SGA.logger import log

cols = ['OBJNAME', 'OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'MORPH', 'DIAM_LIT', 'DIAM_HYPERLEDA',
        'OBJTYPE', 'RA', 'DEC', 'RA_NED', 'DEC_NED', 'RA_HYPERLEDA', 'DEC_HYPERLEDA',
        'MAG_LIT', 'Z', 'PGC', 'ROW_PARENT']


def update_lvd_properties(cat):
    """Gather geometry for all LVD sources.

       name            ra            dec      rhalf
      str22         float64        float64   float64
    ---------- ------------------ ---------- -------
     dw1341-29 205.33416666666665   -29.5675      --
         IC239             39.116     38.969      --
      NGC 1042          2.6733275  -8.433591      --
      NGC 4151         12.1757335 39.4057938      --
      NGC 4424         12.4532467  9.4204423      --
      NGC 5194         202.469625  47.195167      --
    PGC 100170          2.9477205 58.9115793      --
    PGC 166192         20.5090597 60.3540088      --
    PGC 166193         20.5255533 60.8123555      --
      UGC 7490          186.10375  70.334278      --

    """
    def modify_lvg_names(lvg_name):
        lvg_name = np.char.replace(lvg_name, ' ', '')
        lvg_name = np.char.replace(lvg_name, 'SagdSph', 'Sagittarius')
        lvg_name = np.char.replace(lvg_name, 'And ', 'Andromeda ')
        lvg_name = np.char.replace(lvg_name, 'Lac', 'Lacerta')
        lvg_name = np.char.replace(lvg_name, 'UMa', 'UrsaMajor')
        lvg_name = np.char.replace(lvg_name, 'UMin', 'UrsaMinor')
        lvg_name = np.char.replace(lvg_name, 'Hydrus1dw', 'HydrusI')
        lvg_name = np.char.replace(lvg_name, 'UGCA086', 'UGCA86')
        lvg_name = np.char.replace(lvg_name, 'Antlia2', 'AntliaII')
        lvg_name = np.char.replace(lvg_name, 'Horologium2', 'HorologiumII')
        lvg_name = np.char.replace(lvg_name, 'ColumbiaI', 'ColumbaI')
        #lvg_name = np.char.replace(lvg_name, 'Pegasus', 'PegasusdIrr')
        lvg_name[lvg_name == 'Pegasus'] = 'PegasusdIrr'
        return lvg_name


    from astropy.table import vstack, join, hstack
    from astropy.coordinates import SkyCoord
    import astropy.units as u
    from astrometry.libkd.spherematch import match_radec
    from SGA.util import match
    from SGA.io import read_lvd, version_lvd, nedfriendly_lvd

    _lvd = read_lvd()
    version = version_lvd()
    nobj = len(_lvd)

    log.info(f'Updating properties of {nobj} LVD galaxies.')

    #indx_cat = np.where(cat['ROW_LVD'] != -99)[0]
    #lvd = cat[indx_cat]

    lvd = Table()
    lvd['OBJNAME'] = _lvd['OBJNAME']
    lvd['RA'] = _lvd['RA']
    lvd['DEC'] = _lvd['DEC']
    lvd['ROW'] = _lvd['ROW']
    lvd['DIAM'] = np.zeros(nobj, 'f4') - 99.
    lvd['PA'] = np.zeros(nobj, 'f4') - 99.
    lvd['BA'] = np.zeros(nobj, 'f4') - 99.
    lvd['RADEC_REF'] = np.zeros(nobj, 'U19')
    lvd['DIAM_REF'] = np.zeros(nobj, 'U19')
    lvd['PA_REF'] = np.zeros(nobj, 'U19')
    lvd['BA_REF'] = np.zeros(nobj, 'U19')

    lvd['RADEC_REF'] = 'LVD'

    I = np.isfinite(_lvd['RHALF'])
    lvd['DIAM'][I] = np.float32(2. * 1.2 * _lvd['RHALF'][I])
    lvd['DIAM_REF'][I] = version

    I = np.isfinite(_lvd['POSITION_ANGLE'])
    lvd['PA'][I] = np.float32(_lvd['POSITION_ANGLE'][I] % 180)
    lvd['PA_REF'][I] = version

    I = np.isfinite(_lvd['ELLIPTICITY'])
    lvd['BA'][I] = np.float32(1. - _lvd['ELLIPTICITY'][I])
    lvd['BA_REF'][I] = version

    # first, add geometry from the LVGDB; see bin/analyze-lvd
    lvgfile = resources.files('SGA').joinpath('data/SGA2025/lvg_table1_2025-01-26_trim.txt')
    lvg = Table.read(lvgfile, format='ascii.fixed_width')
    log.info(f'Read the properties of {len(lvg)} galaxies from {lvgfile}')

    c_lvg = SkyCoord(ra=lvg['RA_HMS'].value, dec=lvg['DEC_DMS'].value, unit=(u.hourangle, u.deg))
    lvg['RA'] = c_lvg.ra.degree
    lvg['DEC'] = c_lvg.dec.degree
    lvg.remove_columns(['RA_HMS', 'DEC_DMS'])

    [lvg.rename_column(col, f'{col}_LVG') for col in lvg.colnames]

    m1, m2, _ = match_radec(lvd['RA'], lvd['DEC'], lvg['RA_LVG'], lvg['DEC_LVG'], 30./3600., nearest=True)
    log.info(f'Matched {len(m1)}/{len(lvd)} LVD objects based on coordinates.')
    #hstack((lvd['OBJNAME', 'RA', 'DEC'][m1], lvg['NAME_LVG', 'RA_LVG', 'DEC_LVG'][m2])).plog.info(max_lines=-1)

    lvd['DIAM_LVG'] = np.zeros(nobj, 'f4') - 99.
    lvd['BA_LVG'] = np.zeros(nobj, 'f4') - 99.
    #lvd['pa_LVG'] = np.zeros(nobj, 'f4')

    lvd['DIAM_LVG'][m1] = lvg['DIAM_LVG'][m2]
    lvd['BA_LVG'][m1] = lvg['BA_LVG'][m2]
    #lvd['PA_LVG'][m1] = lvg['PA_LVG'][m2]

    # now try matching missing matches by name
    I = np.where(lvd['DIAM_LVG'] == -99.)[0]
    lvg_name = modify_lvg_names(lvg['NAME_LVG'].value)
    m1, m2 = match(np.char.replace(lvd['OBJNAME'][I], ' ', ''), lvg_name)
    log.info(f'Matched {len(m1)}/{len(lvd)} LVD objects based on object name.')
    #hstack((lvd['OBJNAME', 'RA', 'DEC'][I][m1], lvg['NAME_LVG', 'RA_LVG', 'DEC_LVG'][m2])).plog.info(max_lines=-1)

    lvd['DIAM_LVG'][I[m1]] = lvg['DIAM_LVG'][m2]
    lvd['BA_LVG'][I[m1]] = lvg['BA_LVG'][m2]
    #lvd['pa_LVG'][m1] = lvg['pa_LVG'][m2]

    # copy missing values from the LVGDB (prefer LVGDB!)
    #log.info(f'Existing diameters for {np.sum(lvd["DIAM"] == -99.)}/{len(lvd)} objects.')
    #I = (lvd['DIAM'] == -99.) * (lvd['DIAM_LVG'] != -99.)
    I = (lvd['DIAM_LVG'] != -99.)
    log.info(f'Adding {np.sum(I)} diameters from the LVGDB.')
    lvd['DIAM'][I] = lvd['DIAM_LVG'][I]
    lvd['BA'][I] = lvd['BA_LVG'][I]
    lvd['DIAM_REF'][I] = 'LVGDB'
    lvd['BA_REF'][I] = 'LVGDB'

    # v1.0.5 is missing all data for 10 objects. Hard-code the properties for
    # now...
    I = (lvd['DIAM'] == -99.)
    if version != 'v1.0.5':
        raise ValueError('Need hard-coded properties!')

    gals = np.array([
        'IC239',
        'NGC 1042',
        'NGC 4151',
        'NGC 4424',
        'PGC 100170',
        'UGC 7490',
        'dw1341-29',
    ])
    props = [ # diam, ba, pa
        (4.57, 0.912, 125.),
        (5.47, 0.873, 33.6),
        (5.43, 0.879, 140.),
        (3.63, 0.501, 95.),
        (2.58, 0.4, 117.),
        (3.30, 0.850, 0.17),
        (0.5, 1., 0.),
        ]
    refs = [
        ('RC3', 'RC3', '2MASS'),
        ('SGA2020', 'SGA2020', 'SGA2020'),
        ('SGA2020', 'SGA2020', 'SGA2020'),
        ('RC3', 'RC3', 'RC3'),
        ('2MASS', '2MASS', '2MASS'),
        ('SGA2020', 'SGA2020', 'SGA2020'),
        ('VI', 'VI', 'VI'),
        ]
    assert(np.all(lvd[I]['OBJNAME'] == gals))

    props = list(zip(*props))
    refs = list(zip(*refs))

    for col, prop, ref in zip(('DIAM', 'BA', 'PA'), props, refs):
        lvd[col][I] = prop
        lvd[f'{col}_REF'][I] = ref

    # at this point, all objects should have diameters and ellipticities
    #assert(np.all((lvd['DIAM'] != -99.) * (lvd['BA'] != -99.)))

    # Next, gather ellipticities and position angles from the input catalog.
    I = np.where(cat['ROW_LVD'] != -99)[0]
    m1, m2 = match(lvd['ROW'], cat['ROW_LVD'][I])
    lvd = lvd[m1]
    lvdcat = cat[I[m2]]

    for prop in ['BA', 'PA']:
        for ref in ['SGA2020', 'HYPERLEDA', 'LIT']:
            I = (lvd[prop] == -99.) * (lvdcat[f'{prop}_{ref}'] != -99.)
            if np.any(I):
                log.info(f'Adding {np.sum(I)} {prop}s from {ref}')
                lvd[prop][I] = lvdcat[f'{prop}_{ref}'][I]
                if ref == 'LIT':
                    lvd[f'{prop}_REF'][I] = lvdcat[f'{prop}_{ref}_REF'][I]
                else:
                    lvd[f'{prop}_REF'][I] = ref

    # default values for everything else
    I = lvd['PA'] == -99.
    if np.any(I):
        log.info(f'Adopting default PAs for {np.sum(I)} objects.')
        lvd['PA'][I] = 0.
        lvd['PA_REF'][I] = 'default'

    I = lvd['BA'] == -99.
    if np.any(I):
        log.info(f'Adopting default BAs for {np.sum(I)} objects.')
        lvd['BA'][I] = 1.
        lvd['BA_REF'][I] = 'default'

    # override the algorithm above based on VI
    if version == 'v1.0.5':
        updatefile = resources.files('SGA').joinpath('data/SGA2025/LVD-geometry-updates.csv')
        updates = Table.read(updatefile, format='csv', comment='#')
        log.info(f'Read {len(updates)} objects from {updatefile}')

        I = match_to(lvd['OBJNAME'].value, updates['objname'].value)
        assert(np.all(lvd['OBJNAME'][I] == updates['objname'].value))

        log.info('Updating hard-coded properties:')
        for col in ['RA', 'DEC', 'DIAM', 'BA', 'PA']:
            J = np.where(updates[col.lower()] != -99.)[0]
            if len(J) > 0:
                log.info(f'  --{len(J)} {col} values')
                lvd[col][I[J]] = updates[col.lower()][J]
                if col == 'RA' or col == 'DEC':
                    lvd['RADEC_REF'][I[J]] = updates['radec_ref'][J]
                else:
                    lvd[f'{col}_REF'][I[J]] = updates[f'{col.lower()}_ref'][J]

    # write out
    lvd = lvd[np.argsort(lvd['ROW'])]

    outfile = os.path.join(sga_dir(), 'parent', 'external', f'LVD_{version}_geometry.fits')
    lvd.write(outfile, overwrite=True)
    log.info(f'Wrote {len(lvd)} objects to {outfile}')

    #if not os.path.isfile(outfile):
    #    log.info(f'Wrote {len(lvd)} objects to {outfile}')
    #    lvd.write(outfile, overwrite=True)
    #else:
    #    log.info(f'Output file {outfile} exists.')

    # Finally, populate the catalog.
    I = np.where(cat['ROW_LVD'] != -99)[0]
    m1, m2 = match(lvd['ROW'], cat['ROW_LVD'][I])
    for prop in ['RA', 'DEC']:
        cat[f'{prop}'][I[m2]] = lvd[prop][m1]
    for prop in ['DIAM', 'BA', 'PA']:
        cat[f'{prop}_LIT'][I[m2]] = lvd[prop][m1]
        cat[f'{prop}_LIT_REF'][I[m2]] = 'LVD' # lvd[f'{prop}_REF'][m1]

    return cat


def update_properties(cat, verbose=False):
    """Update properties, including incorrect coordinates, in both NED and
    HyperLeda.

    """
    # First, update the LVD objects.
    # [0] Update LVD properties.
    cat = update_lvd_properties(cat)

    propfile = resources.files('SGA').joinpath('data/SGA2025/SGA2025-vi-properties.csv')
    props = Table.read(propfile, format='csv', comment='#')

    out = cat.copy()

    allnewcols = ['ra', 'dec', 'diam', 'pa', 'ba', 'diam_hyperleda']
    allupdatecols = ['RA', 'DEC', 'DIAM_LIT', 'PA_LIT', 'BA_LIT', 'DIAM_HYPERLEDA']

    for prop in props:
        objname = prop['objname_ned']
        I = np.where(objname == cat['OBJNAME'].value)[0]
        if len(I) != 1:
            log.info(f'Problem finding {objname}!')
            pdb.set_trace()
            raise ValueError(f'Problem finding {objname}!')
        if verbose:
            log.info(f'{objname} ({cat["OBJTYPE"][I[0]]}):')

        if prop['update_hyperleda_coords'] == 'Yes':
            newcols = allnewcols + ['ra', 'dec']
            updatecols = allupdatecols + ['RA_HYPERLEDA', 'DEC_HYPERLEDA']
        else:
            newcols = allnewcols
            updatecols = allupdatecols

        for newcol, col in zip(newcols, updatecols):
            newval = prop[newcol]
            oldval = cat[col][I[0]]
            if verbose:
                if newval != -99.:
                    log.info(f'  Updating {col}: {oldval} --> {newval}')
                else:
                    log.info(f'  Retaining {col}: {oldval}')
            if newval != -99.:
                out[col][I] = newval

    return out


def build_parent_vicuts(verbose=False, overwrite=False):
    """Build the parent catalog with VI cuts.

    """
    # quick check on duplicates
    actionsfile = resources.files('SGA').joinpath('data/SGA2025/SGA2025-vi-actions.csv')
    actions = Table.read(actionsfile, format='csv', comment='#')
    uobj, cc = np.unique(actions['objname'].value, return_counts=True)
    if np.any(cc > 1):
        log.info(f'The following objects are duplicates in {actionsfile}')
        for obj in uobj[cc > 1]:
            log.info(obj)
        return

    version = parent_version(nocuts=True)
    catfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent-nocuts-{version}.fits')
    origcat = Table(fitsio.read(catfile))
    log.info(f'Read {len(origcat):,d} objects from {catfile}')

    # [1] Update individual-galaxy properties, including coordinates.
    cat = update_properties(origcat, verbose=verbose)
    #cat[np.isin(cat['OBJNAME'], ['LMC', 'SMC'])]['OBJNAME', 'OBJNAME_LVD', 'RA', 'DEC', 'RA_LVD', 'DEC_LVD', 'RA_NED', 'DEC_NED']

    # [2] Drop systems with uncommon prefixes (after VI).
    cat = remove_by_prefix(cat, merger_type=None, verbose=verbose, build_qa=False)

    # make sure we still have all LVD sources (see the VETO array in remove_by_prefix)
    from SGA.io import read_lvd
    lvd = read_lvd()
    lvdcat = cat[cat['ROW_LVD'] != -99]
    try:
        assert(len(lvd[~np.isin(lvd['ROW'], lvdcat['ROW_LVD'])]) == 0)
    except:
        log.info('Missing the following LVD systems!')
        log.info(lvd[~np.isin(lvd['ROW'], lvdcat['ROW_LVD'])])
        pdb.set_trace()

    # [3] Resolve cross-identification errors in NED/HyperLeda.
    cat = resolve_crossid_errors(cat, verbose=False, build_qa=False, rebuild_file=True)

    ## more general HyperLeda-NED cross-ID errors
    #bb = cat[(cat['RA_HYPERLEDA'] != cat['RA']) * (cat['RA_HYPERLEDA'] != -99.)]
    #m1, m2, sep = match_radec(bb['RA'], bb['DEC'], bb['RA_HYPERLEDA'], bb['DEC_HYPERLEDA'], 3./3600., notself=True)
    #m1 = m1[m1 != m2]
    #multipage_skypatch(bb[m1], cat=cat, width_arcsec=120., overwrite_viewer=True, overwrite=True, 
    #                   clip=True, pngdir='vi', jpgdir='vi', pdffile='vi/vi4.pdf', verbose=True)

    if False:
        from astropy.coordinates import SkyCoord
        import astropy.units as u

        cc = Table(fitsio.read('/Users/ioannis/research/projects/SGA/2025/parent/SGA2025-parent-archive-v1.0.fits'))

        #dr = Table(fitsio.read('/Users/ioannis/research/projects/SGA/2025/parent/SGA2025-parent-archive-dr9-north-v1.0.fits'))
        dr = Table(fitsio.read('/Users/ioannis/research/projects/SGA/2025/parent/SGA2025-parent-archive-dr11-south-v1.0.fits'))
        I = np.where((dr['RA'] != dr['RA_HYPERLEDA']) * (dr['RA_HYPERLEDA'] != -99.) * ~dr['RESOLVED'] * (dr['ROW_LVD'] == -99))[0]
        mm = dr[I]

        c_default = SkyCoord(mm['RA'].value*u.deg, mm['DEC'].value*u.deg)
        c_hyper = SkyCoord(mm['RA_HYPERLEDA'].value*u.deg, mm['DEC_HYPERLEDA'].value*u.deg)
        sep = c_default.separation(c_hyper).to(u.arcsec)
        srt = np.argsort(sep.value)[::-1]
        sep = sep[srt]
        mm = mm[srt]
        diam, _, _, _ = choose_geometry(mm, mindiam=0.)

        #plt.clf() ; plt.hist(sep.value, bins=100) ; plt.savefig('junk.png')

        #bb = mm[sep.value > 1200.]
        #bb = mm[(sep.value > 240.) * (sep.value <= 1200.)]
        #bb = mm[(sep.value > 120.) * (sep.value <= 240.)]
        #bb = mm[(sep.value > 60.) * (sep.value <= 120.)]
        #bb = mm[(sep.value > 30.) * (sep.value <= 60.)]
        #bb = mm[(sep.value > 3.) * (sep.value <= 30.) * (diam > 60.)]
        #bb = mm[(sep.value > 5.) * (sep.value <= 30.) * (diam > 30.) * (diam <= 60.)]
        bb = mm[(sep.value > 5.) * (sep.value <= 30.) * (diam > 15.) * (diam <= 30.)]

        multipage_skypatch(bb, cat=cc, width_arcsec=90., overwrite_viewer=True, overwrite=True, clip=True, pngdir='vi', jpgdir='vi', pdffile='vi/vi4.pdf', verbose=True)
        bb['OBJNAME', 'RA', 'DEC', 'RA_HYPERLEDA', 'DEC_HYPERLEDA'].plog.info(max_lines=-1)

    # [4] Resolve close (1 arcsec) pairs.
    cat = resolve_close(cat, cat, maxsep=1., allow_vetos=True, verbose=False)

    # [4] Visually drop GTrpl and GPair systems with and without measured
    # diameters.
    cat = remove_by_prefix(cat, merger_type='GTrpl', merger_has_diameter=False, verbose=verbose, build_qa=False)
    cat = remove_by_prefix(cat, merger_type='GPair', merger_has_diameter=False, verbose=verbose, build_qa=False)

    # [5] only explicitly drop mergers with diameters via VI and the VI-actions file
    remove_by_prefix(cat, merger_type='GTrpl', merger_has_diameter=True, verbose=verbose, build_qa=False)
    remove_by_prefix(cat, merger_type='GPair', merger_has_diameter=True, verbose=verbose, build_qa=False)

    # [6] Remove close pairs (after extensive VI using


    # write out
    version_vicuts = parent_version(vicuts=True)
    outfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent-vicuts-{version_vicuts}.fits')
    log.info(f'Writing {len(cat):,d} objects to {outfile}')
    cat.meta['EXTNAME'] = 'PARENT-VICUTS'
    cat.write(outfile, overwrite=True)

    #I = np.where(~np.isin(cat['OBJTYPE'], ['G', 'GPair', 'GTrpl', 'IrS', 'UvS', 'VisS']))[0]
    #pdffile = 'tmp-objtype.pdf'
    #multipage_skypatch(cat[I[:50]], cat=cat, width_arcsec=30., ncol=5, nrow=5, clip=True,
    #                   jpgdir='tmp-jpeg', pngdir='tmp-objtype', pngsuffix='objtype',
    #                   pdffile=pdffile, verbose=False, overwrite=True, cleanup=True)


def build_parent_archive(verbose=False, overwrite=False):
    """Build the parent catalog.

    """
    from glob import glob
    from astrometry.util.starutil_numpy import deg2dist
    from astrometry.libkd.spherematch import tree_build_radec, trees_match
    from SGA.io import read_custom_external
    from SGA.ellipse import ellipse_mask_sky, choose_geometry

    version_vicuts = parent_version(vicuts=True)
    catfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent-vicuts-{version_vicuts}.fits')
    cat = Table(fitsio.read(catfile))
    log.info(f'Read {len(cat):,d} objects from {catfile}')

    #if False:
    #    I = cat['DIAM_LIT'] > 25. ; srt = np.argsort(cat[I]['DIAM_LIT'])[::-1] ; cat[I][srt]['OBJNAME', 'DIAM_LIT', 'DIAM_LIT_REF', 'DIAM_HYPERLEDA', 'DIAM_SGA2020', 'RA', 'DEC']
    #    cc = cat ; obj='MESSIER 077' ; I=cc['OBJNAME'] == obj ; m1, m2, _ = match_radec(cc['RA'], cc['DEC'], cc[I]['RA'], cc[I]['DEC'], 240./3600.) ; cc[m1][cols]
    #    qa_skypatch(cc[I][0], group=cc[m1], width_arcmin=2., pngdir='ioannis/tmp/', clip=True, overwrite_viewer=True, overwrite=True)
    #    multipage_skypatch(cat[I], cat=cat, width_arcsec=180., pdffile='ioannis/tmp/aa-sdss.pdf', verbose=True)

    # FIXME - apply cuts based on additional external catalogs
    # (ssl-legacysurvey, Galaxy Zoo VI, etc.)

    # read the ssl-legacysurvey results (including the veto file)
    log.info('Applying the ssl results')

    # First read the classification results and throw out the
    # reference sources, which are visually inspected.
    sslfiles = glob(os.path.join(sga_dir(), 'ssl', 'v[1,3]', 'output', 'ssl-parent-chunk???-v[1,3].txt'))
    #sslfiles = glob(os.path.join(sga_dir(), 'ssl', 'v?', 'output', 'ssl-parent-chunk???-v?.txt'))
    ssl = vstack([Table.read(sslfile, format='ascii.commented_header') for sslfile in sslfiles])
    _, I = np.unique(ssl['ROW'], return_index=True)
    ssl = ssl[I]
    ssl = ssl[ssl['REF'] == 0]

    # Now, 'row' may have changed, so we need to read the parent
    # sample(s) to get 'objname'.
    sslfiles = glob(os.path.join(sga_dir(), 'ssl', 'ssl-parent-cat-v[1,3].fits'))
    ssl_parent = vstack([Table(fitsio.read(sslfile)) for sslfile in sslfiles])
    _, uindx = np.unique(ssl_parent['OBJNAME'].value, return_index=True)
    ssl_parent = ssl_parent[uindx]

    indx_parent, indx_ssl = match(ssl_parent['ROW_PARENT'], ssl['ROW'])
    ssl_parent = ssl_parent[indx_parent]
    ssl = ssl[indx_ssl]
    ssl['OBJNAME'] = ssl_parent['OBJNAME']
    ssl['REGION'] = ssl_parent['REGION']

    # investigate objects with common names
    #bb = ssl[ssl['REGION'] == 'dr9-north']
    #bb = bb[np.argsort(bb['RA'])]
    #bb[(prefix != 'SDSS') * (prefix != 'WISEA') * (prefix != '2MASS') * (prefix != 'GALEXMSC') * (prefix != 'GALEXASC')].plog.info(max_lines=-1)

    # when testing, the container has two SGA repos in PYTHONPATH
    try:
        vetodir = str(resources.files('SGA').joinpath('data/SGA2025')._paths[0])
    except:
        vetodir = str(resources.files('SGA').joinpath('data/SGA2025'))
    vetofiles = glob(os.path.join(vetodir, 'ssl-veto-v[1,3].txt'))
    veto = vstack([Table.read(vetofile, format='csv', comment='#') for vetofile in vetofiles])

    # Make sure that the objects we're trying to veto are actually in the ssl
    # files.
    I = np.isin(ssl['OBJNAME'], veto['objname'])
    if np.sum(I) != len(veto):
        log.info('Missing objects in veto files!')
        log.info(veto[~np.isin(veto['objname'], ssl['OBJNAME'])])
        pdb.set_trace()
    ssl = ssl[~I]

    # Add to the veto array any objects with an entry in either the
    # 'properties' or (non-drop) 'actions' file.
    propfile = resources.files('SGA').joinpath('data/SGA2025/SGA2025-vi-properties.csv')
    props = Table.read(propfile, format='csv', comment='#')
    I = np.isin(ssl['OBJNAME'].value, props['objname_ned'].value)
    if np.any(I):
        log.info(f'WARNING: need to add the following {np.sum(I):,d} objects to the appropriate veto file')
        log.info(ssl[I])
        pdb.set_trace()

    actionsfile = resources.files('SGA').joinpath('data/SGA2025/SGA2025-vi-actions.csv')
    actions = Table.read(actionsfile, format='csv', comment='#')
    actions = actions[actions['action'] == 'hyperleda-coords']
    I = np.isin(ssl['OBJNAME'].value, actions['objname'].value)
    if np.any(I):
        log.info(f'WARNING: need to add the following {np.sum(I):,d} objects to the appropriate veto file')
        log.info(ssl[I])
        pdb.set_trace()

    log.info(f'Removing {len(ssl):,d}/{len(cat):,d} objects based on SSL results.')
    cat = cat[~np.isin(cat['OBJNAME'], ssl['OBJNAME'])]

    # Resolve more close pairs -- these choices were made after
    # investigating a bunch of QA.
    ##from SGA.util import find_close

    # Create refcat by sorting cat by PGC, otherwise RA will be used
    # to define the "first" group member, which is not usually what we
    # want. E.g., UGC 08168 is dropped in favor of 2MASS
    # J13034084+5129425 even though the latter has a larger PGC number
    # (but has sep==0.).
    maxsep = 5.
    log.info('Resolving additional close pairs.')
    refcat =  cat.copy()
    refcat[refcat['PGC'] < 0]['PGC'] = np.max(cat['PGC'].value)+1
    srt = np.argsort(refcat['PGC'])
    refcat = refcat[srt]
    cat = cat[srt]
    #bb = refcat[np.isin(refcat['OBJNAME'], ['UGC 08168', '2MASS J13034084+5129425'])]
    #bb = refcat[np.isin(refcat['OBJNAME'], ['GALEXASC J072041.35+561217.0', 'WISEA J072041.29+561218.0'])]
    cat = resolve_close(cat, refcat, maxsep=maxsep, allow_vetos=False,
                        ignore_objtype=True, trim=True, verbose=False)
    cat = cat[np.argsort(cat['RA'].value)]
    del refcat

    # flag objects in the LMC and SMC
    cat['IN_LMC'] = np.zeros(len(cat), bool)
    cat['IN_SMC'] = np.zeros(len(cat), bool)
    for cloud in ['LMC', 'SMC']:
        gal = cat[cat['OBJNAME'] == cloud]
        racen, deccen = gal['RA'].value, gal['DEC'].value
        #gal['DIAM_HYPERLEDA', 'PA_HYPERLEDA', 'BA_HYPERLEDA', 'DIAM_LIT', 'DIAM_LIT_REF', 'PA_LIT', 'BA_LIT']

        diam, ba, pa, _ = choose_geometry(gal)

        semia = diam / 2. / 3600. # [degrees]
        semib = ba * semia
        phi = np.radians(90. - pa)

        I = np.where(ellipse_mask_sky(
            racen, deccen, semia, semib, phi,
            cat['RA'].value, cat['DEC'].value))
        cat[f'IN_{cloud}'][I] = True

        #alldiam, _, _, _ = choose_geometry(cat[I], mindiam=0.)

        #out = Table()
        ##out['name'] = cat['OBJNAME'][I]
        #out['ra'] = cat['RA'][I]
        #out['dec'] = cat['DEC'][I]
        #out.write('ioannis/tmp/junk.fits', overwrite=True)
        #
        #from astropy.coordinates import SkyCoord
        #import astropy.units as u
        #coord = SkyCoord(ra=racen*u.deg, dec=deccen*u.deg)
        #coord_cat = SkyCoord(ra=cat['RA'].value*u.deg, dec=cat['DEC'].value*u.deg)
        #sep = coord.separation(coord_cat)
        #I = sep < 5. * u.deg
        #
        #out = Table()
        ##out['name'] = cat['OBJNAME'][I]
        #out['ra'] = cat['RA'][I]
        #out['dec'] = cat['DEC'][I]
        #out.write('ioannis/tmp/junk2.fits', overwrite=True)
        #cat[I][alldiam/60>1]['OBJNAME', ].write('junk.txt', format='csv', overwrite=True)

    # apply cuts based on the photometry files
    #log.info('Processing the photometry files')
    photo, nphoto = [], 0
    for region in ['dr11-south', 'dr9-north']:
        photofiles = sorted(glob(os.path.join(sga_dir(), 'parent', 'photo', f'parent-photo-{region}-v?.?.fits')))
        for photofile in photofiles:
            photo1 = Table(fitsio.read(photofile))
            #log.info(f'Read {len(photo1):,d} objects from {photofile}')
            photo.append(photo1)
            nphoto += 1
    photo = vstack(photo)
    log.info(f'Read {len(photo):,d} objects from {nphoto} photometry files.')

    # Trim the photo catalog to the objects that are still in the sample.
    I = np.isin(photo['OBJNAME'], cat['OBJNAME'])
    log.info(f'Keeping {np.sum(I):,d}/{len(photo):,d} photometry rows of objects ' + \
          'that are still in the current parent sample.')
    photo = photo[I]

    # Do not throw out objects in the properties, actions, and
    # 'custom' catalogs, which were added by-hand!
    custom = read_custom_external()
    I = np.logical_or.reduce((np.isin(photo['OBJNAME'], props['objname_ned']),
                              np.isin(photo['OBJNAME'], custom['OBJNAME_NED']),
                              np.isin(photo['OBJNAME'], actions['objname'])))
    log.info(f'Removing {np.sum(I):,d}/{len(photo):,d} photometry rows of ' + \
          'objects in the custom or properties tables.')
    photo = photo[~I]

    # The photo files are limited to objects with DIAM_INIT<20 arcsec;
    # after significant VI, most of these are genuinely small objects,
    # so throw them all out for now. However, Keep objects in the
    # LMC,SMC, since those were also visually inspected.
    I = np.isin(cat['OBJNAME'], photo['OBJNAME'])
    J = np.logical_or(~I, np.logical_or(cat['IN_LMC'], cat['IN_SMC']))
    log.info(f'Removing {np.sum(J):,d}/{len(cat):,d} objects with diam_init<20 ' + \
          'arcsec (not in the SMC,LMC) based on the photometry files.')
    cat = cat[J]

    # add the 'resolved' bit
    resfile = resources.files('SGA').joinpath('data/SGA2025/SGA2025-resolved.csv')
    res = Table.read(resfile, format='csv', comment='#')
    log.info(f'Read {len(res)} objects from {resfile}')
    oo, cc = np.unique(res['objname'].value, return_counts=True)
    if np.any(cc > 1):
        log.info('Warning: duplicates in resolve file!')
        pdb.set_trace()
        log.info(oo[cc>1])

    cat['RESOLVED'] = np.zeros(len(cat), bool)
    I = np.isin(cat['OBJNAME'].value, res['objname'].value)
    if np.sum(I) != len(res):
        log.info('Missing objects!')
        pdb.set_trace()
        log.info(res[~np.isin(res['objname'].value, cat['OBJNAME'].value)])
    cat['RESOLVED'][I] = True

    # add the 'FORCEGAIA' and 'FORCEPSF' bits
    for suffix in ['gaia', 'psf']:
        forcefile = resources.files('SGA').joinpath(f'data/SGA2025/SGA2025-force{suffix}.csv')
        force = Table.read(forcefile, format='csv', comment='#')
        log.info(f'Read {len(force)} objects from {forcefile}')
        oo, cc = np.unique(force['objname'].value, return_counts=True)
        if np.any(cc > 1):
            log.info(f'Warning: duplicates in the force{suffix} file!')
            log.info(oo[cc>1])
            pdb.set_trace()

        cat[f'FORCE{suffix.upper()}'] = np.zeros(len(cat), bool)
        I = np.isin(cat['OBJNAME'].value, force['objname'].value)
        if np.sum(I) != len(force):
            log.info('Missing objects!')
            pdb.set_trace()
            log.info(force[~np.isin(force['objname'].value, cat['OBJNAME'].value)])
        cat[f'FORCE{suffix.upper()}'][I] = True

    # add the Gaia mask bits
    log.info(f'Adding Gaia bright-star masking bits.')
    cat['STARFDIST'] = np.zeros(len(cat), 'f4') + 99.
    cat['STARDIST'] = np.zeros(len(cat), 'f4') + 99.
    cat['STARMAG'] = np.zeros(len(cat), 'f4') + 99.

    gaiafile = os.path.join(sga_dir(), 'gaia', 'gaia-mask-dr3-galb9.fits')
    gaia = Table(fitsio.read(gaiafile, columns=['ra', 'dec', 'radius', 'mask_mag', 'isbright', 'ismedium']))
    log.info(f'Read {len(gaia):,d} Gaia stars from {gaiafile}')
    I = gaia['radius'] > 0.
    log.info(f'Trimmed to {np.sum(I):,d}/{len(gaia):,d} stars with radius>0')
    gaia = gaia[I]

    dmag = 1.
    bright = np.min(np.floor(gaia['mask_mag']))
    faint = np.max(np.ceil(gaia['mask_mag']))
    magbins = np.arange(bright, faint, dmag)

    for mag in magbins:
        # find all Gaia stars in this magnitude bin
        I = np.where((gaia['mask_mag'] >= mag) * (gaia['mask_mag'] < mag+dmag))[0]

        # search within 2 times the largest masking radius
        maxradius = 2. * np.max(gaia['radius'][I]) # [degrees]
        log.info(f'Found {len(I):,d} Gaia stars in magnitude bin {mag:.0f} to ' + \
              f'{mag+dmag:.0f} with max radius {maxradius:.4f} degrees.')

        m1, m2, sep = match_radec(cat['RA'], cat['DEC'], gaia['ra'][I], gaia['dec'][I], maxradius, nearest=True)
        if len(m1) > 0:
            zero = np.where(sep == 0.)[0]
            if len(zero) > 0:
                cat['STARDIST'][m1[zero]] = 0.
                cat['STARFDIST'][m1[zero]] = 0.
                cat['STARMAG'][m1[zero]] = gaia['mask_mag'][I[m2[zero]]]

            # separations can be identically zero
            pos = np.where(sep > 0.)[0]
            if len(pos) > 0:
                # distance to the nearest star (in this mag bin)
                # relative to the mask radius of that star (given its
                # mag), capped at a factor of 2; values <1 mean the
                # object is within the star's masking radius
                fdist = sep[pos] / gaia['radius'][I[m2[pos]]].value
                # only store the smallest value
                J = np.where((fdist < 2.) * (fdist < cat['STARFDIST'][m1[pos]]))[0]
                if len(J) > 0:
                    cat['STARDIST'][m1[pos[J]]] = sep[pos[J]] # [degrees]
                    cat['STARFDIST'][m1[pos[J]]] = fdist[J]
                    cat['STARMAG'][m1[pos[J]]] = gaia['mask_mag'][I[m2[pos[J]]]]

    version = parent_version(archive=True)
    outfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent-archive-{version}.fits')
    log.info(f'Writing {len(cat):,d} objects to {outfile}')
    cat.meta['EXTNAME'] = 'PARENT-ARCHIVE'
    cat.write(outfile, overwrite=True)


    if False:
        # How many sources with diam<0.1 have no other source within 10 arcsec?
        radius_isolated = 10.

        diam = np.max((cat['DIAM_LIT'].value, cat['DIAM_HYPERLEDA'].value), axis=0)
        I = np.where(diam == -99.)[0]
        #I = np.where((diam > 0.) * (diam < 0.1))[0]
        #I = np.where(diam < 0.1)[0]
        cat_isolated = cat[I]
        matches = match_radec(cat_isolated['RA'].value, cat_isolated['DEC'].value, cat['RA'].value,
                              cat['DEC'].value, radius_isolated/3600., indexlist=True, notself=True)
        indx_isolated = []
        for iobj, onematch in enumerate(matches):
            if onematch is None:
                continue
            if len(onematch) == 1:
                indx_isolated.append(iobj)
        indx_isolated = np.array(indx_isolated)

        out = cat_isolated[indx_isolated]
        #m1, m2, _ = match_radec(cat['RA'], cat['DEC'], out[0]['RA'], out[0]['DEC'], 10./3600) ; cat[m1][cols]
        #prim=0 ; qa_skypatch(cat[m1[prim]], group=cat[m1])

        prefix = np.array(list(zip(*np.char.split(out['OBJNAME'].value, ' ').tolist()))[0])

        pdffile = 'qa-isolated.pdf'
        multipage_skypatch(out[:50], cat=cat, width_arcsec=30., ncol=1, nrow=1, clip=True,
                           jpgdir='tmp-jpeg', pngdir='tmp-objtype', pngsuffix='objtype',
                           pdffile=pdffile, verbose=False, overwrite=True, cleanup=True)


def build_parent(verbose=False, overwrite=False):
    """Build the parent catalog.

    """
    from SGA.ellipse import choose_geometry
    from SGA.brick import brickname as get_brickname
    from SGA.io import sga2025_name
    from SGA.groups import build_group_catalog
    from SGA.coadds import FITBITS, REGIONBITS

    version = parent_version()
    version_archive = parent_version(archive=True)
    outdir = os.path.join(sga_dir(), 'parent')

    cols = ['OBJNAME',
            #'OBJTYPE', 'MORPH', 'BASIC_MORPH',
            'RA', 'DEC', 'PGC', #'RESOLVED',
            'STARFDIST', 'STARDIST', 'STARMAG', 'REGION']#, 'ROW_PARENT']
    #colindx = np.where(np.array(cols)=='DEC')[0][0] + 1

    # merge the two regions
    mindiam = 30. # [arcsec]
    parent = []
    for region in ['dr11-south', 'dr9-north']:
        catfile = os.path.join(outdir, f'SGA2025-parent-archive-{region}-{version_archive}.fits')
        cat = Table(fitsio.read(catfile))#, rows=np.arange(5000)))
        log.info(f'Read {len(cat):,d} objects from {catfile}')

        diam, ba, pa, ref = choose_geometry(cat, mindiam=0.)
        I = np.logical_or.reduce((cat['ROW_LVD'] != -99, cat['IN_LMC'],
                                  cat['IN_SMC'], diam > mindiam))
        ##########################
        ## trim very small objects and to a specific set of test bricks
        #diam, ba, pa, ref = choose_geometry(cat, mindiam=10.)
        #diam /= 60. # [arcmin]
        #
        #I = (diam > 45./60.)# * (diam < 2.)
        #cat = cat[I]
        #
        #bricknames = get_brickname(cat['RA'].value, cat['DEC'].value)
        #I = np.where(np.isin(bricknames, ['0545m052', '0545m050',
        #                                  '0542m050']))[0]
        #cat = cat[I]
        ##########################

        cat = cat[I]
        log.info(f'Selected {np.sum(I):,d} objects with diameter>' + \
                 f'{mindiam:.1f} arcsec.')

        # add the region bit
        cat['REGION'] = np.int16(REGIONBITS[region])
        parent.append(cat)
    parent = vstack(parent)

    # merge north-south duplicates
    parent.remove_columns(['NCCD', 'FILTERS']) # can be useful to see in the duplicates
    dups, cc = np.unique(parent['OBJNAME'].value, return_counts=True)
    dup = parent[np.isin(parent['OBJNAME'], dups[cc>1])]
    dup = dup[np.argsort(dup['OBJNAME'])]
    assert(np.all(dup['OBJNAME'][0::2] == dup['OBJNAME'][1::2]))
    log.info(f'Found {len(dup):,d}/{len(parent):,d} objects ' + \
             'in north-south overlap region.')

    parent = parent[~np.isin(parent['OBJNAME'], dup['OBJNAME'])]

    dup = dup[0::2] # choose every other one
    dup['REGION'] = 2**0 + 2**1

    parent = vstack((parent, dup))
    parent = parent[np.lexsort([parent['OBJNAME'].value, parent['RA'].value])]
    log.info(f'Combined parent sample has {len(parent):,d} unique objects.')
    assert(np.sum(parent['REGION'] == 3) == len(dup) == len(dups[cc>1]))

    # build the group catalog from the full sample
    diam, ba, pa, ref, mag, band = choose_geometry(
        parent, mindiam=0., get_mag=True)
    diam /= 60. # [arcmin]
    if np.any(diam <= 0.):
        log.warning('Some objects have zero diameter!')
        diam[diam <= 0.] = 10./60.
    assert(np.all(diam > 0.))

    ra, dec = parent['RA'].value, parent['DEC'].value
    allmorph = []
    for objtype, morph, basic_morph in zip(
            parent['OBJTYPE'].value, parent['MORPH'].value,
            parent['BASIC_MORPH'].value):
        morph1 = np.array([objtype, morph, basic_morph])
        morph1 = ';'.join(morph1[morph1 != '']).replace('  ', '')
        allmorph.append(morph1)

    # process the fitting behavior bits
    fitbits = np.zeros(len(parent), np.int16)
    fitbits[parent['RESOLVED']] = FITBITS['ignore']
    fitbits[parent['FORCEGAIA']] = FITBITS['forcegaia']
    fitbits[parent['FORCEPSF']] = FITBITS['forcepsf']
    fitbits[parent['IN_LMC']] = FITBITS['forcegaia']
    fitbits[parent['IN_SMC']] = FITBITS['forcegaia']

    sgaid = np.arange(len(parent))
    grp = parent[cols]

    grp.add_column(sgaid, name='SGAID', index=0)
    grp.add_column(sga2025_name(ra, dec, unixsafe=True),
                   name='SGANAME', index=1)
    grp.add_column(allmorph, name='MORPH', index=3)
    grp.add_column(get_brickname(ra, dec), name='BRICKNAME', index=6)
    grp.add_column(fitbits, name='FITBIT', index=7)
    grp.add_column(diam.astype('f4'), name='DIAM', index=8)
    grp.add_column(ba.astype('f4'), name='BA', index=9)
    grp.add_column(pa.astype('f4'), name='PA', index=10)
    grp.add_column(mag.astype('f4'), name='MAG', index=11)
    grp.add_column(band, name='BAND', index=12)

    grp = build_group_catalog(grp)

    ## final selection and data model
    #I = (diam > 45./60.)# * (diam < 2.)
    #J = np.isin(grp['GROUP_ID'], grp['GROUP_ID'][I])
    #out = grp[J]
    #log.info(f'Selecting {np.sum(J):,d}/{len(cat):,d} objects in {len(set(out["GROUP_ID"])):,d} unique groups')
    out = grp

    outfile = os.path.join(outdir, f'SGA2025-parent-{version}.fits')
    log.info(f'Writing {len(out):,d} objects to {outfile}')
    out.meta['EXTNAME'] = 'PARENT'
    out.write(outfile, overwrite=True)


def qa_parent(nocuts=False, sky=False, size_mag=False):
    """QA of the parent sample.

    """
    from SGA.io import sga_dir
    from SGA.qa import fig_sky, fig_size_mag

    qadir = os.path.join(sga_dir(), 'parent', 'qa')
    if not os.path.isdir(qadir):
        os.makedirs(qadir)

    if nocuts:
        version = parent_version(nocuts=True)
        suffix = '-nocuts'
    else:
        version = parent_version()
        suffix = ''
    catfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent{suffix}-{version}.fits')

    cat = Table(fitsio.read(catfile))#, rows=np.arange(10000)))
    log.info(f'Read {len(cat):,d} objects from {catfile}')

    if sky:
        png = os.path.join(qadir, f'qa-sky-parent{suffix}-{version}.png')
        #I = cat['DIAM_LIT'] > 0.2
        fig_sky(cat, racolumn='RA', deccolumn='DEC', pngfile=png,
                clip_lo=0., clip_hi=300., mloc=50.)

    if size_mag:
        png = os.path.join(qadir, f'qa-sizemag-parent{suffix}-{version}.png')
        fig_size_mag(cat, nocuts=nocuts, pngfile=png)

    # compare diameters
    if False:
        pngfile = os.path.join(qadir, 'qa-diamleda.png')
        I = np.where((allcat['DIAM_LIT'] > 0.) * (allcat['DIAM_HYPERLEDA'] > mindiam))[0]
        log.info(len(I))
        lim = (-1.7, 3.)
        #lim = (np.log10(mindiam)-0.1, 3.)

        import corner
        from SGA.qa import plot_style
        sns, colors = plot_style(talk=True, font_scale=1.2)

        fig, ax = plt.subplots(figsize=(7, 7))
        xx = np.log10(allcat['DIAM_HYPERLEDA'][I])
        yy = np.log10(allcat['DIAM_LIT'][I])
        J = ['IRAS' in morph for morph in allcat['MORPH'][I]]
        corner.hist2d(xx, yy, levels=[0.5, 0.75, 0.95, 0.995],
                      bins=100, smooth=True, color=colors[0], ax=ax, # mpl.cm.get_cmap('viridis'),
                      plot_density=True, fill_contours=True, range=(lim, lim),
                      data_kwargs={'color': colors[0], 'alpha': 0.4, 'ms': 4},
                      contour_kwargs={'colors': 'k'},)
        ax.scatter(xx[J], yy[J], s=15, marker='x', color=colors[2])
        #ax.scatter(xx, yy, s=10)
        ax.set_xlabel(r'$\log_{10}$ (Diameter) [HyperLeda]')
        ax.set_ylabel(r'$\log_{10}$ (Diameter) [archive]')
        #ax.set_xlim(lim)
        #ax.set_ylim(lim)
        ax.plot(lim, lim, color='k', lw=2)
        fig.tight_layout()
        log.info(f'Writing {pngfile}')
        fig.savefig(pngfile)#, bbox_inches='tight')
        plt.close(fig)

        I = np.where(allcat['DIAM_LIT'] > mindiam)[0]
        log.info(f'Trimming to {len(I):,d}/{len(allcat):,d} ({100.*len(I)/len(allcat):.1f}%) ' + \
              f'objects with DIAM_LIT>{60.*mindiam:.1f} arcsec.')
        cat = allcat[I]

        srt = np.argsort(cat['DIAM_LIT'])[::-1]
        cat[srt]['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_LVD', 'DIAM_LIT', 'DIAM_LIT_REF', 'MORPH']


def read_existing_footprint(cat, region, version='v1.0'):
    """Read existing / previous catalogs of running the footprint code on an
    input catalog.

    """
    catfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent-archive-{region}-{version}.fits')
    #ccdsfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent-ccds-{region}-{version}.fits')
    if os.path.isfile(catfile):# and os.path.isfile(ccdsfile):
        refcat = Table(fitsio.read(catfile, columns=['OBJNAME', 'RA', 'DEC', 'ROW_PARENT', 'NCCD', 'FILTERS']))
        #refcat = refcat[:1000]

        log.info(f'Read {len(refcat):,d} objects from the existing {region} ' + \
              f'in-footprint catalog {catfile}')

        # We know that objects with sep==0. are in the footprint, so
        # we don't have to "find" them again...
        m1, m2, sep = match_radec(cat['RA'].value, cat['DEC'].value, refcat['RA'].value, 
                                  refcat['DEC'].value, 1./3600., nearest=True)
        I = sep == 0.
        if np.any(I):
            m1 = m1[I]
            m2 = m2[I]
            log.info(f'Matched {len(m1):,d}/{len(cat):,d} objects to the existing ' + \
                  f'{region} in-footprint catalog.')
            todo = np.delete(np.arange(len(cat)), m1)
            cat_todo = cat[todo] # could be an empty set

            refcat_done = refcat[m2]
            cat_done = cat[m1]
            cat_done['NCCD'] = refcat_done['NCCD']
            cat_done['FILTERS'] = refcat_done['FILTERS']

            #ccds_rows = fitsio.read(ccdsfile, columns='ROW_PARENT')
            #rows = np.where(np.isin(ccds_rows, cat_done['ROW_PARENT'].value))[0]
            #ccds_done = Table(fitsio.read(ccdsfile, rows=rows))

            ##ccds_done = ccds_done[:1000]
            #log.info('Matching CCDs...but have to loop!')
            #for ii in range(len(cat_done)):
            #    ccds_done['ROW_PARENT'][refcat_done[ii]['ROW_PARENT'] == ccds_done['ROW_PARENT']] = cat_done[ii]['ROW_PARENT']
            return cat_todo, cat_done#, ccds_done

    return cat, Table()#, Table()


def in_footprint_work(allcat, chunkindx, allccds, comm=None, radius=1.,
                      width_pixels=38, bands=BANDS):
    """Support routine for in_footprint.

    """
    from SGA.sky import get_ccds

    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size

    if rank == 0:
        from SGA.util import weighted_partition

        t0 = time.time()

        # work on a "chunk" of the catalog
        cat = allcat[chunkindx]

        # I, which is a list of len(cat), is the variable-length of indices into
        # allccds of the matches, or None if no match (which we filter out).
        allindx_ccds = match_radec(cat['RA'].value, cat['DEC'].value, allccds.ra,
                                   allccds.dec, radius, indexlist=True)

        indx_cat = []
        indx_ccds = []
        nccdperobj = []
        for icat, indx_ccds1 in enumerate(allindx_ccds):
            if indx_ccds1 is not None:
                indx_cat.append(icat)
                indx_ccds.append(indx_ccds1)
                nccdperobj.append(len(indx_ccds1))
        nccdperobj = np.array(nccdperobj)
        indx_cat = np.array(indx_cat)

        nobj = len(indx_cat)
        if nobj > 0:
            log.info(f'  Found {len(indx_cat):,d}/{len(cat):,d} objects with at ' + \
                  f'least one CCD within {radius} degree')

            work_byrank = weighted_partition(nccdperobj, size)
            log.info(f'  Distributing {len(np.unique(np.hstack(indx_cat))):,d} objects and ' + \
                  f'{len(np.unique(np.hstack(indx_ccds))):,d} CCDs to {size:,d} ranks')
        else:
            log.info(f'  No objects with at least one CCD within {radius} degree')
    else:
        nobj = 0

    if comm:
        nobj = comm.bcast(nobj, root=0)

    if nobj == 0:
        return Table(), Table()

    if comm:
        # Rank 0 sends work...
        if rank == 0:
            for onerank in range(1, size):
                #log.info(f'Rank {rank} sending work to rank {onerank}')
                if len(work_byrank[onerank]) > 0:
                    comm.send(cat[indx_cat[work_byrank[onerank]]], dest=onerank, tag=1)
                    # build and then send the per-object tables of CCDs
                    ccds_onerank = []
                    for work_byrank_onegal in work_byrank[onerank]:
                        ccds_onerank.append(allccds[indx_ccds[work_byrank_onegal]])
                    comm.send(ccds_onerank, dest=onerank, tag=2)
                else:
                    comm.send(Table(), dest=onerank, tag=1)
                    comm.send(Table(), dest=onerank, tag=2)
            # work for rank 0
            cat_onerank = cat[indx_cat[work_byrank[rank]]]
            ccds_onerank = []
            for work_byrank_onegal in work_byrank[rank]:
                ccds_onerank.append(allccds[indx_ccds[work_byrank_onegal]])
            #log.info(f'Rank {rank} received {len(cat_onerank):,d} objects.')
        else:
            # ...and the other ranks receive the work.
            cat_onerank = comm.recv(source=0, tag=1)
            ccds_onerank = comm.recv(source=0, tag=2)
            #log.info(f'Rank {rank} received {len(cat_onerank):,d} objects.')
    else:
        if len(work_byrank[rank]) > 0:
            cat_onerank = cat[indx_cat[work_byrank[rank]]]
            ccds_onerank = []
            for work_byrank_onegal in work_byrank[rank]:
                ccds_onerank.append(allccds[indx_ccds[work_byrank_onegal]])
        else:
            cat_onerank = Table()
            ccds_onerank = Table()

    if comm:
        comm.barrier()

    # now perform a more refined per-object search for matching CCDs
    fcat = []
    fccds = []
    nobj = len(cat_onerank)
    #log.info(rank, nobj)

    #if nobj == 0:
    #    log.info(f'Rank {rank}: all done; no work to do.')
    if nobj > 0:
        t1 = time.time()
        #log.info(f'Rank {rank}: gathering CCDs for {nobj:,d} objects.')
        for icat, (cat_onegal, ccds_onegal) in enumerate(zip(cat_onerank, ccds_onerank)):
            #if icat % 1000 == 0:
            #    log.info(f'Rank {rank}: Working on galaxy: {icat:,d}/{nobj:,d}')
            fccds1, fcat1 = get_ccds(ccds_onegal, cat_onegal, width_pixels,
                                     pixscale=PIXSCALE, return_ccds=True)
            fcat.append(fcat1)
            fccds.append(fccds1)
        if len(fcat) > 0:
            fcat = vstack(fcat)
            fccds = vstack(fccds)
        else:
            fcat = Table()
            fccds = Table()
        #log.info(f'Rank {rank}: all done in {(time.time()-t1)/60.:.3f} minutes')

    if comm:
        fcat = comm.gather(fcat, root=0)
        fccds = comm.gather(fccds, root=0)

    # sort and return
    if rank == 0:
        fcat = vstack(fcat)
        fccds = vstack(fccds)

        if len(fcat) > 0:
            fcat = fcat[np.argsort(fcat['ROW_PARENT'])]
            fccds = fccds[np.argsort(fccds['ROW_PARENT'])]
            log.info(f'  Gathered {len(fccds):,d} CCDs for {len(fcat):,d}/{len(indx_cat):,d} ' + \
                  f'objects in {time.time()-t0:.2f} sec')

        return fcat, fccds
    else:
        return Table(), Table()


def in_footprint(region='dr9-north', comm=None, radius=1., width_pixels=38,#152, 
                 bands=BANDS, ntest=None):
    """Find which objects are in the given survey footprint based on positional
    matching with a very generous (1 deg) search radius.

    radius in degrees
    width_pixels - diameter for more refined search

    """
    from SGA.coadds import RUNS

    if comm is None:
        rank = 0
    else:
        rank = comm.rank

    if rank == 0:
        from legacypipe.runs import get_survey
        from SGA.io import set_legacysurvey_dir

        t0 = time.time()

        # read the parent catalog
        version = parent_version(archive=True)
        catfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent-archive-{version}.fits')

        F = fitsio.FITS(catfile)
        N = F[1].get_nrows()
        if ntest is not None:
            rng = np.random.default_rng(seed=1)
            I = rng.choice(N, size=ntest, replace=False)
            I = I[np.argsort(I)]
        else:
            #log.info('HACK TO JUST SELECT THE RC3!!')
            #ref = fitsio.read(catfile, columns='DIAM_LIT_REF')
            #I = np.where(ref == 'RC3')[0]
            I = np.arange(N)

        #log.info('HACK!!')
        #I = np.array([4784167, 4784170])
        #I = np.array([2016776])

        cat = Table(fitsio.read(catfile, columns=['OBJNAME', 'RA', 'DEC', 'ROW_PARENT'], rows=I))
        cat['NCCD'] = np.zeros(len(cat), int)
        cat['FILTERS'] = np.zeros(len(cat), '<U4')
        log.info(f'Read {len(cat):,d} objects from {catfile}')

        # sort by right ascension to try to speed things up...
        cat = cat[np.argsort(cat['RA'])]

        set_legacysurvey_dir(region)
        survey = get_survey(RUNS[region], allbands=BANDS[region])

        _ = survey.get_ccds_readonly()
        allccds = survey.ccds
        log.info(f'Read {len(allccds):,d} CCDs from region={region}')
        I = allccds.ccd_cuts == 0
        log.info(f'Trimming to {np.sum(I):,d}/{len(allccds):,d} CCDs with ccd_cuts==0')
        allccds = allccds[I]

        # in dr9-north, censor the CCDs in and around M31 and one failed brick around (ra,dec)=(328.8, 0.0)
        # https://www.legacysurvey.org/viewer-desi?ra=10.6890&dec=41.2719&layer=unwise-neo7&zoom=9&ccds9n
        # https://www.legacysurvey.org/viewer-desi?ra=328.8798&dec=0.0422&layer=ls-dr9-north&zoom=12&ccds9n&bricks
        # 3288p000
        if region == 'dr9-north':
            I = (allccds.ra > 8.5) * (allccds.ra < 12.5) * (allccds.dec > 35.) * (allccds.dec < 45.)
            log.info(f'Removing {np.sum(I)} CCDs around M31.')
            allccds = allccds[~I]

            B = survey.get_brick_by_name('3288p000')
            I = (cat['RA'] > B.ra1) * (cat['RA'] < B.ra2) * (cat['DEC'] > B.dec1) * (cat['DEC'] < B.dec2)
            if np.count_nonzero(I) > 0:
                log.info(f'Removing {np.sum(I):,d} objects located in failed brick 3288p000')
                cat = cat[~I]

        sys.stdout.flush()

        # check for an existing CCDs catalog from previous runs of this code
        #cat, cat_done, ccds_done = read_existing_footprint(cat, allccds, region, version=version)
        cat, cat_done = read_existing_footprint(cat, region, version=version)
        #log.info('HACK!!')
        #cat_done = []

        # divide the parent catalog into more managable chunks, and loop
        if region == 'dr9-north':
            nperchunk = 100000
        else:
            nperchunk = 10000
        nobj = len(cat)
        if nobj == 0: # if already done
            nchunk = 1
        else:
            nchunk = int(np.ceil(nobj / nperchunk))
        chunkindx = np.array_split(np.arange(nobj), nchunk)
    else:
        cat = None
        chunkindx = None
        allccds = None

    if comm:
        chunkindx = comm.bcast(chunkindx, root=0)

    allfcat = []
    #allfccds = []
    for ichunk, indx in enumerate(chunkindx):
        if rank == 0:
            log.info(f'Working on chunk {ichunk+1:,d}/{nchunk:,d} with {len(indx):,d} objects')
        fcat, fccds = in_footprint_work(cat, indx, allccds, comm=comm, radius=radius, 
                                        width_pixels=width_pixels, bands=bands)
        sys.stdout.flush()

        if rank == 0:
            allfcat.append(fcat)
            #allfccds.append(fccds)

    if comm:
        comm.barrier()

    # gather up the results and write out
    if rank == 0:
        from SGA.util import match
        allfcat = vstack(allfcat)
        #allfccds = vstack(allfccds)

        # is there an existing catalog?
        if len(cat_done) > 0:
            allfcat = vstack((allfcat, cat_done))
            #allfccds = vstack((allfccds, ccds_done))

        outcat = Table(fitsio.read(catfile)) # read the whole catalog
        # match on position not ROW_PARENT, which can change
        cat_indx, fcat_indx, sep = match_radec(
            outcat['RA'].value, outcat['DEC'].value, allfcat['RA'].value,
            allfcat['DEC'].value, 1./3600., nearest=True)
        #cat_indx, fcat_indx = match(outcat['ROW_PARENT'], allfcat['ROW_PARENT'])
        outcat = outcat[cat_indx]
        allfcat = allfcat[fcat_indx]
        assert(np.all(outcat['ROW_PARENT'] == allfcat['ROW_PARENT']))

        outcat['NCCD'] = allfcat['NCCD']
        outcat['FILTERS'] = allfcat['FILTERS']
        outcat = outcat[np.argsort(outcat['ROW_PARENT'])]

        version = parent_version(archive=True)
        outfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent-archive-{region}-{version}.fits')
        log.info(f'Writing {len(outcat):,d} objects to {outfile}')
        outcat.write(outfile, overwrite=True)

        ## need to figure this out...
        #if 'col0' in allfccds.columns:
        #    allfccds.remove_column('col0')

        #allfccds = allfccds[np.argsort(allfccds['ROW_PARENT'])]
        #ccdsfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent-ccds-{region}-{version}.fits')
        #log.info(f'Writing {len(allfccds):,d} CCDs to {ccdsfile}')
        #allfccds.write(ccdsfile, overwrite=True)

        log.info(f'All done in {(time.time()-t0)/60.:.2f} min')


def qa_footprint(region='dr9-north', show_fullcat=False, show_fullccds=False):
    """Build some simple QA of the in-footprint sample.

    TODO - make this a sky map

    """
    from collections import Counter
    import matplotlib.pyplot as plt
    from SGA.qa import plot_style

    sns, colors = plot_style(talk=True, font_scale=0.9)

    version = parent_version(archive=True)

    catfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent-archive-{region}-{version}.fits')
    cat = Table(fitsio.read(catfile, columns=['OBJNAME', 'RA', 'DEC', 'ROW_PARENT', 'FILTERS']))
    log.info(f'Read {len(cat):,d} objects from {catfile}')

    qafile = os.path.join(sga_dir(), 'parent', 'qa', f'qa-parent-archive-{region}-{version}.png')

    fig, ax = plt.subplots(figsize=(8, 6))

    if show_fullcat:
        fullcatfile = os.path.join(sga_dir(), 'parent', f'SGA2025-parent-archive-{version}.fits')
        fullcat = Table(fitsio.read(fullcatfile, columns=['OBJNAME', 'RA', 'DEC', 'ROW_PARENT']))
        log.info(f'Read {len(fullcat):,d} objects from {fullcatfile}')
        ax.scatter(fullcat['RA'], fullcat['DEC'], s=1, color='gray')

    if show_fullccds:
        from legacypipe.runs import get_survey
        from SGA.io import set_legacysurvey_dir
        from SGA.coadds import RUNS

        set_legacysurvey_dir(region)
        survey = get_survey(RUNS[region], allbands=BANDS[region])
        _ = survey.get_ccds_readonly()
        allccds = survey.ccds
        log.info(f'Read {len(allccds):,d} CCDs from region={region}')
        I = allccds.ccd_cuts == 0
        log.info(f'Trimming to {np.sum(I):,d}/{len(allccds):,d} CCDs with ccd_cuts==0')
        allccds = allccds[I]

        ax.scatter(allccds.ra, allccds.dec, s=1, color='gray')

    #for bands in sorted(set(cat['FILTERS'])):
    C = Counter(cat['FILTERS'])
    for iband, bands in enumerate(sorted(C, reverse=True)):
        I = cat['FILTERS'] == bands
        if np.sum(I) > 0:
            ax.scatter(cat['RA'][I], cat['DEC'][I], s=1, alpha=0.7, zorder=iband,
                       label=f'{bands} (N={np.sum(I):,d})')
    ax.set_xlabel('R.A. (degree)')
    ax.set_ylabel('Dec (degree)')
    if False: # M31 + family
        ax.set_xlim(15., 8.)
        ax.set_ylim(30., 50.)
    else:
        ax.set_xlim(360., 0.)
        if region == 'dr9-north':
            ax.set_ylim(-25., 90.)
            loc = 'lower left'
        else:
            ax.set_ylim(-95., 95.)
            loc = 'upper left'
    ax.legend(fontsize=9, ncols=4, markerscale=8, loc=loc)
    fig.tight_layout()
    fig.savefig(qafile)
    log.info(f'Wrote {qafile}')


def main():
    """Main wrapper

    """
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--mp', default=1, type=int, help='Number of multiprocessing processes per MPI rank.')
    parser.add_argument('--ntest', type=int, default=None, help='Number of test objects to read (only with --in-footprint.')
    parser.add_argument('--region', default='dr9-north', choices=['dr9-north', 'dr9-south', 'dr10-south', 'dr11-south'], 
                        type=str, help='Region to pass to --in-footprint.')

    parser.add_argument('--build-parent-nocuts', action='store_true', help='Merge the catalogs retrieved by SGA2025-query-ned into the parent-nocuts catalog.')
    parser.add_argument('--build-parent-vicuts', action='store_true', help='Apply VI results to the parent-nocuts catalog.')
    parser.add_argument('--build-parent-archive', action='store_true', help='Apply ssl and other cuts but maintain "archive" object names and data model.')
    parser.add_argument('--build-parent', action='store_true', help='Generate the final parent catalog with the final data model.')
    parser.add_argument('--in-footprint', action='store_true', help='Match the various external catalogs to the CCDs files.')

    parser.add_argument('--qa-footprint', action='store_true', help='Build QA of the in-footprint samples.')
    parser.add_argument('--qa-parent', action='store_true', help='Build QA of the parent sample.')

    parser.add_argument('--verbose', action='store_true', help='Be verbose.')
    parser.add_argument('--overwrite', action='store_true', help='Overwrite existing files.')
    args = parser.parse_args()

    # https://docs.nersc.gov/development/languages/python/parallel-python/#use-the-spawn-start-method
    if args.mp > 1 and 'NERSC_HOST' in os.environ:
        import multiprocessing
        multiprocessing.set_start_method('spawn')

    try:
        from mpi4py import MPI
        from mpi4py.util import pkl5
        comm = pkl5.Intracomm(MPI.COMM_WORLD)
    except ImportError:
        comm = None

    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size

    if args.build_parent_nocuts:
        from SGA.parent import build_parent_nocuts
        build_parent_nocuts()

    if args.build_parent_vicuts:
        build_parent_vicuts(overwrite=args.overwrite, verbose=args.verbose)

    if args.build_parent_archive:
        build_parent_archive(overwrite=args.overwrite, verbose=args.verbose)

    if args.build_parent:
        build_parent(overwrite=args.overwrite, verbose=args.verbose)

    if args.in_footprint:
        in_footprint(region=args.region, comm=comm, ntest=args.ntest)

    if args.qa_footprint:
        qa_footprint(region=args.region)

    if args.qa_parent:
        qa_parent(nocuts=False, size_mag=True, sky=True)


if __name__ == '__main__':
    main()
