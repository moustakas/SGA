#!/usr/bin/env python

"""Code to build the parent SGA2024 sample based on a combination of internal and external catalogs.

SGA2024-build-parent --build-parent-nocuts
SGA2024-build-parent --build-parent --mindiam 0.25

SGA2024-build-parent --in-footprint --mp 128 --overwrite

Or, on my laptop:
/opt/homebrew/bin/python3 ~/code/SGA/bin/SGA2024/SGA2024-build-parent --merge

"""
import os, time, shutil, pdb
from importlib import resources
from glob import glob
from collections import Counter
import numpy as np
import numpy.ma as ma
import fitsio
from astropy.table import Table, vstack, hstack, join
import astropy.units as u
from astropy.coordinates import SkyCoord, match_coordinates_sky
import matplotlib.pyplot as plt

from astrometry.util.starutil_numpy import arcsec_between
from astrometry.libkd.spherematch import match_radec

from SGA.coadds import PIXSCALE, BANDS
from SGA.util import get_basic_geometry, match, match_to, choose_primary, resolve_close
from SGA.io import (sga_dir, get_raslice, read_hyperleda, version_hyperleda,
                    read_hyperleda_galaxies, version_hyperleda_galaxies,
                    read_hyperleda_multiples, version_hyperleda_multiples,
                    nedfriendly_hyperleda, read_nedlvs, version_nedlvs,
                    read_lvd, version_lvd, nedfriendly_lvd, version_custom_external,
                    read_custom_external)


cols = ['OBJNAME', 'OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'MORPH', 'DIAM', 'DIAM_LEDA',
        'OBJTYPE', 'RA', 'DEC', 'RA_NED', 'DEC_NED', 'RA_HYPERLEDA', 'DEC_HYPERLEDA',
        'MAG', 'Z', 'PGC', 'PARENT_ROW']

qadir = os.path.join(sga_dir(), 'parent', 'qa')
if not os.path.isdir(qadir):
    os.makedirs(qadir)


def qa_skypatch(primary=None, group=None, racol='RA', deccol='DEC', suffix='group',
                pngsuffix=None, objname=None, racenter=None, deccenter=None,
                add_title=True, width_arcmin=2., pngdir='.', jpgdir='.', clip=False,
                verbose=False, overwrite_viewer=False, overwrite=False):
    """Build QA which shows all the objects in a ~little patch of sky.

    primary - parent-style catalog
    group (optional) - parent-style catalog with additional systems in the field

    """
    import matplotlib.image as mpimg
    from matplotlib.patches import Circle
    from matplotlib.lines import Line2D
    from urllib.request import urlretrieve
    from astropy.wcs import WCS
    from astropy.io import fits


    if pngsuffix is None:
        pngsuffix = suffix

    def get_wcs(racenter, deccenter, width_arcmin=2., survey='ls'):
        pix = {'ls': 0.262, 'unwise': 2.75}
        pixscale = pix[survey]
        width = int(60. * width_arcmin / pixscale)
        hdr = fits.Header()
        hdr['NAXIS'] = 2
        hdr['NAXIS1'] = width
        hdr['NAXIS2'] = width
        hdr['CTYPE1'] = 'RA---TAN'
        hdr['CTYPE2'] = 'DEC--TAN'
        hdr['CRVAL1'] = racenter
        hdr['CRVAL2'] = deccenter
        hdr['CRPIX1'] = width/2+0.5
        hdr['CRPIX2'] = width/2+0.5
        hdr['CD1_1'] = -pixscale/3600.
        hdr['CD1_2'] = 0.0
        hdr['CD2_1'] = 0.0
        hdr['CD2_2'] = +pixscale/3600.
        wcs = WCS(hdr)
        width = wcs.pixel_shape[0]
        return wcs, width, pixscale


    def get_url(racenter, deccenter, width, layer):
        url = f'https://www.legacysurvey.org/viewer/jpeg-cutout?ra={racenter}&dec=' + \
            f'{deccenter}&width={width}&height={width}&layer={layer}'
        #print(url)
        return url

    bbox = dict(boxstyle='round', facecolor='k', alpha=0.5)
    ref_pixscale = 0.262

    if primary is None and group is None:
        raise ValueError('Must specify group *and/or* primary')

    if primary is not None and group is None:
        group = Table(primary)
    if primary is None and group is not None:
        primary = Table(group[0])

    if verbose:
        print(group[cols])
    N = len(group)

    if racenter is None and deccenter is None:
        racenter = primary[racol]
        deccenter = primary[deccol]
    if objname is None:
        objname = primary['OBJNAME']

    outname = objname.replace(' ', '_')
    pngfile = os.path.join(pngdir, f'{outname}-{pngsuffix}.png')
    if os.path.isfile(pngfile) and not overwrite:
        print(f'Output file {pngfile} exists; skipping.')
        return


    # check if the viewer cutout file exists
    surveys = ['ls', 'ls', 'unwise']
    layers = ['ls-dr9', 'ls-dr10', 'unwise-neo7']
    for survey, layer in zip(surveys, layers):
        jpgfile = os.path.join(jpgdir, f'{outname}-{suffix}-{layer}.jpeg')
        if os.path.isfile(jpgfile):
            surveys = [survey]
            layers = [layer]

    for survey, layer in zip(surveys, layers):
        wcs, width, pixscale = get_wcs(racenter, deccenter, survey=survey, width_arcmin=width_arcmin)
        jpgfile = os.path.join(jpgdir, f'{outname}-{suffix}-{layer}.jpeg')
        if os.path.isfile(jpgfile) and not overwrite_viewer:
            img = mpimg.imread(jpgfile)
        else:
            urlretrieve(get_url(racenter, deccenter, width, layer=layer), jpgfile)
            img = mpimg.imread(jpgfile)
            if np.all(img == 32): # no data
                os.remove(jpgfile)
            else:
                break

    decsort = np.argsort(group[deccol])

    leg_colors = ['red']
    leg_lines = ['-']
    leg_labels = ['Adopted']

    #fig = plt.figure()
    #ax = fig.add_subplot()#projection=wcs)
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.imshow(img, origin='lower')
    for imem, (mem, yoffset) in enumerate(zip(group[decsort], (np.arange(0, N)+0.5) * width / N)):

        label = f'{mem["OBJNAME"]} ({mem["OBJTYPE"]}): D={mem["DIAM"]:.3g}\nPGC {mem["PGC"]}: D(LEDA)={mem["DIAM_LEDA"]:.3g})'

        if imem % 2 == 0:
            xoffset = 0.2 * width
        else:
            xoffset = width - 0.2 * width

        if width-yoffset > width / 2:
            va = 'top'
        else:
            va = 'bottom'

        pix = wcs.wcs_world2pix(mem[racol], mem[deccol], 1)
        if np.abs(pix[1]-yoffset) < int(0.03*width):
            yoffset += int(0.03*width)

        ax.add_artist(Circle((pix[0], width-pix[1]), radius=4.*ref_pixscale/pixscale,
                             facecolor='none', edgecolor='red', lw=2, ls='-', alpha=0.9, clip_on=clip))
        ax.annotate('', xy=(pix[0], width-pix[1]), xytext=(xoffset, width-yoffset),
                    annotation_clip=False, arrowprops=dict(
                        facecolor='red', width=3, headwidth=8, shrink=0.01, alpha=0.9))
        ax.annotate(label, xy=(xoffset, width-yoffset), xytext=(xoffset, width-yoffset),
                    va=va, ha='center', color='white', bbox=bbox, fontsize=9,
                    annotation_clip=clip)

        if mem['RA_HYPERLEDA'] != -99. and mem['RA'] != mem['RA_HYPERLEDA']:
            hyper_pix = wcs.wcs_world2pix(mem['RA_HYPERLEDA'], mem['DEC_HYPERLEDA'], 1)
            ax.add_artist(Circle((hyper_pix[0], width-hyper_pix[1]), radius=8.*ref_pixscale/pixscale,
                                 facecolor='none', edgecolor='cyan', lw=1, ls='--', alpha=0.5, clip_on=clip))
            ax.annotate('', xy=(hyper_pix[0], width-hyper_pix[1]), xytext=(xoffset, width-yoffset),
                        annotation_clip=clip, arrowprops=dict(
                            facecolor='cyan', width=1, ls='--', headwidth=4, shrink=0.01, alpha=0.5))
            leg_colors += ['cyan']
            leg_lines += ['--']
            leg_labels += ['HyperLeda']

        if mem['RA_NED'] != -99. and mem['RA'] != mem['RA_NED']:
            ned_pix = wcs.wcs_world2pix(mem['RA_NED'], mem['DEC_NED'], 1)
            ax.add_artist(Circle((ned_pix[0], width-ned_pix[1]), radius=8.*ref_pixscale/pixscale,
                                 facecolor='none', edgecolor='yellow', lw=1, ls='--', alpha=0.5, clip_on=clip))
            ax.annotate('', xy=(ned_pix[0], width-ned_pix[1]), xytext=(xoffset, width-yoffset),
                        annotation_clip=clip, arrowprops=dict(
                            facecolor='yellow', width=1, ls='--', headwidth=4, shrink=0.01, alpha=0.5))
            leg_colors += ['yellow']
            leg_lines += ['--']
            leg_labels += ['NED']

        if mem['RA_LVD'] != -99. and mem['RA'] != mem['RA_LVD']:
            lvd_pix = wcs.wcs_world2pix(mem['RA_LVD'], mem['DEC_LVD'], 1)
            ax.add_artist(Circle((lvd_pix[0], width-lvd_pix[1]), radius=8.*ref_pixscale/pixscale,
                                 facecolor='none', edgecolor='blue', lw=1, ls='--', alpha=0.5, clip_on=clip))
            ax.annotate('', xy=(lvd_pix[0], width-lvd_pix[1]), xytext=(xoffset, width-yoffset),
                        annotation_clip=clip, arrowprops=dict(
                            facecolor='blue', width=1, ls='--', headwidth=4, shrink=0.01, alpha=0.5))
            leg_colors += ['blue']
            leg_lines += ['--']
            leg_labels += ['LVD']

    _, uindx = np.unique(leg_labels, return_index=True)
    ax.legend([Line2D([0], [0], color=col, linewidth=2, linestyle=ls)
               for col, ls in zip(np.array(leg_colors)[uindx], np.array(leg_lines)[uindx])],
              np.array(leg_labels)[uindx], loc='upper left', frameon=True, framealpha=0.7,
              fontsize=8)

    ax.invert_yaxis() # JPEG is flipped relative to my FITS WCS
    if add_title:
        ax.set_title(f'{objname} ({racenter:.8f},{deccenter:.8f})')
    ax.axis('off')
    fig.tight_layout()
    fig.savefig(pngfile, bbox_inches=0)#, dpi=200)
    plt.close()
    #print(f'Wrote {pngfile}')

    return pngfile


def multipage_skypatch(primaries, cat=None, width_arcsec=75., ncol=4, nrow=6,
                       add_title=True, pngsuffix='group', jpgdir='.', pngdir='.',
                       pdffile='multipage-skypatch.pdf', clip=False, verbose=False,
                       overwrite_viewer=False, overwrite=True, cleanup=False):
    """Call qa_skypatch on a table of sources.

    """
    from matplotlib.backends.backend_pdf import PdfPages
    from matplotlib.image import imread

    if not os.path.isdir(jpgdir):
        os.makedirs(jpgdir)
    if not os.path.isdir(pngdir):
        os.makedirs(pngdir)

    primaries = Table(primaries)
    nobj = len(primaries)

    if cat is None:
        groups = [None] * nobj
    else:
        matches = match_radec(primaries['RA'].value, primaries['DEC'].value, cat['RA'].value,
                              cat['DEC'].value, width_arcsec/3600., indexlist=True, notself=False)
        groups = [cat[onematch] for onematch in matches]


    pngfile = []
    for iobj, (primary, group) in enumerate(zip(primaries, groups)):
        #width = crossid['dtheta_arcsec'] / 60. # [arcmin]
        objname = f'{primary["OBJNAME"]}-{primary["OBJTYPE"]}'
        racenter, deccenter = primary['RA'], primary['DEC']
        #pngfile.append(
        #    qa_skypatch(group=fullcat[match_full[iobj+ss]], pngsuffix='original',
        #                add_title=False,
        #                objname=objname, racenter=racenter, deccenter=deccenter,
        #                pngdir=pngdir, jpgdir=jpgdir, verbose=True, overwrite=True)
        #    )
        pngfile.append(
            qa_skypatch(primary, group=group, pngsuffix=pngsuffix, add_title=add_title,
                        objname=objname, racenter=racenter, deccenter=deccenter,
                        width_arcmin=width_arcsec / 60., clip=clip, pngdir=pngdir,
                        jpgdir=jpgdir, verbose=verbose, overwrite=overwrite,
                        overwrite_viewer=overwrite_viewer)
        )
        if verbose:
            print()

    pngfile = np.array(pngfile)
    allindx = np.arange(len(pngfile))

    nperpage = ncol * nrow
    npage = int(np.ceil(len(pngfile) / nperpage))

    pdf = PdfPages(pdffile)
    for ipage in range(npage):
        indx = allindx[ipage*nperpage:(ipage+1)*nperpage]
        fig, ax = plt.subplots(nrow, ncol, figsize=(2*ncol, 2*nrow))
        for iax, xx in enumerate(np.atleast_1d(ax).flat):
            if iax < len(indx):
                xx.imshow(imread(pngfile[indx[iax]]), interpolation='None')
            xx.axis('off')
        #for xx in np.arange(iax, nperpage):
        fig.subplots_adjust(wspace=0., hspace=0., bottom=0.05, top=0.95, left=0.05, right=0.95)
        pdf.savefig(fig, dpi=150)
        plt.close()
    pdf.close()
    print(f'Wrote {pdffile}')

    # clean up
    if cleanup:
        for png in pngfile:
            if os.path.isfile(png):
                if verbose:
                    print(f'Removing {png}')
                os.remove(png)
        if os.path.isfile(pngdir):
            shutil.rmtree(pngdir)


def drop_by_prefix(drop_prefix, allprefixes, pgc=None, diam=None, objname=None,
                   VETO=None, reverse=False, verbose=False):
    """Drop sources according to their name prefix. Most/all of these have
    been visually inspected. However, don't drop a source if it has PGC
    number and at least one diameter.

    """
    I = drop_prefix == allprefixes
    if pgc is not None and diam is not None:
        G = (pgc != -99) * (diam != -99.)
        I *= ~G
    elif diam is not None:
        G = (diam != -99.)
        I *= ~G

    if objname is not None and VETO is not None:
        I *= ~np.isin(objname, VETO)

    if reverse:
        I = np.where(~I)[0]
    else:
        I = np.where(I)[0]

    if verbose:
        if reverse:
            print(f'Keeping {len(I):,d} object(s) without prefix {drop_prefix}.')
        else:
            print(f'Dropping {len(I):,d} object(s) with prefix {drop_prefix}.')
    return I


def resolve_crossid_errors(fullcat, verbose=False, cleanup=True, rebuild_file=False,
                           build_qa=False):
    """Identify cross-identification errors.

    """
    crossidfile = resources.files('SGA').joinpath('data/SGA2024/SGA2024-crossid-errors.csv')

    # Read or, optionally, rebuild the cross-id error file.
    if rebuild_file:
        VETO = [
            'SBS 0924+554',            # SBS 0924+554 and SDSS J092824.31+551107.4 are distinct.
            '2MASS J04374625-2711389', # 2MASS J04374625-2711389 and WISEA J043745.83-271135.1 are distinct.
            '2MFGC 16892',             # points to the center of the system
            'IV Zw 056',               # IV Zw 056 and IV Zw 056 NOTES02 are distinct
        ]

        # First, resolve 1-arcsec pairs **excluding** GPair and GTrpl systems.
        I = (fullcat['OBJTYPE'] != 'GPair') * (fullcat['OBJTYPE'] != 'GTrpl')

        cat = resolve_close(fullcat[I], fullcat[I], maxsep=1., allow_vetos=True, verbose=False)
        cat = vstack((cat, fullcat[~I]))
        cat = cat[np.argsort(cat['PARENT_ROW'])]

        # Next, algorithmically identify cross-identification errors.
        # Basically, NED typically associates the properties from HyperLeda
        # (PGC number, diameter, etc.) with the *system* rather than with the
        # appropriate galaxy (which is another entry in NED).
        print(f'Writing {crossidfile}')
        F = open(crossidfile, 'w')
        F.write('objname_ned_from,pgc_from,objname_ned_to,dtheta_arcsec,comment\n')

        #for system in ['GPair']:
        for system in ['GTrpl', 'GPair']:
            M = np.where((cat['OBJTYPE'] == system) * (cat['PGC'] != -99))[0]
            for m1 in M: # [1516:]:
                if cat[m1]['OBJNAME_NED'] in VETO:
                    print(f'Skipping vetoed system {cat[m1]["OBJNAME_NED"]}')
                    continue

                dtheta = arcsec_between(cat[m1]['RA_HYPERLEDA'], cat[m1]['DEC_HYPERLEDA'], cat['RA'], cat['DEC'])
                # Is there *another* source more than 1 arcsec away which is within
                # 1.5 arcsec of the *HyperLeda* coordinates with PGC==-99?
                m2 = np.where((dtheta < 1.5) * (cat['PGC'] == -99))[0]
                if len(m2) == 0:
                    continue
                elif len(m2) > 1:
                    #qa_skypatch(cat[m1], group=vstack((cat[m1], cat[m2])), pngdir='.', jpgdir='.', verbose=True, overwrite=True)
                    group = cat[m2]
                    group['SEP'] = arcsec_between(cat[m1]['RA_HYPERLEDA'], cat[m1]['DEC_HYPERLEDA'], group['RA'], group['DEC'])
                    primary, drop = choose_primary(group)
                    m2 = m2[primary]

                # If so, copy over the HyperLeda quantities over to the other
                # source.
                m2 = m2[0]

                # delta-theta between NED (old) and HyperLeda (adopted)
                dtheta_ned = arcsec_between(cat[m1]['RA'], cat[m1]['DEC'], cat[m1]['RA_HYPERLEDA'],
                                            cat[m1]['DEC_HYPERLEDA'])

                # objname_ned_from,objname_ned_to,dtheta_arcsec,comment\n')
                F.write(f'{cat[m1]["OBJNAME_NED"]},{cat[m1]["PGC"]},{cat[m2]["OBJNAME_NED"]},{dtheta_ned:.3f},{cat[m1]["OBJTYPE"]}\n')
                if verbose:
                    print(f'Writing {cat[m1]["OBJNAME"]} (PGC {cat[m1]["PGC"]}) --> {cat[m2]["OBJNAME"]} (PGC {cat[m2]["PGC"]})')

        F.close()

    # Update the input catalog.
    crossids = Table.read(crossidfile, format='csv', comment='#')
    print(f'Read {len(crossids):,d} rows from {crossidfile}')

    newcat = fullcat.copy()

    drop, dropcat = [], []
    for crossid in crossids:
        m1 = np.where(crossid['objname_ned_from'] == newcat['OBJNAME'])[0][0]
        m2 = np.where(crossid['objname_ned_to']== newcat['OBJNAME'])[0][0]
        drop.append(m1)
        dropcat.append(newcat[m1][cols])
        if verbose:
            print(f'Copying {newcat[m1]["OBJNAME"]} (PGC {newcat[m1]["PGC"]}, {newcat[m1]["OBJTYPE"]}) to ' + \
                  f'{newcat[m2]["OBJNAME"]} (PGC {newcat[m2]["PGC"]}, PGC {newcat[m2]["OBJTYPE"]})')
        for col in ['OBJNAME_HYPERLEDA', 'RA_HYPERLEDA', 'DEC_HYPERLEDA', 'DIAM', 'BA', 'PA',
                    'DIAM_LEDA', 'BA_LEDA', 'PA_LEDA', 'ROW_HYPERLEDA', 'PGC']:
            new = newcat[col][m1]
            old = newcat[col][m2]
            if new == '' and old == '':
                raise ValueError('Special case - write me')
            if 'DIAM' in col:
                new = np.max((new, old))
            if (new != '' or new != -99) and (old == '' or old == -99):
                if verbose:
                    print(f'  Replacing {col}: {old} --> {new}')
                # Do not create duplicate PGC or coordinate values...
                newcat[col][m2] = new
                newcat[col][m1] = old
            else:
                if verbose:
                    print(f'  Keeping {col}: {old} (ignoring {new})')
        for col in ['RA', 'DEC']:
            old = newcat[col][m2]
            new = newcat[f'{col}_HYPERLEDA'][m2]
            if verbose:
                print(f'  Replacing {col}: {old} --> {new}')
            newcat[col][m1] = new
        if verbose:
            print()

    drop = np.hstack(drop)
    dropcat = vstack(dropcat)

    print(f'Dropping {len(drop):,d} cross-id errors (all GTrpl and GPair) from the catalog.')
    newcat = newcat[np.delete(np.arange(len(newcat)), drop)]

    # Read and act on the "VI actions" file.
    actionsfile = resources.files('SGA').joinpath('data/SGA2024/SGA2024-vi-actions.csv')
    actions = Table.read(actionsfile, format='csv', comment='#')

    for action in np.unique(actions['action']):
        match action:
            # drop from the sample
            case 'drop':
                obj = actions[action == actions['action']]['objname'].value
                I = np.isin(newcat['OBJNAME'].value, obj)
                if np.sum(I) != len(obj):
                    pdb.set_trace()
                    print(obj[~np.isin(obj, newcat['OBJNAME'][I].value)])
                    raise ValueError('Some objects not found!')
                if verbose:
                    print(f'Action: dropping {len(obj):,d} objects.')
                newcat = newcat[~I]
            # NED coordinates are wrong; adopt HyperLeda
            case 'hyperleda-coords':
                obj = actions[action == actions['action']]['objname'].value
                I = np.isin(newcat['OBJNAME'].value, obj)
                if np.sum(I) != len(obj):
                    pdb.set_trace()
                    print(obj[~np.isin(obj, newcat['OBJNAME'][I].value)])
                    raise ValueError('Some objects not found!')
                if verbose:
                    print(f'Action: adopting HyperLeda coordinates for {len(obj):,d} object(s).')
                for col in ['RA', 'DEC']:
                    newcat[col][I] = newcat[I][f'{col}_HYPERLEDA']


    # resolve close sources / duplicates (but only in the cross-ID fields)
    match_new = match_radec(dropcat['RA'].value, dropcat['DEC'].value, newcat['RA'].value,
                            newcat['DEC'].value, 75./3600., indexlist=True, notself=False)

    I = np.unique(np.hstack(match_new))
    dups = resolve_close(newcat[I], newcat[I], maxsep=1., allow_vetos=True, verbose=verbose, trim=False)
    I = np.isin(newcat['PARENT_ROW'], dups[dups['PRIMARY'] == False]['PARENT_ROW'])
    print(f'Dropping {np.sum(I):,d} close pairs.')
    newcat = newcat[~I]

    # sort by diameter
    srt = np.argsort(np.max((dropcat['DIAM'].value, dropcat['DIAM_LEDA'].value), axis=0))[::-1]
    dropcat = dropcat[srt]

    # add a VI bit; all these systems have been visually checked
    width_arcsec = 75.
    matches = match_radec(dropcat['RA'].value, dropcat['DEC'].value, newcat['RA'].value,
                          newcat['DEC'].value, width_arcsec/3600., indexlist=True, notself=False)
    matches = np.unique(np.hstack(matches))
    newcat['VI'] = np.zeros(len(newcat), bool)
    newcat['VI'][matches] = True

    # Build QA showing the sources at the center of each of the objects dropped
    # (in "dropcat").
    if build_qa:
        jpgdir = os.path.join(sga_dir(), 'parent', 'vi', f'viewer-crossid-errors')
        pngdir = os.path.join(sga_dir(), 'parent', 'vi', f'crossid-errors')

        # make multiple pdfs
        nperpdf = 192
        npdf = int(np.ceil(len(crossids) / nperpdf))

        for ii in range(npdf):
            ss = ii * nperpdf
            ee = (ii + 1) * nperpdf
            if ee > len(dropcat)-1:
                ee = len(dropcat)-1
            pdffile = os.path.join(sga_dir(), 'parent', 'vi', f'vi-crossid-errors-{ss:04}-{ee-1:04}.pdf')

            #multipage_skypatch(dropcat[ss:ee], cat=fullcat, pngsuffix='group', jpgdir=jpgdir,
            multipage_skypatch(dropcat[ss:ee], cat=newcat, width_arcsec=width_arcsec, clip=True,
                               pngsuffix='group', jpgdir=jpgdir, pngdir=pngdir, pdffile=pdffile,
                               verbose=verbose, overwrite=True, add_title=True, cleanup=cleanup)

    #newcat[match_new[np.where(dropcat['OBJNAME'] == 'CGCG 039-044')[0][0]]][cols]
    #newcat[np.flatnonzero(np.core.defchararray.find(newcat['OBJNAME'].value, 'APMUKS') != -1)][cols]

    #m1, m2, _ = match_radec(newcat['RA'], newcat['DEC'], 0.3728, 13.0985, 75./3600.)
    #m1, m2, _ = match_radec(newcat['RA'], newcat['DEC'], newcat[newcat['PGC'] == 680515]['RA'], newcat[newcat['PGC'] == 680515]['DEC'], 120./3600.)
    #m1, m2, _ = match_radec(newcat['RA'], newcat['DEC'], newcat[newcat['PGC'] == 680515]['RA'], newcat[newcat['PGC'] == 680515]['DEC'], 120./3600.)
    #newcat[m1][cols]
    #qa_skypatch(newcat[m1[15]], group=newcat[m1], pngdir='.', jpgdir='.', verbose=True, overwrite=True, width_arcmin=4.)

    #prefix = np.array(list(zip(*np.char.split(newcat['OBJNAME'].value, ' ').tolist()))[0])
    #allprefix = np.array(list(zip(*np.char.split(fullcat['OBJNAME'].value, ' ').tolist()))[0])
    #newcat[match_new[np.where(dropcat['OBJNAME'] == 'ESO 292-IG 010')[0][0]]][cols]
    #fullcat[match_full[np.where(dropcat['OBJNAME'] == 'APMUKS(BJ) B020550.65-13135 ID')[0][0]]][cols]
    #newcat[np.flatnonzero(np.core.defchararray.find(newcat['OBJNAME'].value, 'APMUKS') != -1)][cols]

    return newcat


def remove_by_prefix(fullcat, merger_type=None, build_qa=False, cleanup=True,
                     verbose=False, overwrite=False):
    """After VI, remove merger_types based on their name "prefix" (or reference),
    most of which are ultra-faint or galaxy groups (i.e., not galaxies).

    """
    width_arcsec = 120. # 30.
    ncol, nrow = 1, 1
    clip = True

    if merger_type is not None:
        suffix = merger_type
        qasuffix = merger_type
        allprefix = np.array(list(zip(*np.char.split(fullcat['OBJNAME'].value, ' ').tolist()))[0])
        T = fullcat[fullcat['OBJTYPE'] == merger_type]
        print(f'Analyzing the prefix frequency of {len(T):,d}/{len(fullcat):,d} objects.')
    else:
        suffix = ''
        qasuffix = 'uncommon-prefix'
        T = fullcat.copy()

    prefix = np.array(list(zip(*np.char.split(T['OBJNAME'].value, ' ').tolist()))[0])
    diam = np.max((T['DIAM'].value, T['DIAM_LEDA'].value), axis=0)
    objname = T['OBJNAME'].value

    # do not cut on PGC number, only on diameter
    #pgc = cat['PGC'].value
    pgc = None

    C = Counter(prefix)
    #print(C.most_common())

    match merger_type:
        case 'GTrpl':
            VETO = None
            drop_prefixes = ['AGC', 'HIPASS', '[BWH2007]', '2dFGRS', 'APMUKS(BJ)',
                             'RSCG', 'MCG', 'LGG', 'LCLG', 'IRAS', 'PPS2', 'GALEXMSC',
                             '[KLI2009]', 'DEEP2', '[ALB2009]', 'WBL', 'KTS', 'ARP',
                             'AM', 'V1CG', 'KTG', 'UGC', 'VII', 'VV', 'NSCS', 'WISEA',
                             'CGCG', 'LDCE', 'HDCE', 'USGC', 'UZC-CG', 'ESO',
                             ]
            drop_references = None
        case 'GPair':
            #drop_prefixes = None
            VETO = None
            drop_prefixes = ['[PCM2000]', '[ATS2004]', '[BFH91]', '[H87]', '[PPC2002]', 'ZwCl',
                             '[vvd91a]', '2MASS', '2MASX', '2MASXi', '2MFGC', 'AGC', 'APMBGC',
                             'CGPG', 'Cocoon', 'CSL', 'CTS', 'FCCB', 'FLASH', 'MESSIER', 'GIN',
                             'VPCX', 'WAS', 'TOLOLO', 'SGC', 'KOS', 'PKS', 'PGC1', 'SARS',
                             'MAPS-NGP', 'FOCA', 'LSBG', 'LSBC', 'PGC', 'MRK', 'UM',
                             'UGCA', 'GALEX', 'GALEXMSC', 'VCC', 'USGC', 'MGC', 'LCRS',
                             'I', 'II', 'III', 'IV', 'VI',
                             ]
            drop_references = None
        case _:
            VETO = None
            drop_prefixes = ['WINGS', 'Mr20:[BFW2006]', 'Mr19:[BFW2006]', 'Mr18:[BFW2006]',
                             '[BFW2006]', '[IBG2003]', '[BJG2014]', 'OPH', 'COMAi', 'zCOSMOS',
                             'CDWFS', 'NuSTAR', '4U', '2CXO', '1RXS', 'SSTSL2', '[MHH96]',
                             'NRGs', 'NSA', '[KSP2018]', '[AHH96]', 'NEP', '[MKB2002]',
                             '[PSP2017]', 'ZwCl', '[SAB2007]', 'COSMOS', '1256+27W01',
                             'Subaru-UDG', 'S-CANDELS', 'GOODS', 'TKRS', 'DEEP2', 'VVDS',
                             'GMASS', 'COMBO-17', 'CNOC2', 'RCS', 'CANDELS', 'ACS-GC',
                             'UKIDSS:[WQF2009]', 'UDF:[CBS2006]', 'FDF', 'EDCSN', 'HDF:[WBD96]',
                             'CFRS', 'HDFS:[RVB2005]', 'UDF:[XPM2007]', 'COSMOS2015',
                             'FIREWORKS', 'UDF:[BSK2006]', 'ADBS', 'AKARI', 'Bolocam', '[CIR2010]',
                             'LH610MHz', 'RG', 'CB-19.04116', 'ISO_A2218_54', 'ISO_A2218_70',
                             'HIDEEP', '[MGP2017]', 'Shadowy', 'VLANEP', 'TAFFY2', 'SMM',
                             'Lock', 'SSSG', 'LQAC', 'HSG', 'HIZSS', 'HOLM', '[KLI2009]',
                             'NSCS', 'H-KPAIR', 'KPAIR', 
                             ]
            #drop_prefixes = ['KUG', '2MFGC', 'NGPFG', 'SA', 'MGC', ]
            drop_references = ['MESSIER 087:[SRB2011]', 'ABELL 1656:[BDG95]', 'ABELL 1656:[EF2011]',
                               'NGC 3607:[H2009]', # candidate globular clusters
                               "Stephan's Quintet:[XLC2003]", 'HCG 092:[DUM2012]', 'HCG 092:[KAG2014]',
                               'ABELL 0671:[MK91]', 'ABELL 1656:[GMP83]', 'ABELL 3558:[KMD98]',
                               'ABELL 3558:[MGP94]', 'ABELL 2218:[BOW83]', 'ABELL 2218:[LPS92]',
                               'ABELL 2218:AMI2012', 'GEMS', 'SCOSMOS', 'PKS 0537-441:[HJN2003]',
                               'PKS 0405-12:[PWC2006]', 'PKS 2005-489:[PWC2011]', 'PKS 1614+051:[HBS2015]',
                               'PKS 2155-304:[FFD2016]', 'PKS 1934-63:[RHL2016]', 'PKS 2135-14:[HBC97]',
                               'HS 1700+6416:[SSE2005]',
                               ]

    # optionally build QA
    if build_qa:
        jpgdir = os.path.join(sga_dir(), 'parent', 'vi', f'viewer-{qasuffix}')
        pngdir = os.path.join(sga_dir(), 'parent', 'vi', qasuffix)

        #qa_prefixes = [pre for pre in list(C.keys()) if pre not in drop_prefixes]
        qa_prefixes = [pre for pre in list(C.keys()) if C[pre] <= 60 and pre not in drop_prefixes]

        #qa_prefixes = ['ARP'] ; width_arcsec = 120. ; overwrite = True
        #m1, m2, _ = match_radec(fullcat['RA'], fullcat['DEC'], fullcat[fullcat['OBJNAME'] == 'IC 0186B']['RA'], fullcat[fullcat['OBJNAME'] == 'IC 0186B']['DEC'], 75./3600.)
        #m1, m2, _ = match_radec(fullcat['RA'], fullcat['DEC'], T[prefix == 'MGC']['RA'], T[prefix == 'MGC']['DEC'], 75./3600.)
        #m1, m2, _ = match_radec(fullcat['RA'], fullcat['DEC'], T[prefix == '[BFH91]']['RA'], T[prefix == '[BFH91]']['DEC'], 75./3600.)
        #qa_skypatch(fullcat[m1[3]], group=fullcat[m1], clip=True)
        #fullcat[np.flatnonzero(np.core.defchararray.find(fullcat['OBJNAME'].value, 'ABELL 2247') != -1)][cols]

        #for drop_prefix in ['III']:
        for drop_prefix in qa_prefixes:
        #for drop_prefix in drop_prefixes:
            Idrop = drop_by_prefix(drop_prefix, prefix, pgc=pgc, diam=diam, objname=objname,
                                   VETO=VETO, verbose=False)
            if len(Idrop) == 0:
                print(f'No {suffix} sources with prefix {drop_prefix}')
            else:
                print(f'Building QA for {len(Idrop):,d} {suffix} sources with prefix {drop_prefix}')
                pdffile = os.path.join(sga_dir(), 'parent', 'vi', f'vi-{qasuffix}-{drop_prefix}.pdf')
                multipage_skypatch(T[Idrop], cat=fullcat, width_arcsec=width_arcsec, ncol=ncol, nrow=nrow,
                                   jpgdir=jpgdir, pngdir=pngdir, pngsuffix=f'{qasuffix}-{drop_prefix}',
                                   pdffile=pdffile, clip=clip, verbose=verbose, overwrite=overwrite,
                                   cleanup=cleanup)
        pdb.set_trace()

    I = []
    # drop by prefix (but not if there's a measured diameter)
    if drop_prefixes is not None:
        for drop_prefix in drop_prefixes:
            Idrop = drop_by_prefix(drop_prefix, prefix, pgc=pgc, diam=diam, objname=objname,
                                   VETO=VETO, verbose=False)
            print(f'Dropping {len(Idrop):,d} sources with prefix {drop_prefix}')
            I.append(Idrop)

    # drop by reference
    if drop_references is not None:
        for drop_reference in drop_references:
            Idrop = np.flatnonzero(np.core.defchararray.find(T['OBJNAME'].value, drop_reference) != -1)
            print(f'Dropping {len(Idrop):,d} {suffix} sources with reference {drop_reference}')
            I.append(Idrop)

    if len(I) > 0:
        I = np.unique(np.hstack(I))
        if verbose:
            print(f'Removing {len(I):,d}/{len(T):,d} {suffix} sources.')
            out = fullcat[~np.isin(fullcat['PARENT_ROW'], T[I]['PARENT_ROW'])]
    else:
        out = fullcat

    return out


def resolve_multiples(fullcat, system='GTrpl', build_qa=False, verbose=False, overwrite=False):
    """Remove (most) GTrpl and GPair systems, most of which are actually galaxy
    groups in NED. We want to keep the real pairs and triples, so we can make
    sure we have the individual galaxies making up the system (e.g., all the
    Arp systems).

    """
    from glob import glob

    # Drop objects by prefix (after VI). Only consider objects (of the given
    # prefix) close to the input {GPair,GTrpl} system.
    T = fullcat[(fullcat['OBJTYPE'] == system) * (fullcat['VI'] == False)]
    #T = cat[np.logical_or(cat['OBJTYPE'] == 'GTrpl', cat['OBJTYPE'] == 'GPair')]
    allmatches = match_radec(T['RA'].value, T['DEC'].value, fullcat['RA'].value, fullcat['DEC'].value,
                             75./3600., indexlist=True, notself=False)
    cat = fullcat[np.unique(np.hstack(allmatches))]
    allmatches = match_radec(T['RA'].value, T['DEC'].value, cat['RA'].value, cat['DEC'].value,
                             75./3600., indexlist=True, notself=False)

    match system:
        case 'GTrpl':
            drop_prefixes = None
            #drop_prefixes = [
            #    'Mr20:[BFW2006]', 'Mr19:[BFW2006]', 'Mr18:[BFW2006]', '[BFW2006]',
            #    '[BJG2014]', '[H87]', '[PCM2000]', '[SPS2007]', '3C', 'AKARI', 'APMBGC',
            #    'CB-20.07763', 'HIDEEP', 'III', 'LCRS', 'LCSB',
            #    'MLCG', 'PM2GC', '[IBG2003]', 'SCG', 'SSRS', 'VCC', 'VI', 'WAS',
            #    'FLASH', 'COSMOS', '[SAB2007]', 'DUKST', 'PPS2',
            #]
        case 'GPair':
            # These are mostly very faint and/or high-redshift, and ~none have
            # diamters larger than ~10 arcsec.
            drop_prefixes = None
            #drop_prefixes = [
            #    'GOODS', 'S-CANDELS', 'CANDELS', 'ACS-GC', 'WINGS', 'COSMOS',
            #    'zCOSMOS', 'ADBS', '[SAB2007]', 'Mr20:[BFW2006]', 'Mr19:[BFW2006]',
            #    'Mr18:[BFW2006]', 'EDCSN', 'GEMS', 'GEMS_N1566_05', 'GEMS_N1566_11',
            #    'GEMS_N3783_05', 'SSTSL2', '[BFH91]', '[MKB2002]', '[PSP2017]', '[vvd91a]',
            #    'AKARI', 'Bolocam', 'ABELL', 'FCCB', 'FOCA', 'GALEX', 'GALEXMSC',
            #    'GEMS_N7144_06', 'GIN', 'LQAC', 'LSBC', 'MESSIER', 'MaxBCG', 'NPM1G',
            #    'SCOSMOS160', 'Shadowy', 'SMM', 'TAFFY2', 'USGC', 'VLANEP', 'VPC',
            #    '[IBG2003]', 'Lock', 'MM', 'HDF:[LYF96]', 'HDF:[T2003]', 'CTS', 'PG',
            #    'UDF:[JBM2015]', 'HIZSS', 'HS', 'ISO_A2218_54']
        case _:
            drop_prefixes = None


    prefix = np.array(list(zip(*np.char.split(cat['OBJNAME'].value, ' ').tolist()))[0])
    diam = np.max((cat['DIAM'].value, cat['DIAM_LEDA'].value), axis=0)
    pgc = cat['PGC'].value

    if drop_prefixes is not None:
        I = []
        for drop_prefix in drop_prefixes:
            I.append(drop_by_prefix(drop_prefix, prefix, pgc=pgc, diam=diam, verbose=False))
        if len(I) > 0:
            I = np.unique(np.hstack(I))
            if verbose:
                print(f'Removing {len(I):,d}/{len(cat):,d} {system} centrals and satellites based on prefix.')
                cat = cat[~np.isin(cat['PARENT_ROW'], cat[I]['PARENT_ROW'])]

    # optionally build QA
    if build_qa:
        jpgdir = os.path.join(sga_dir(), 'parent', 'vi', f'viewer-{system}')
        pngdir = os.path.join(sga_dir(), 'parent', 'vi', f'{system}')

        # Remake the group catalogs.
        T = cat[cat['OBJTYPE'] == system]
        #T = cat[np.logical_or(cat['OBJTYPE'] == 'GPair', cat['OBJTYPE'] == 'GTrpl')]
        allmatches = match_radec(T['RA'].value, T['DEC'].value, cat['RA'].value, cat['DEC'].value,
                                 75./3600., indexlist=True, notself=False)

        prefix = np.array(list(zip(*np.char.split(T['OBJNAME'].value, ' ').tolist()))[0])
        allprefix = np.array(list(zip(*np.char.split(cat['OBJNAME'].value, ' ').tolist()))[0])

        C = Counter(prefix)
        C.most_common()
        #Call = Counter(allprefix)
        #Call.most_common()




        #for pre, count in sorted(C.items()):
        testprefix = ['I'] # ['ARK', 'CoD', 'HARO', 'KAZ', 'NPM1G', 'Dwarf', 'PG', 'RB', 'TOLOLO']
        for pre, count in zip(testprefix, [1]*len(testprefix)):
            if (count > 0) * (count < 3):
            #if (count > 1) * (count < 3):
                print(f'Working on prefix {pre} (N={count})')
                I = np.where(pre == prefix)[0]
                for J in I:
                    primary = T[J]
                    group= cat[allmatches[J]]
                    qa_skypatch(primary, group=group, suffix=system, pngdir=pngdir,
                                jpgdir=jpgdir, verbose=True, overwrite=overwrite)
                print()


    return cat


def build_parent(mindiam=15./60., verbose=False, overwrite=False):
    """Build the parent catalog.

    mindiam in arcmin

    """
    catfile = os.path.join(sga_dir(), 'parent', f'SGA2024-parent-nocuts.fits')
    cat = Table(fitsio.read(catfile))#, rows=np.arange(10000)))
    print(f'Read {len(cat):,d} objects from {catfile}')

    # [1] Drop systems with uncommon prefixes (after VI).
    cat = remove_by_prefix(cat, merger_type=None, verbose=verbose, build_qa=False, cleanup=True)

    # [2] Resolve cross-identification errors in NED/HyperLeda.
    cat = resolve_crossid_errors(cat, verbose=False, rebuild_file=False, build_qa=False)

    # [3] Resolve close (1 arcsec) pairs.
    cat = resolve_close(cat, cat, maxsep=1., allow_vetos=True, verbose=False)

    # [4] Visually drop GTrpl and GPair systems without measured diameters and/or PGC numbers.
    cat = remove_by_prefix(cat, merger_type='GTrpl', verbose=verbose, build_qa=False, cleanup=True, overwrite=True)
    cat = remove_by_prefix(cat, merger_type='GPair', verbose=verbose, build_qa=True, cleanup=True, overwrite=True)

    pdb.set_trace()

    ## Resolve 1-arcsec pairs **excluding** GPair and GTrpl systems.
    #I = (fullcat['OBJTYPE'] != 'GPair') * (fullcat['OBJTYPE'] != 'GTrpl')
    #cat = resolve_close(fullcat[I], fullcat[I], maxsep=1., allow_vetos=True, verbose=False)
    #cat = vstack((cat, fullcat[~I]))
    #cat = cat[np.argsort(cat['PARENT_ROW'])]

    # Drop systems with uncommon prefixes (after VI).
    for system in ['GTrpl', 'GPair', None]:
        cat = remove_by_prefixx(cat, system=system, verbose=verbose,
                                build_qa=True, cleanup=True)




    allprefix = np.array(list(zip(*np.char.split(cat['OBJNAME'].value, ' ').tolist()))[0])

    I = cat['OBJTYPE'] == 'GTrpl'
    prefix = np.array(list(zip(*np.char.split(cat[I]['OBJNAME'].value, ' ').tolist()))[0])
    C = Counter(prefix)
    C.most_common()

    J = np.isin(prefix, ['KUG', '2MFGC', 'NGPFG', 'SA', 'MGC', 'VCC', 'WAS', 'CGPG', 'APMBGC', '[H87]', 'LCRS', 'SSRS',
                         'CB-20.07763', '[PCM2000]', 'SCG', '3C', 'FLASH', 'HIDEEP', '[SPS2007]', 'Mr19:[BFW2006]',
                         'COMAi', 'AKARI', 'PM2GC', 'MLCG'])
    multipage_skypatch(cat[I][J], cat=cat, width_arcsec=75., ncol=1, nrow=1,
                       pngsuffix='group', pdffile=f'pairs.pdf', verbose=True, overwrite=False, cleanup=False)


    K = np.isin(allprefix, 'COMAi')
    multipage_skypatch(cat[K], cat=cat, width_arcsec=15., ncol=4, nrow=4, pngsuffix='group', pdffile='comai.pdf', verbose=True, overwrite=False, cleanup=False)


    pdb.set_trace()

    # First, resolve 1-arcsec pairs:
    I = allcat['OBJTYPE'] == 'GPair'
    cat = resolve_close(allcat[I], allcat, maxsep=1., keep_all_mergers=True,
                        allow_vetos=True, verbose=verbose)
    pdb.set_trace()

    #trimcat = resolve_close(trimcat, trimcat, maxsep=3., verbose=True)

    # Re-resolve 1-arcsec pairs because in 'resolve_multiples' we fix the
    # coordinates of some sources, which leads to new overlaps.
    cat = resolve_close(cat, cat, maxsep=1., keep_all_mergers=False, verbose=verbose)
    pdb.set_trace()

    c_cat = SkyCoord(ra=allcat['RA']*u.deg, dec=allcat['DEC']*u.deg)
    indx_cat, sep2d, _ = match_coordinates_sky(c_cat, c_cat, nthneighbor=2)
    #dd = allcat[(sep2d.arcsec < 1.) * (allcat['DIAM'] > 1.)]
    dd = allcat[(sep2d.arcsec < 1.)]
    dd = dd[np.argsort(dd['RA'])]

    pdb.set_trace()

    #W = np.where(allcat['DIAM_LEDA'] > 10.)[0]
    W = np.where((allcat['DIAM_LEDA'] > 5.) * (allcat['DIAM_LEDA'] < 10.))[0]
    srt = np.argsort(allcat['DIAM_LEDA'][W])[::-1]
    bb = allcat[W[srt]]['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_LVD', 'DIAM', 'DIAM_LEDA', 'DIAM_REF', 'MORPH']

    # compare diameters
    pngfile = os.path.join(qadir, 'qa-diamleda.png')
    I = np.where((allcat['DIAM'] > 0.) * (allcat['DIAM_LEDA'] > mindiam))[0]
    print(len(I))
    lim = (-1.7, 3.)
    #lim = (np.log10(mindiam)-0.1, 3.)



    import corner
    from SGA.qa import plot_style
    sns, colors = plot_style(talk=True, font_scale=1.2)

    fig, ax = plt.subplots(figsize=(7, 7))
    xx = np.log10(allcat['DIAM_LEDA'][I])
    yy = np.log10(allcat['DIAM'][I])
    J = ['IRAS' in morph for morph in allcat['MORPH'][I]]
    corner.hist2d(xx, yy, levels=[0.5, 0.75, 0.95, 0.995],
                  bins=100, smooth=True, color=colors[0], ax=ax, # mpl.cm.get_cmap('viridis'),
                  plot_density=True, fill_contours=True, range=(lim, lim),
                  data_kwargs={'color': colors[0], 'alpha': 0.4, 'ms': 4},
                  contour_kwargs={'colors': 'k'},)
    ax.scatter(xx[J], yy[J], s=15, marker='x', color=colors[2])
    #ax.scatter(xx, yy, s=10)
    ax.set_xlabel(r'$\log_{10}$ (Diameter) [HyperLeda]')
    ax.set_ylabel(r'$\log_{10}$ (Diameter) [archive]')
    #ax.set_xlim(lim)
    #ax.set_ylim(lim)
    ax.plot(lim, lim, color='k', lw=2)
    fig.tight_layout()
    print(f'Writing {pngfile}')
    fig.savefig(pngfile)#, bbox_inches='tight')
    plt.close(fig)

    I = np.where(allcat['DIAM'] > mindiam)[0]
    print(f'Trimming to {len(I):,d}/{len(allcat):,d} ({100.*len(I)/len(allcat):.1f}%) ' + \
          f'objects with DIAM>{60.*mindiam:.1f} arcsec.')
    cat = allcat[I]

    srt = np.argsort(cat['DIAM'])[::-1]
    cat[srt]['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_LVD', 'DIAM', 'DIAM_REF', 'MORPH']

    pdb.set_trace()


def qa_parent(nocuts=False, sky=False, size_mag=False):
    """QA of the parent sample.

    """
    from SGA.qa import fig_sky, fig_size_mag

    if nocuts:
        catfile = os.path.join(sga_dir(), 'parent', f'SGA2024-parent-nocuts.fits')
    else:
        pass

    cat = Table(fitsio.read(catfile))#, rows=np.arange(10000)))
    print(f'Read {len(cat):,d} objects from {catfile}')

    ###########
    # need to move this to the merge script
    cat['RA'] = cat['RA_LVD'].value
    cat['DEC'] = cat['DEC_LVD'].value
    I = np.where((cat['RA'] < 0.) * (cat['RA_NED']))[0]
    if len(I) > 0:
        cat['RA'][I] = cat[I]['RA_NED'].value
        cat['DEC'][I] = cat[I]['DEC_NED'].value
    I = np.where((cat['RA'] < 0.) * (cat['RA_HYPERLEDA']))[0]
    if len(I) > 0:
        cat['RA'][I] = cat[I]['RA_HYPERLEDA'].value
        cat['DEC'][I] = cat[I]['DEC_HYPERLEDA'].value
    I = np.sum(cat['RA'] < 0.)
    ###########

    if sky:
        png = os.path.join(qadir, 'qa-sky-parent-nocuts.png')
        #I = cat['DIAM'] > 0.2
        fig_sky(cat, racolumn='RA', deccolumn='DEC', pngfile=png,
                clip_lo=0., clip_hi=300., mloc=50.)

    if size_mag:
        png = os.path.join(qadir, 'qa-sizemag-parent-nocuts.png')
        fig_size_mag(cat, pngfile=png)
    pdb.set_trace()


def build_parent_nocuts(verbose=True):
    """Merge the external catalogs from SGA2024-query-ned.

    """
    import re

    def readit(catalog, version, bycoord=False):
        if bycoord:
            suffix = 'bycoord'
        else:
            suffix = 'byname'
        datafile = os.path.join(sga_dir(), 'parent', 'external', f'NED{suffix}-{catalog}_{version}.fits')
        data = Table(fitsio.read(datafile))
        print(f'Read {len(data):,d} objects from {datafile}')
        return data


    def populate_parent(input_cat, input_basic, verbose=False):
        from SGA.util import parent_datamodel
        parent = parent_datamodel(len(input_cat))
        for col in parent.columns:
            if col in input_cat.columns:
                if verbose:
                    print(f'Populating {col}')
                parent[col] = input_cat[col]
            if col in input_basic.columns:
                if verbose:
                    print(f'Populating {col}')
                parent[col] = input_basic[col]
        return parent


    print('#####')
    print('Input data:')
    custom = read_custom_external(overwrite=True) # always regenerate the FITS file
    lvd = read_lvd()
    nedlvs = read_nedlvs()
    hyper = read_hyperleda()
    print()

    ned_lvd = readit('LVD', version_lvd())
    ned_nedlvs = readit('NEDLVS', version_nedlvs())

    ## For Kim!!
    #basic = get_basic_geometry(ned_nedlvs, galaxy_column='OBJNAME')
    #basic.write('/Users/ioannis/Downloads/NEDgeometry-NEDLVS_20210922_v2.fits', overwrite=True)

    # merge all the ned_hyper catalogs and reset row (to match SGA.io.read_hyperleda())
    ned_hyper_galaxies = readit('HyperLeda-galaxies', version_hyperleda_galaxies())
    ned_hyper_multiples = readit('HyperLeda-multiples', version_hyperleda_multiples())
    ned_hyper_galaxies_coords = readit('HyperLeda-galaxies-coords', f'{version_hyperleda_galaxies()}', bycoord=True)
    ned_hyper_multiples_coords = readit('HyperLeda-multiples-coords', f'{version_hyperleda_multiples()}', bycoord=True)
    ned_hyper_galaxies_coords.remove_columns(['INPUT_POSITION', 'INPUT_RA', 'INPUT_DEC', 'SEPARATION'])
    ned_hyper_multiples_coords.remove_columns(['INPUT_POSITION', 'INPUT_RA', 'INPUT_DEC', 'SEPARATION'])

    ned_hyper_galaxies = vstack((ned_hyper_galaxies, ned_hyper_galaxies_coords))
    indx_hyper, indx_galaxies = match(hyper['OBJNAME'], ned_hyper_galaxies['OBJNAME'])
    #indx_hyper, indx_galaxies = match(hyper['ROW_GALAXIES'], ned_hyper_galaxies['ROW'])
    ned_hyper_galaxies['ROW'][indx_galaxies] = hyper[indx_hyper]['ROW']

    ned_hyper_multiples = vstack((ned_hyper_multiples, ned_hyper_multiples_coords))
    indx_hyper, indx_multiples = match(hyper['OBJNAME'], ned_hyper_multiples['OBJNAME'])
    #indx_hyper, indx_multiples = match(hyper['ROW_MULTIPLES'], ned_hyper_multiples['ROW'])
    ned_hyper_multiples['ROW'][indx_multiples] = hyper[indx_hyper]['ROW']

    ned_hyper = vstack((ned_hyper_galaxies, ned_hyper_multiples))
    ned_hyper = ned_hyper[np.argsort(ned_hyper['ROW'])]

    del ned_hyper_galaxies, ned_hyper_multiples, ned_hyper_galaxies_coords, ned_hyper_multiples_coords

    #match_radec(ned_hyper['RA'], ned_hyper['DEC'], custom['RA'], custom['DEC'], 3./3600.)

    nobj_ned_lvd = len(ned_lvd)
    nobj_ned_nedlvs = len(ned_nedlvs)
    nobj_ned_hyper = len(ned_hyper)

    hyper_noned = hyper[~np.isin(hyper['ROW'], ned_hyper['ROW'])]

    # [0] Preprocess the data.

    # ned_nedlvs - 21 objects are duplicates, apparently because of
    # cross-identification problems in NED. Keep just the first one of each
    # occurrance here (=10 unique objects)
    print()
    print('#####')
    print('ned_nedlvs:')
    col = 'OBJNAME_NED'
    rr, cc = np.unique(ned_nedlvs[col], return_counts=True)
    dd = ned_nedlvs[np.isin(ned_nedlvs[col], rr[cc>1].value)]
    dd = dd[np.argsort(dd[col])]

    basic_dd = get_basic_geometry(dd, galaxy_column='OBJNAME_NED', verbose=verbose)
    #basic_dd.rename_column('GALAXY', 'OBJNAME')
    #basic_dd['RA'] = dd['RA']
    #basic_dd['DEC'] = dd['DEC']
    #basic_dd['Z'] = dd['Z']
    #basic_dd['OBJTYPE'] = dd['OBJTYPE']
    #basic_dd['DIAM_LEDA'] = -99.
    #basic_dd['PGC'] = -99
    #res = resolve_close(ned_nedlvs, basic_dd, maxsep=3.1, objname_column='OBJNAME', keep_all_mergers=False, verbose=True, trim=False)
    #I = np.where(res['PRIMARY'])[0]
    toss = []
    for objname in np.unique(dd[col]):
        I = np.where(ned_nedlvs[col] == objname)[0]
        J = np.where(basic_dd['GALAXY'] == objname)[0]
        this = np.where(basic_dd[J]['DIAM'] > 0.)[0]
        if len(this) == 0:
            toss.append(np.delete(I, np.argmin(ned_nedlvs[I]['ROW'])))
        else:
            this = this[np.argsort(ned_nedlvs[I][this]['ROW'])]
            toss.append(np.delete(I, this[0]))
        #I = I[np.argsort(ned_nedlvs[col][I])]
        #toss.append(I[1:]) # keep the zeroth match
    toss = np.hstack(toss)
    print(f'Removing {len(toss):,d}/{len(ned_nedlvs):,d} ({100.*len(toss)/len(ned_nedlvs):.1f}%) {col} duplicates.')
    ned_nedlvs = ned_nedlvs[np.delete(np.arange(len(ned_nedlvs)), toss)]

    c_nedlvs = SkyCoord(ra=ned_nedlvs['RA']*u.deg, dec=ned_nedlvs['DEC']*u.deg)
    indx_nedlvs, sep2d, _ = match_coordinates_sky(c_nedlvs, c_nedlvs, nthneighbor=2)
    dd = ned_nedlvs[sep2d.arcsec == 0.]
    dd = dd[np.argsort(dd['RA'])]
    basic_dd = get_basic_geometry(dd, galaxy_column='OBJNAME_NED', verbose=verbose)
    radecs = np.char.add(np.round(dd['RA'], 10).astype(str), np.round(dd['DEC'], 10).astype(str))
    ref_radecs = np.char.add(np.round(ned_nedlvs['RA'], 10).astype(str), np.round(ned_nedlvs['DEC'], 10).astype(str))
    toss = []
    for radec in np.unique(radecs):
        I = np.where(radec == ref_radecs)[0]
        J = np.where(radec == radecs)[0]
        this = np.where(basic_dd[J]['DIAM'] > 0.)[0]
        if len(this) == 0:
            toss.append(np.delete(I, np.argmin(ned_nedlvs[I]['ROW'])))
        else:
            this = this[np.argsort(ned_nedlvs[I][this]['ROW'])]
            toss.append(np.delete(I, this[0]))
    toss = np.hstack(toss)
    print(f'Removing {len(toss):,d}/{len(ned_nedlvs):,d} ({100.*len(toss)/len(ned_nedlvs):.1f}%) coordinate duplicates.')
    ned_nedlvs = ned_nedlvs[np.delete(np.arange(len(ned_nedlvs)), toss)]

    #from pydl.pydlutils.spheregroup import spheregroup
    #ingroup, group_mult, firstgroup, nextgroup = spheregroup(ned_nedlvs['RA'], ned_nedlvs['DEC'], 1.5/60.)

    # Toss out non-galaxies in ned_nedlvs. But note:
    # * Need to make sure the individual members of the GGroup systems are
    #   in the final parent sample.
    # https://ned.ipac.caltech.edu/help/ui/nearposn-list_objecttypes?popup=1
    toss = np.where(np.isin(ned_nedlvs['OBJTYPE'], ['QSO', 'Q_Lens', 'G_Lens', '*',
                                                    'Other', 'GGroup'#,
                                                    #'GPair', 'GTrpl'
                                                    ]))[0]
    print(f'Removing {len(toss):,d}/{len(ned_nedlvs):,d} ({100.*len(toss)/len(ned_nedlvs):.1f}%) non-galaxies.')
    ned_nedlvs = ned_nedlvs[np.delete(np.arange(len(ned_nedlvs)), toss)]

    ## ned_hyper - 1 object (WINGSJ125256.27-152110.4) is a duplicate. As
    ## the primary object, it's PGC4777821, but as the alternate object,
    ## it's also [CZ2003]1631C-0295:095 = PGC6729485. In addition, remove
    ## the ~2500 objects not in NED and the ~11k objects resolve to the same
    ## object in NED; choose the first one.
    #warn = np.array(['WARNING' in objnote for objnote in ned_hyper['OBJECT_NOTE']])
    #print(f'Removing {np.sum(warn):,d}/{len(ned_hyper):,d} objects with NED warnings from ned_hyper.')
    #ned_hyper = ned_hyper[~warn]

    #col = 'OBJNAME'
    #rr, cc = np.unique(ned_hyper[col], return_counts=True)
    #dd = ned_hyper[np.isin(ned_hyper[col], rr[cc>1].value)]
    #dd = dd[np.argsort(dd[col])]
    #toss = []
    #for objname in np.unique(dd[col]):
    #    I = np.where(ned_hyper[col] == objname)[0]
    #    I = I[np.argsort(ned_hyper[col][I])]
    #    toss.append(I[1:]) # keep the zeroth match
    #toss = np.hstack(toss)
    #print(f'Removing {len(toss):,d}/{len(ned_hyper):,d} {col} duplicates from ned_hyper.')
    #ned_hyper = ned_hyper[np.delete(np.arange(len(ned_hyper)), toss)]
    print()
    print('ned_hyper:')

    col = 'OBJNAME_NED'
    rr, cc = np.unique(ned_hyper[col], return_counts=True)
    dd = ned_hyper[np.isin(ned_hyper[col], rr[cc>1].value)]
    dd = dd[np.argsort(dd[col])]
    basic_dd = get_basic_geometry(dd, galaxy_column='OBJNAME_NED', verbose=verbose)
    toss = []
    for objname in np.unique(dd[col]):
        I = np.where(ned_hyper[col] == objname)[0]
        J = np.where(basic_dd['GALAXY'] == objname)[0]
        this = np.where(basic_dd[J]['DIAM'] > 0.)[0]
        if len(this) == 0:
            toss.append(np.delete(I, np.argmin(ned_hyper[I]['ROW'])))
        else:
            this = this[np.argsort(ned_hyper[I][this]['ROW'])]
            toss.append(np.delete(I, this[0]))
    toss = np.hstack(toss)
    print(f'Removing {len(toss):,d}/{len(ned_hyper):,d} ({100.*len(toss)/len(ned_hyper):.1f}%) {col} duplicates.')
    ned_hyper = ned_hyper[np.delete(np.arange(len(ned_hyper)), toss)]

    c_hyper = SkyCoord(ra=ned_hyper['RA']*u.deg, dec=ned_hyper['DEC']*u.deg)
    indx_hyper, sep2d, _ = match_coordinates_sky(c_hyper, c_hyper, nthneighbor=2)
    dd = ned_hyper[sep2d.arcsec == 0.]
    dd = dd[np.argsort(dd['RA'])]
    basic_dd = get_basic_geometry(dd, galaxy_column='OBJNAME_NED', verbose=verbose)

    basic_dd.rename_column('GALAXY', 'OBJNAME')
    basic_dd['RA'] = dd['RA']
    basic_dd['DEC'] = dd['DEC']
    basic_dd['Z'] = dd['Z']
    basic_dd['OBJTYPE'] = dd['OBJTYPE']
    basic_dd['DIAM_LEDA'] = -99.
    basic_dd['PGC'] = -99
    res = resolve_close(basic_dd, basic_dd, maxsep=1., objname_column='OBJNAME',
                        keep_all_mergers=False, verbose=False, trim=False)
    toss = np.where(np.isin(ned_hyper['ROW'], dd[~res['PRIMARY']]['ROW']))[0]

    #radecs = np.char.add(np.round(dd['RA'], 10).astype(str), np.round(dd['DEC'], 10).astype(str))
    #ref_radecs = np.char.add(np.round(ned_hyper['RA'], 10).astype(str), np.round(ned_hyper['DEC'], 10).astype(str))
    #toss = []
    #for radec in np.unique(radecs):
    #    I = np.where(radec == ref_radecs)[0]
    #    J = np.where(radec == radecs)[0]
    #    this = np.where(basic_dd[J]['DIAM'] > 0.)[0]
    #    if len(this) == 0:
    #        toss.append(np.delete(I, np.argmin(ned_hyper[I]['ROW'])))
#   #     elif len(this) == 1:
#her#e
#   #         this = this[np.argsort(ned_hyper[I][this]['ROW'])]
    #    else:
    #        this = this[np.argsort(ned_hyper[I][this]['ROW'])]
    #        toss.append(np.delete(I, this[0]))
    #toss = np.hstack(toss)
    print(f'Removing {len(toss):,d}/{len(ned_hyper):,d} ({100.*len(toss)/len(ned_hyper):.1f}%) coordinate duplicates.')
    ned_hyper = ned_hyper[np.delete(np.arange(len(ned_hyper)), toss)]

    # Toss out non-galaxies in ned_hyper. But note:
    # * Need to make sure the individual members of the GGroup systems are
    #   in the final parent sample.
    # * Some objects classified as point sources (*) have SDSS redshifts,
    #   so the classification is wrong (e.g., GAMA743045=SDSSJ141614.97-005648.2)
    # * Also throw out VIRGO01, which incorrectly maps to 'Virgo I Dwarf'.

    # https://ned.ipac.caltech.edu/help/ui/nearposn-list_objecttypes?popup=1
    toss = np.where(np.isin(ned_hyper['OBJTYPE'], ['PofG', '!V*', '!PN', '**', 'GClstr', 'WD*',
                                                   'Red*', '!HII', 'C*', 'PN', '*Ass', 'Blue*',
                                                   '!**', 'SN', '!*', 'Other', 'SNR', '*Cl',
                                                   '!WD*', 'GGroup', 'WR*',
                                                   #'GPair', 'GTrpl',
                                                   'V*', '*', 'HII', 'Nova', 'Neb', 'RfN', '!V*', '!C*',
                                                   'QSO', 'Q_Lens', 'G_Lens']))[0]
    toss = np.hstack((toss, np.where(ned_hyper['OBJNAME'] == 'VIRGO01')[0]))
    print(f'Removing {len(toss):,d}/{len(ned_hyper):,d} ({100.*len(toss)/len(ned_hyper):.1f}%) non-galaxies.')
    ned_hyper = ned_hyper[np.delete(np.arange(len(ned_hyper)), toss)]

    # check
    print()
    print('After basic cuts:')
    for name, cat, norig in zip(['ned_lvd', 'ned_nedlvs', 'ned_hyper'],
                                [ned_lvd, ned_nedlvs, ned_hyper],
                                [nobj_ned_lvd, nobj_ned_nedlvs, nobj_ned_hyper]):
        nobj = len(cat)
        print(f'{name}: {nobj:,d}/{norig:,d} objects')
        for col in ['OBJNAME', 'OBJNAME_NED', 'ROW']:
            assert(len(np.unique(cat[col])) == nobj)
            #rr, cc = np.unique(cat[col], return_counts=True)
            ##print(rr[cc>1])
            #bb = cat[np.isin(cat[col], rr[cc>1].value)]
            #bb = bb[np.argsort(bb[col])]

    # [1] - Match HyperLeda{-altname} to NEDLVS using OBJNAME_NED.
    print()
    print('#####')

    keys = np.array(ned_nedlvs.colnames)
    keys = keys[~np.isin(keys, ['OBJNAME', 'ROW', 'RA', 'DEC'])]
    out1 = join(ned_hyper, ned_nedlvs, keys=keys, table_names=['HYPERLEDA', 'NEDLVS'])

    # round-off
    out1.rename_columns(['RA_HYPERLEDA', 'DEC_HYPERLEDA'], ['RA', 'DEC'])
    out1.remove_columns(['RA_NEDLVS', 'DEC_NEDLVS'])
    out1.rename_columns(['RA', 'DEC', 'Z'], ['RA_NED', 'DEC_NED', 'Z_NED'])
    print(f'Matched {len(out1):,d}/{len(ned_hyper):,d} ({100.*len(out1)/len(ned_hyper):.1f}%) ned_hyper and ' + \
          f'{len(out1):,d}/{len(ned_nedlvs):,d} ({100.*len(out1)/len(ned_nedlvs):.1f}%) ned_nedlvs objects using OBJNAME_NED.')

    basic_out1 = get_basic_geometry(out1, galaxy_column='OBJNAME_NED', verbose=verbose)

    #indx_out, indx_hyper = match(out1['ROW_HYPERLEDA'], hyper['ROW'])
    #out1['OBJNAME_HYPERLEDA'][indx_out] = hyper['OBJNAME'][indx_hyper]
    #out1 = out1[np.argsort(out1['ROW_HYPERLEDA'])]

    parent1 = populate_parent(out1, basic_out1, verbose=verbose)

    indx_parent, indx_hyper = match(parent1['ROW_HYPERLEDA'], hyper['ROW'])
    if verbose:
        for col in ['RA_HYPERLEDA', 'DEC_HYPERLEDA', 'Z_HYPERLEDA']:
            print(f'Populating {col}')
    parent1['RA_HYPERLEDA'][indx_parent] = hyper['RA'][indx_hyper]
    parent1['DEC_HYPERLEDA'][indx_parent] = hyper['DEC'][indx_hyper]
    I = np.where(~np.isnan(hyper['V'][indx_hyper]))[0]
    parent1['Z_HYPERLEDA'][indx_parent[I]] = hyper['V'][indx_hyper[I]] / 2.99e5

    indx_parent, indx_nedlvs = match(parent1['ROW_NEDLVS'], nedlvs['ROW'])
    if verbose:
        for col in ['RA_NEDLVS', 'DEC_NEDLVS', 'Z_NEDLVS']:
            print(f'Populating {col}')
    parent1['RA_NEDLVS'][indx_parent] = nedlvs['RA'][indx_nedlvs]
    parent1['DEC_NEDLVS'][indx_parent] = nedlvs['DEC'][indx_nedlvs]
    I = np.where(~np.isnan(nedlvs['Z'][indx_nedlvs]))[0]
    parent1['Z_NEDLVS'][indx_parent[I]] = nedlvs['Z'][indx_nedlvs[I]]

    for col in ['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_NEDLVS', 'ROW_HYPERLEDA', 'ROW_NEDLVS']:
        assert(len(np.unique(parent1[col])) == len(parent1))
    print()
    print(f'Parent 1: N={len(parent1):,d}')
    #pdb.set_trace()

    # [2] - Add as many of the remaining ned_hyper objects as possible. Special
    # case VIRGO1, which incorrectly matches (in NED) to 'Virgo I Dwarf' rather
    # than 'Virgo I'.
    print()
    print('#####')
    miss_hyper = ned_hyper[np.logical_and(~np.isin(ned_hyper['ROW'], parent1['ROW_HYPERLEDA']),
                                          (ned_hyper['OBJNAME'] != 'VIRGO1'))]
    miss_hyper.rename_columns(['OBJNAME', 'ROW'], ['OBJNAME_HYPERLEDA', 'ROW_HYPERLEDA'])
    miss_hyper.rename_columns(['RA', 'DEC', 'Z'], ['RA_NED', 'DEC_NED', 'Z_NED'])

    print(f'Adding the remaining {len(miss_hyper):,d} objects from ned_hyper which did not name-match ned_nedlvs.')
    basic_miss_hyper = get_basic_geometry(miss_hyper, galaxy_column='OBJNAME_NED', verbose=verbose)

    parent2 = populate_parent(miss_hyper, basic_miss_hyper, verbose=verbose)

    indx_parent, indx_hyper = match(parent2['ROW_HYPERLEDA'], hyper['ROW'])
    if verbose:
        for col in ['RA_HYPERLEDA', 'DEC_HYPERLEDA', 'Z_HYPERLEDA']:
            print(f'Populating {col}')
    parent2['RA_HYPERLEDA'][indx_parent] = hyper['RA'][indx_hyper]
    parent2['DEC_HYPERLEDA'][indx_parent] = hyper['DEC'][indx_hyper]
    I = np.where(~np.isnan(hyper['V'][indx_hyper]))[0]
    parent2['Z_HYPERLEDA'][indx_parent[I]] = hyper['V'][indx_hyper[I]] / 2.99e5

    for col in ['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'ROW_HYPERLEDA']:
        assert(len(np.unique(parent2[col])) == len(parent2))
    print()
    print(f'Parent 2: N={len(parent2):,d}')
    #pdb.set_trace()

    # [3] - Add the rest of the ned_nedlvs objects, being careful about exact
    # duplicates.
    print()
    print('#####')

    parent = vstack((parent1, parent2))

    miss_nedlvs = ned_nedlvs[~np.isin(ned_nedlvs['ROW'], parent['ROW_NEDLVS'])]
    miss_nedlvs.rename_columns(['OBJNAME', 'ROW'], ['OBJNAME_NEDLVS', 'ROW_NEDLVS'])
    miss_nedlvs.rename_columns(['RA', 'DEC', 'Z'], ['RA_NED', 'DEC_NED', 'Z_NED'])
    print(f'Analyzing the remaining {len(miss_nedlvs):,d} ned_nedlvs objects.')

    c_parent = SkyCoord(ra=parent['RA_NED']*u.deg, dec=parent['DEC_NED']*u.deg)
    c_nedlvs = SkyCoord(ra=miss_nedlvs['RA_NED']*u.deg, dec=miss_nedlvs['DEC_NED']*u.deg)
    indx_dup_nedlvs, sep2d, _ = c_parent.match_to_catalog_sky(c_nedlvs)
    indx_dup_parent = np.where(sep2d.arcsec == 0.)[0]
    indx_dup_nedlvs = indx_dup_nedlvs[indx_dup_parent]

    # also remove some duplicates that arose because my NEDLVS query came well
    # before (~July 2024) my HyperLeda-multiples query, and I guess some of the
    # coordinates changed...
    _miss_nedlvs = miss_nedlvs[np.delete(np.arange(len(miss_nedlvs)), indx_dup_nedlvs)]
    dups = _miss_nedlvs[np.isin(_miss_nedlvs['OBJNAME_NED'], parent['OBJNAME_NED'])]['OBJNAME_NED'].value
    indx_dup_nedlvs = np.hstack((indx_dup_nedlvs, np.where(np.isin(miss_nedlvs['OBJNAME_NED'], dups))[0]))

    #dup_parent = parent[indx_dup_parent]
    #dup_parent['OBJNAME_HYPERLEDA', 'OBJNAME_NED', 'OBJNAME_NEDLVS', 'RA_NED', 'DEC_NED'][:10]
    #miss_nedlvs[indx_dup_nedlvs]['OBJNAME_NEDLVS', 'OBJNAME_NED', 'RA_NED', 'DEC_NED'][:10]

    print(f'Removing {len(indx_dup_nedlvs):,d}/{len(miss_nedlvs):,d} ({100.*len(indx_dup_nedlvs)/len(miss_nedlvs):.1f}%) ' + \
          f'ned_nedlvs duplicates (sep=0.0 arcsec) already in parent sample.')
    #parent = parent[np.delete(np.arange(len(parent)), indx_dup_parent)]
    miss_nedlvs = miss_nedlvs[np.delete(np.arange(len(miss_nedlvs)), indx_dup_nedlvs)]

    basic_miss_nedlvs = get_basic_geometry(miss_nedlvs, galaxy_column='OBJNAME_NED', verbose=verbose)

    parent3 = populate_parent(miss_nedlvs, basic_miss_nedlvs, verbose=verbose)

    indx_parent, indx_nedlvs = match(parent3['ROW_NEDLVS'], nedlvs['ROW'])
    if verbose:
        for col in ['RA_NEDLVS', 'DEC_NEDLVS', 'Z_NEDLVS']:
            print(f'Populating {col}')
    parent3['RA_NEDLVS'][indx_parent] = nedlvs['RA'][indx_nedlvs]
    parent3['DEC_NEDLVS'][indx_parent] = nedlvs['DEC'][indx_nedlvs]
    I = np.where(~np.isnan(nedlvs['Z'][indx_nedlvs]))[0]
    parent3['Z_NEDLVS'][indx_parent[I]] = nedlvs['Z'][indx_nedlvs[I]]

    for col in ['OBJNAME_NED', 'OBJNAME_NEDLVS', 'ROW_NEDLVS']:
        assert(len(np.unique(parent3[col])) == len(parent3))
    print()
    print(f'Parent 3: N={len(parent3):,d}')
    #pdb.set_trace()

    parent = vstack((parent, parent3))

    # [4] - Add any outstanding hyper objects with measured diameters.
    print()
    print('#####')

    miss_hyper = hyper_noned[~np.isin(hyper_noned['ROW'], parent['ROW_HYPERLEDA'])]
    miss_hyper.rename_columns(['OBJNAME', 'ROW'], ['OBJNAME_HYPERLEDA', 'ROW_HYPERLEDA'])
    miss_hyper.rename_columns(['RA', 'DEC'], ['RA_HYPERLEDA', 'DEC_HYPERLEDA'])
    miss_hyper['Z_HYPERLEDA'] = np.zeros(len(miss_hyper)) - 99.
    I = np.where(~np.isnan(miss_hyper['V']))[0]
    miss_hyper['Z_HYPERLEDA'][I] = hyper['V'][I] / 2.99e5

    # http://atlas.obs-hp.fr/hyperleda/leda/param/celpos.html
    #I = np.where((0.1*10**miss_hyper['LOGD25'] > 1.) * (miss_hyper['F_ASTROM'] < 1))[0]
    I = np.where((miss_hyper['LOGD25'] > 0.) *
                 (miss_hyper['OBJTYPE'] != 'R') *  # radio source
                 (miss_hyper['OBJTYPE'] != 'PG') * # part of galaxy
                 (miss_hyper['OBJTYPE'] != 'u')    # catalog error
                 )[0]
    print(f'Adding {len(I):,d}/{len(miss_hyper):,d} HyperLeda objects with measured diameters ' + \
          'not in ned_hyper and not already in our catalog.')

    miss_hyper = miss_hyper[I]
    basic_miss_hyper = get_basic_geometry(miss_hyper, galaxy_column='OBJNAME_HYPERLEDA', verbose=verbose)

    parent4 = populate_parent(miss_hyper, basic_miss_hyper, verbose=verbose)

    for col in ['OBJNAME_HYPERLEDA', 'ROW_HYPERLEDA']:
        assert(len(np.unique(parent4[col])) == len(parent4))
    print()
    print(f'Parent 4: N={len(parent4):,d}')
    #pdb.set_trace()

    parent = vstack((parent, parent4))

    # [5] Add LVD.
    print()
    print('#####')
    print(f'Analyzing {len(lvd):,d} LVD objects, of which {len(ned_lvd):,d} ({100.*len(ned_lvd)/len(lvd):.1f}%) are in ned_lvd.')

    # ned_lvd - already in parent sample
    I = np.where(parent['OBJNAME_NED'] != '')[0]
    #oo, cc = np.unique(parent[I]['OBJNAME_NED'], return_counts=True)
    #p2 = parent2[np.isin(parent2['OBJNAME_NED'], oo[cc>1].value)]
    #p3 = parent3[np.isin(parent3['OBJNAME_NED'], oo[cc>1].value)]
    #p2 = p2[np.argsort(p2['OBJNAME_NED'])]
    #p3 = p3[np.argsort(p3['OBJNAME_NED'])]
    #np.diag(arcsec_between(p2['RA_NED'], p2['DEC_NED'], p3['RA_NED'], p3['DEC_NED']))
    indx_parent, indx_lvd = match(parent[I]['OBJNAME_NED'], ned_lvd['OBJNAME_NED'])

    nexisting = len(indx_parent)
    if verbose:
        print(f'Populating ROW_LVD')
    parent['ROW_LVD'][I[indx_parent]] = ned_lvd['ROW'][indx_lvd]
    print(f'Matched {len(indx_lvd):,d}/{len(lvd):,d} ({100.*len(indx_lvd)/len(lvd):.1f}%) ' + \
          'ned_lvd objects to the /existing/ parent sample using OBJNAME_NED.')

    indx_parent2, indx_lvd2 = match(parent['ROW_LVD'][I[indx_parent]], lvd['ROW'])
    if verbose:
        for col in ['OBJNAME_LVD', 'RA_LVD', 'DEC_LVD']:
            print(f'Populating {col}')
    parent['OBJNAME_LVD'][I[indx_parent[indx_parent2]]] = lvd['OBJNAME'][indx_lvd2]
    parent['RA_LVD'][I[indx_parent[indx_parent2]]] = lvd['RA'][indx_lvd2]
    parent['DEC_LVD'][I[indx_parent[indx_parent2]]] = lvd['DEC'][indx_lvd2]
    #parent[I[indx_parent[indx_parent2]]]['OBJNAME_NED', 'OBJNAME_LVD', 'RA_NED', 'DEC_NED', 'RA_LVD', 'DEC_LVD', 'ROW_LVD', 'PGC']

    # ned_lvd - not in parent sample (new)
    miss_lvd = ned_lvd[~np.isin(ned_lvd['ROW'], parent['ROW_LVD'])]
    miss_lvd.rename_columns(['OBJNAME', 'ROW'], ['OBJNAME_LVD', 'ROW_LVD'])
    miss_lvd.rename_columns(['RA', 'DEC', 'Z'], ['RA_NED', 'DEC_NED', 'Z_NED'])
    print(f'Adding {len(miss_lvd):,d}/{len(lvd):,d} ({100.*len(miss_lvd)/len(lvd):.1f}%) ' + \
          '/new/ ned_lvd objects to the parent sample.')

    basic_miss_lvd = get_basic_geometry(miss_lvd, galaxy_column='OBJNAME_LVD', verbose=verbose)

    parent5a = populate_parent(miss_lvd, basic_miss_lvd, verbose=verbose)

    # LVD - not in parent sample (new)
    miss_lvd = lvd[np.logical_and(~np.isin(lvd['ROW'], parent['ROW_LVD']),
                                  ~np.isin(lvd['ROW'], parent5a['ROW_LVD']))]
    miss_lvd.rename_columns(['OBJNAME', 'ROW'], ['OBJNAME_LVD', 'ROW_LVD'])
    miss_lvd.rename_columns(['RA', 'DEC'], ['RA_LVD', 'DEC_LVD'])
    print(f'Adding {len(miss_lvd):,d}/{len(lvd):,d} ({100.*len(miss_lvd)/len(lvd):.1f}%) ' + \
          'new LVD objects to the parent sample.')

    basic_miss_lvd = get_basic_geometry(miss_lvd, galaxy_column='OBJNAME_LVD', verbose=verbose)

    parent5b = populate_parent(miss_lvd, basic_miss_lvd, verbose=verbose)

    parent5 = vstack((parent5a, parent5b))

    # Fill in a bit more info.
    indx_parent, indx_lvd = match(parent5['ROW_LVD'], lvd['ROW'])
    print(f'Matching {len(indx_parent):,d} objects to the original LVD catalog.')
    if verbose:
        for col in ['OBJNAME_LVD (replacing the NED-friendly names)', 'RA_LVD', 'DEC_LVD']:
            print(f'Populating {col}')
    parent5['OBJNAME_LVD'][indx_parent] = lvd['OBJNAME'][indx_lvd] # replace the NED-friendly names
    parent5['RA_LVD'][indx_parent] = lvd['RA'][indx_lvd]
    parent5['DEC_LVD'][indx_parent] = lvd['DEC'][indx_lvd]

    print()
    print(f'Parent 5: N={len(parent5)+nexisting:,d}')
    #pdb.set_trace()

    # [6] include the custom-added objects
    print()
    print('#####')
    print(f'Adding {len(custom):,d} more objects from the custom catalog.')

    custom.rename_column('ROW', 'ROW_CUSTOM')
    basic_custom = get_basic_geometry(custom, galaxy_column='OBJNAME', verbose=verbose)
    parent6 = populate_parent(custom, basic_custom, verbose=verbose)

    print()
    print(f'Parent 6: N={len(parent6):,d}')

    # [7] build the final sample
    parent = vstack((parent, parent5, parent6))

    # sort, check for uniqueness, and then write out
    srt = np.lexsort((parent['ROW_HYPERLEDA'].value, parent['ROW_NEDLVS'].value,
                      parent['ROW_LVD'].value, parent['ROW_CUSTOM'].value))
    parent = parent[srt]

    for col in ['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_NEDLVS', 'OBJNAME_LVD']:
        I = parent[col] != ''
        try:
            assert(len(parent[I]) == len(np.unique(parent[col][I])))
        except:
            print(f'Problem with column {col}!')
            raise ValueError()

    I = parent['PGC'] > 0
    assert(len(parent[I]) == len(np.unique(parent['PGC'][I])))

    #pgc, count = np.unique(parent['PGC'][I], return_counts=True)
    #bb = parent[np.isin(parent['PGC'], pgc[count>1].value)]['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_NEDLVS',
    #                                                        'OBJNAME_LVD', 'RA_NED', 'DEC_NED', 'PGC', 'ROW_HYPERLEDA', 'ROW_NEDLVS', 'ROW_LVD']
    #bb = bb[np.argsort(bb['PGC'])]

    for col in ['ROW_HYPERLEDA', 'ROW_NEDLVS', 'ROW_LVD', 'ROW_CUSTOM']:
        I = parent[col] != -99
        assert(len(parent[I]) == len(np.unique(parent[col][I])))

    print()
    print('#####')
    print(f'Final parent sample: N={len(parent):,d}')

    # Populate OBJNAME, RA, DEC, and Z
    print()
    for dataset in ['NED', 'LVD', 'NEDLVS', 'HYPERLEDA']:
        I = np.where((parent['RA'] == -99.) * (parent[f'RA_{dataset}'] != -99.))[0]
        if len(I) > 0:
            print(f'Adopting {len(I):,d}/{len(parent):,d} ({100.*len(I)/len(parent):.1f}%) RA,DEC values from {dataset}.')
            parent['RA'][I] = parent[I][f'RA_{dataset}']
            parent['DEC'][I] = parent[I][f'DEC_{dataset}']
        I = np.where((parent['OBJNAME'] == '') * (parent[f'OBJNAME_{dataset}'] != ''))[0]
        if len(I) > 0:
            print(f'Adopting {len(I):,d}/{len(parent):,d} ({100.*len(I)/len(parent):.1f}%) OBJNAMEs from {dataset}.')
            # Roughly 4300 objects have "SDSSJ" names rather than "SDSS J".
            # Standardize that here so we match NED-LVS and so that we're
            # NED-friendly.
            if dataset == 'HYPERLEDA':
                objname = nedfriendly_hyperleda(parent[I][f'OBJNAME_{dataset}'])
                # put a space after the prefix, to better match NED
                prefix = np.array([re.findall(r'\d*\D+', obj)[0] for obj in objname])
                newobjname = objname.copy()
                for iobj, (obj, pre) in enumerate(zip(objname, prefix)):
                    if not 'SDSS' in pre:
                        newobjname[iobj] = obj.replace(pre, f'{pre} ')
                parent['OBJNAME'][I] = newobjname
            else:
                parent['OBJNAME'][I] = parent[I][f'OBJNAME_{dataset}']

    for dataset in ['NEDLVS', 'NED', 'HYPERLEDA']:
        I = np.where((parent['Z'] == -99.) * (parent[f'Z_{dataset}'] != -99.))[0]
        if len(I) > 0:
            print(f'Adopting {len(I):,d}/{len(parent):,d} ({100.*len(I)/len(parent):.1f}%) Z values from {dataset}.')
            parent['Z'][I] = parent[I][f'Z_{dataset}']
    print()

    # reset and then prioritize the diameters
    basic_hyper = get_basic_geometry(hyper, galaxy_column='ROW', verbose=verbose)
    basic_ned_hyper = get_basic_geometry(ned_hyper, galaxy_column='ROW', verbose=verbose)
    basic_ned_nedlvs = get_basic_geometry(ned_nedlvs, galaxy_column='ROW', verbose=verbose)
    basic_lvd = get_basic_geometry(lvd, galaxy_column='ROW', verbose=verbose)
    basic_custom = get_basic_geometry(custom, galaxy_column='ROW_CUSTOM', verbose=verbose)

    for col in ['DIAM', 'BA', 'PA', 'MAG']:
        parent[col] = -99.
        parent[f'{col}_REF'] = ''
    parent['BAND'] = ''

    print()
    for basic, row, dataset in zip((basic_custom, basic_lvd, basic_ned_hyper, basic_ned_nedlvs, basic_hyper),
                                   ('ROW_CUSTOM', 'ROW_LVD', 'ROW_HYPERLEDA', 'ROW_NEDLVS', 'ROW_HYPERLEDA'),
                                   ('CUSTOM', 'LVD', 'NED-HyperLeda', 'NEDLVS', 'HyperLeda')):
        for col in ['DIAM', 'BA', 'PA', 'MAG']:
            I = np.where((parent[col] == -99.) * (parent[row] != -99))[0]
            if len(I) > 0:
                indx_parent, indx_basic = match(parent[I][row], basic['GALAXY'])
                G = np.where(basic[indx_basic][col] != -99.)[0]
                if len(G) > 0:
                    print(f'Populating parent with {len(G):,d}/{len(I):,d} {col}s from {dataset}.')
                    parent[col][I[indx_parent[G]]] = basic[indx_basic[G]][col]
                    parent[f'{col}_REF'][I[indx_parent[G]]] = basic[indx_basic[G]][f'{col}_REF']
                    if col == 'MAG':
                        parent['BAND'][I[indx_parent[G]]] = basic[indx_basic[G]]['BAND']
                    #parent[I[indx_parent[G]]]['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_NEDLVS', 'OBJNAME_LVD', 'DIAM', 'DIAM_REF', 'BA', 'BA_REF', 'PA', 'PA_REF']
        print()

        # special columns for HyperLeda geometry
        if dataset == 'HyperLeda':
            I = np.where(parent[row] != -99)[0]
            indx_parent, indx_basic = match(parent[I][row], basic['GALAXY']) # 'GALAXY' here is actually 'ROW'
            for col in ['DIAM', 'BA', 'PA']:
                parent[f'{col}_LEDA'][I[indx_parent]] = basic[indx_basic][col]

    # final statistics
    nobj = len(parent)
    for col in ['DIAM', 'BA', 'PA', 'MAG']:
        N = parent[col] != -99.
        refs = np.unique(parent[N][f'{col}_REF'])
        print(f'N({col}) = {np.sum(N):,d}/{nobj:,d} ({100.*np.sum(N)/nobj:.1f}%)')
        for ref in refs:
            R = parent[N][f'{col}_REF'] == ref
            print(f'  N({ref}) = {np.sum(R):,d}/{np.sum(N):,d} ({100.*np.sum(R)/np.sum(N):.1f}%)')

    parent['PARENT_ROW'] = np.arange(len(parent))

    outfile = os.path.join(sga_dir(), 'parent', f'SGA2024-parent-nocuts.fits')
    print(f'Writing {len(parent):,d} objects to {outfile}')
    parent.write(outfile, overwrite=True)


#def domatch_coord(cat, refcat, rank=0):
#    """Match a catalog and a reference catalog based coordinates.
#
#    """
#    import astropy.units as u
#    from astropy.coordinates import SkyCoord
#    from SGA.util import match
#
#    print('Rank {rank:03d}: Matching based on coordinates.')
#    c_cat = SkyCoord(cat['RA']*u.deg, cat['DEC']*u.deg)
#    c_refcat = SkyCoord(refcat['RA']*u.deg, refcat['DEC']*u.deg)
#    indx_refcat, sep2d, _ = c_cat.match_to_catalog_sky(c_refcat)
#    print(f'Rank {rank:03d}: Mean and max separation between {len(cat)} galaxies is ' \
#          f'{np.mean(sep2d.arcsec):.3f}+/-{np.std(sep2d.arcsec):.3f}, {np.max(sep2d.arcsec):.3f} arcsec.')
#
#    srt = np.argsort(sep2d)[::-1]
#    refcat[indx_refcat[srt]][:10]
#    cat[srt][:10]
#
#    info = hstack((refcat[indx_refcat]['PGC', 'OBJNAME', 'RA', 'DEC', 'LOGD25'], cat['GALAXY', 'RA', 'DEC', 'RHALF', 'SURFACE_BRIGHTNESS_RHALF']))
#    info['SEPARCSEC'] = sep2d.arcsec.astype('f4')


def read_cat(catalog):
    """Wrapper to read one of the known external catalogs.

    """
    match catalog.lower():
        case 'hyperleda':
            cat = read_hyperleda()
        case 'wxsc':
            cat = read_wxsc()
        case 'nedlvs':
            cat = read_nedlvs()
        case 'lvd':
            cat = read_lvd()
        case _:
            raise ValueError(f'Unrecognized catalog name {catalog}')

    return cat


#def _get_ccds(args):
#    """Wrapper for the multiprocessing."""
#    return get_ccds(*args)


def get_ccds(allccds, onegal, width_pixels, pixscale=PIXSCALE, return_ccds=False):
    """Quickly get the CCDs touching this custom brick.  This code is mostly taken
    from legacypipe.runbrick.stage_tims.

    """
    from SGA.coadds import custom_brickname
    from legacypipe.survey import wcs_for_brick, BrickDuck, ccds_touching_wcs

    brickname = f'custom-{custom_brickname(onegal["RA"], onegal["DEC"])}'
    brick = BrickDuck(onegal['RA'], onegal['DEC'], brickname)

    targetwcs = wcs_for_brick(brick, W=float(width_pixels), H=float(width_pixels), pixscale=pixscale)
    I = ccds_touching_wcs(targetwcs, allccds)
    #ccds = survey.ccds_touching_wcs(targetwcs)
    #print(len(I))

    # no CCDs within width_pixels
    if len(I) == 0:
        if return_ccds:
            return Table(), Table()
        else:
            return Table()

    ccds = allccds[I]

    onegal['NCCD'] = len(ccds)
    onegal['FILTERS'] = ''.join(sorted(set(ccds.filter)))

    if return_ccds:
        # convert to an astropy Table so we can vstack
        _ccds = ccds.to_dict()
        ccds = Table()
        for key in _ccds.keys():
            ccds[key.upper()] = _ccds[key]

        ccds = ccds['RA', 'DEC', 'CAMERA', 'EXPNUM', 'PLVER', 'CCDNAME', 'FILTER']
        #ccds['GALAXY'] = [galaxy]
        ccds['ROW'] = onegal['ROW']

        return ccds, Table(onegal)
    else:
        return Table(onegal)


def in_footprint(cat, allccds, radius=1., width_pixels=152, bands=BANDS, comm=None, mp=1):
    """Find which objects are in the given survey footprint based on positional
    matching with a very generous (1 deg) search radius.

    radius in degrees

    """
    from SGA.util import weighted_partition, match_to

    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size

    cat['NCCD'] = np.zeros(len(cat), int)
    cat['FILTERS'] = np.zeros(len(cat), '<U4')

    if rank == 0:
        t0 = time.time()

        # I, which is a list of len(cat), is the variable-length of indices into
        # allccds of the matches, or None if no match (which we filter out).
        indx_ccds = match_radec(cat['RA'], cat['DEC'], allccds.ra, allccds.dec,
                                radius, indexlist=True)

        indx_cat = []
        nccdperobj = []
        for icat, val in enumerate(indx_ccds):
            if val is not None:
                indx_cat.append(icat)
                nccdperobj.append(len(indx_ccds[icat]))
        print(f'Rank {rank:03d}: Found {len(indx_cat):,d}/{len(cat):,d} objects with at least one CCD within {radius} deg.')

        groups = weighted_partition(nccdperobj, size)
    else:
        indx_cat = []
        indx_ccds = []
        groups = [np.array([])]

    # broadcast the work to the other ranks
    if comm:
        indx_cat = comm.bcast(indx_cat, root=0)
        indx_ccds = comm.bcast(indx_ccds, root=0)
        groups = comm.bcast(groups, root=0)

    # now perform a more refined search for each matching object

    fcat = []
    #ccds = []
    for icat, indx in enumerate(groups[rank]):
        catindx = indx_cat[indx]
        onegal = cat[catindx]
        if icat % len(groups[rank]) == 0 or icat+1 == len(groups[rank]):
            print(f'Rank {rank:03d}: Working on galaxy: {icat+1:,d}/{len(groups[rank])}')
        #print(f'Rank {rank:03d} working on galaxy: {onegal["GALAXY"]}')
        one_fcat = get_ccds(allccds[indx_ccds[catindx]], onegal, width_pixels,
                            pixscale=PIXSCALE, return_ccds=False)
        fcat.append(one_fcat)

    if len(fcat) > 0:
        fcat = vstack(fcat)

    #mpargs = []
    #for icat, catindx in enumerate(M):
    #    mpargs.append([icat, allccds[I[catindx]], cat[catindx], width_pixels, PIXSCALE])
    #
    #if mp > 1:
    #    import multiprocessing
    #    with multiprocessing.Pool(mp) as P:
    #        out = P.map(_get_ccds, mpargs)
    #else:
    #    out = [_get_ccds(_mpargs) for _mpargs in mpargs]
    #out = list(zip(*out))
    #
    #ccds = out[0]
    #fcat = out[1]
    #if len(ccds) > 0:
    #    ccds = vstack(ccds)
    #    fcat = vstack(fcat)
    #print(f'Final sample: {len(fcat):,d}/{len(indx_cat):,d} objects and {len(ccds):,d} CCDs.')
    #return fcat, ccds

    if comm:
        fcat = comm.gather(fcat, root=0)

    # sort and return
    if rank == 0:
        fcat = vstack(fcat)

        print(f'Rank {rank:03d}: Final sample: {len(fcat):,d}/{len(indx_cat):,d} objects.')

        fcat = fcat[match_to(fcat['ROW'], cat['ROW'])]
        print(f'Rank {rank:03d}: Total time: {(time.time()-t0)/60.:.3f} min')
        return fcat


def main():
    """Main wrapper

    """
    import argparse

    regions = ['north', 'south']
    catalogs = ['HyperLeda', 'NEDLVS', 'WXSC', 'LVD'] # 'HECATE', 'Z0MGS']

    parser = argparse.ArgumentParser()
    parser.add_argument('--build-parent-nocuts', action='store_true', help='Merge the catalogs retrieved by SGA2024-query-ned into the parent-nocuts catalog.')
    parser.add_argument('--build-parent', action='store_true', help='Merge the catalogs retrieved by SGA2024-query-ned.')
    parser.add_argument('--qa-parent', action='store_true', help='Build QA.')
    parser.add_argument('--mindiam', default=15./60., type=float, help='Minimum diameter for selecting parent sample.')
    parser.add_argument('--region', choices=regions, type=str, nargs='*', help='Region to pass to --in-footprint.')
    parser.add_argument('--catalog', choices=catalogs, type=str, help='External catalog to pass to --in-footprint.')
    parser.add_argument('--in-footprint', action='store_true', help='Match the various external catalogs to the CCDs files.')
    parser.add_argument('--verbose', action='store_true', help='Be verbose.')
    parser.add_argument('--overwrite', action='store_true', help='Overwrite existing files.')
    parser.add_argument('--mp', default=1, type=int, help='Number of multiprocessing processes per MPI rank.')
    args = parser.parse_args()

    # https://docs.nersc.gov/development/languages/python/parallel-python/#use-the-spawn-start-method
    if args.mp > 1 and 'NERSC_HOST' in os.environ:
        import multiprocessing
        multiprocessing.set_start_method('spawn')

    try:
        from mpi4py import MPI
        from mpi4py.util import pkl5
        #comm = MPI.COMM_WORLD
        comm = pkl5.Intracomm(MPI.COMM_WORLD)
    except ImportError:
        comm = None

    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size


    if args.build_parent_nocuts:
        build_parent_nocuts()

    if args.build_parent:
        build_parent(mindiam=args.mindiam, overwrite=args.overwrite, verbose=args.verbose)

    if args.qa_parent:
        qa_parent(nocuts=True, size_mag=True)
        #qa_parent(nocuts=True, sky=True)



    #basedir = sga_dir()
    #footdir = os.path.join(basedir, 'parent', 'in-footprint')
    #if args.in_footprint:
    #    from legacypipe.runs import get_survey
    #
    #    if not os.path.isdir(footdir):
    #        os.makedirs(footdir)
    #
    #    for region in np.atleast_1d(args.region):
    #        survey = get_survey(region)#, allbands=BANDS)
    #        _ = survey.get_ccds_readonly()
    #
    #        for catalog in np.atleast_1d(args.catalog):
    #            outfile = os.path.join(footdir, f'{catalog}-{region}.fits')
    #            qafile = os.path.join(footdir, f'qa-{catalog}-{region}.png')
    #            if rank == 0:
    #                print(f'Rank {rank:03d}: Working on region={region} and catalog={catalog}')
    #                fullcat = read_cat(catalog)
    #            else:
    #                fullcat = Table()
    #
    #            if comm:
    #                fullcat = comm.bcast(fullcat, root=0)
    #
    #            if not os.path.isfile(outfile) or args.overwrite:
    #                cat = in_footprint(fullcat, allccds=survey.ccds, bands=None, comm=comm, mp=1)#=args.mp)
    #                # write out
    #                if rank == 0:
    #                    nccds = np.sum(cat['NCCD'])
    #                    print(f'Rank {rank:03d}: Writing {len(cat):,d} objects with {nccds:,d} CCDs to {outfile}')
    #                    #print(f'Writing {len(cat):,d} objects and {len(ccds):,d} CCDs to {outfile}')
    #                    fitsio.write(outfile, cat.as_array(), extname='CATALOG', clobber=True)
    #                    #fitsio.write(outfile, ccds.as_array(), extname='CCDS')
    #            else:
    #                if rank == 0:
    #                    cat = Table(fitsio.read(outfile, ext='CATALOG'))
    #                    #ccds = Table(fitsio.read(outfile, ext='CCDS'))
    #                    print(f'Rank {rank:03d}: Read {len(cat):,d} objects from {outfile}')
    #
    #            if rank == 0:
    #                # simple QA
    #                import matplotlib.pyplot as plt
    #                import seaborn as sns
    #
    #                if len(fullcat) < 1e3:
    #                    s = 20
    #                    markerscale = 1
    #                else:
    #                    s = 1
    #                    markerscale = 10
    #                fig, ax = plt.subplots(figsize=(8, 6))
    #                ax.scatter(fullcat['RA'], fullcat['DEC'], s=s, color='gray')
    #                for bands in sorted(set(cat['FILTERS'])):
    #                    I = cat['FILTERS'] == bands
    #                    ax.scatter(cat['RA'][I], cat['DEC'][I], s=s, alpha=0.7, label=f'{bands} (N={np.sum(I):,d})')
    #                ax.set_xlabel('RA')
    #                ax.set_ylabel('Dec')
    #                ax.set_xlim(360., 0.)
    #                ax.set_ylim(-90., 90.)
    #                #ax.invert_xaxis()
    #                ax.legend(fontsize=10, ncols=2, markerscale=markerscale, loc='lower left')
    #                fig.tight_layout()
    #                fig.savefig(qafile)
    #                print(f'Rank {rank:03d}: Wrote {qafile}')
    #

    #if args.merge:
    #    # What's missing?
    #
    #    #region = 'south'
    #    region = 'north'
    #    cat = Table(fitsio.read(os.path.join(footdir, f'LVD-{region}.fits'), 'CATALOG'))
    #    cat = add_pgc(cat, 'lvd')
    #
    #    cat = Table(fitsio.read(os.path.join(footdir, f'NEDLVS-{region}.fits'), 'CATALOG'))
    #    refcat = Table(fitsio.read(os.path.join(footdir, f'HyperLeda-{region}.fits'), 'CATALOG'))
    #
    #    #info = domatch_pgc(cat, refcat)
    #    info = domatch_coord(cat, refcat)
    #
    #    cat = read_lvd()
    #    cc = cat[cat['PGC'] == 0]['NAME', 'RA', 'DEC', 'CONFIRMED_REAL', 'REF_STRUCTURE']
    #    for oo in cc:
    #        print(f'{oo["NAME"]} {oo["RA"]}d {oo["DEC"]}d 0.5')
    #
    #    ned = read_nedlvs()
    #    I = [indx for indx, gg in enumerate(ned['OBJNAME']) if 'bootes' in gg.lower()]


if __name__ == '__main__':
    main()
