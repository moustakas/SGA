#!/usr/bin/env python

"""Code to build the parent SGA2024 sample based on a combination of internal and external catalogs.

SGA2024-build-parent --build-parent-nocuts
SGA2024-build-parent --build-parent --mindiam 0.25

SGA2024-build-parent --in-footprint --mp 128 --overwrite

Or, on my laptop:
/opt/homebrew/bin/python3 ~/code/SGA/bin/SGA2024/SGA2024-build-parent --merge

"""
import os, time, pdb
import numpy as np
import numpy.ma as ma
import fitsio
from astropy.table import Table, vstack, hstack, join
import astropy.units as u
from astropy.coordinates import SkyCoord, match_coordinates_sky
import matplotlib.pyplot as plt

from astrometry.util.starutil_numpy import arcsec_between
from astrometry.libkd.spherematch import match_radec

from SGA.coadds import PIXSCALE, BANDS
from SGA.util import get_basic_geometry, match, match_to
from SGA.io import (sga_dir, get_raslice, read_hyperleda, version_hyperleda,
                    nedfriendly_hyperleda, read_nedlvs, version_nedlvs,
                    read_lvd, version_lvd, nedfriendly_lvd, version_custom_external,
                    read_custom_external)


qadir = os.path.join(sga_dir(), 'parent', 'qa')
if not os.path.isdir(qadir):
    os.makedirs(qadir)


def parent_datamodel(nobj):
    """Initialize the data model for the parent-nocuts sample.

    """
    parent = Table()
    parent['OBJNAME'] = np.zeros(nobj, '<U30')
    parent['OBJNAME_NED'] = np.zeros(nobj, '<U30')
    parent['OBJNAME_HYPERLEDA'] = np.zeros(nobj, '<U30')
    parent['OBJNAME_NEDLVS'] = np.zeros(nobj, '<U30')
    parent['OBJNAME_LVD'] = np.zeros(nobj, '<U30')
    parent['OBJTYPE'] = np.zeros(nobj, '<U6')
    parent['MORPH'] = np.zeros(nobj, '<U20')
    parent['BASIC_MORPH'] = np.zeros(nobj, '<U40')

    parent['RA'] = np.zeros(nobj, 'f8') -99.
    parent['DEC'] = np.zeros(nobj, 'f8') -99.
    parent['RA_NED'] = np.zeros(nobj, 'f8') -99.
    parent['DEC_NED'] = np.zeros(nobj, 'f8') -99.
    parent['RA_HYPERLEDA'] = np.zeros(nobj, 'f8') -99.
    parent['DEC_HYPERLEDA'] = np.zeros(nobj, 'f8') -99.
    parent['RA_NEDLVS'] = np.zeros(nobj, 'f8') -99.
    parent['DEC_NEDLVS'] = np.zeros(nobj, 'f8') -99.
    parent['RA_LVD'] = np.zeros(nobj, 'f8') -99.
    parent['DEC_LVD'] = np.zeros(nobj, 'f8') -99.

    parent['Z'] = np.zeros(nobj, 'f8') -99.
    parent['Z_NED'] = np.zeros(nobj, 'f8') -99.
    parent['Z_HYPERLEDA'] = np.zeros(nobj, 'f8') -99.
    parent['Z_NEDLVS'] = np.zeros(nobj, 'f8') -99.

    parent['PGC'] = np.zeros(nobj, '<i8') -99
    parent['ESSENTIAL_NOTE'] = np.zeros(nobj, '<U80')

    parent['MAG'] = np.zeros(nobj, 'f4') -99.
    parent['MAG_REF'] = np.zeros(nobj, '<U7')
    parent['BAND'] = np.zeros(nobj, '<U1')
    parent['DIAM'] = np.zeros(nobj, 'f4') -99.
    parent['DIAM_REF'] = np.zeros(nobj, '<U7')
    parent['BA'] = np.zeros(nobj, 'f4') -99.
    parent['BA_REF'] = np.zeros(nobj, '<U7')
    parent['PA'] = np.zeros(nobj, 'f4') -99.
    parent['PA_REF'] = np.zeros(nobj, '<U7')

    parent['DIAM_LEDA'] = np.zeros(nobj, 'f4') -99.
    parent['BA_LEDA'] = np.zeros(nobj, 'f4') -99.
    parent['PA_LEDA'] = np.zeros(nobj, 'f4') -99.

    parent['ROW_HYPERLEDA'] = np.zeros(nobj, '<i8') -99
    parent['ROW_NEDLVS'] = np.zeros(nobj, '<i8') -99
    parent['ROW_LVD'] = np.zeros(nobj, '<i8') -99
    parent['ROW_CUSTOM'] = np.zeros(nobj, '<i8') -99

    return parent


def choose_primary(group, verbose=False, keep_all_mergers=False):
    """Choose the primary member of a group.

    keep_all is helpful for returning a group catalog without dropping any
    sources.

    if keep_all_mergers=True then always keep {GPair,GTrpl} sources, even
      if they do not have a diameter.

    if allow_vetos=True then do not drop systems that are in a 'veto' list,
      even if they overlap with another source.

    """
    if keep_all_mergers:
        IM = np.logical_or(group['OBJTYPE'] == 'GPair', group['OBJTYPE'] == 'GTrpl')
        IG = group['OBJTYPE'] == 'G'
    else:
        IG = np.logical_or(group['OBJTYPE'] == 'G', group['OBJTYPE'] == 'GPair', group['OBJTYPE'] == 'GTrpl')

    #IG = np.logical_or.reduce((group['OBJTYPE'] == 'G', group['OBJTYPE'] == 'GPair', group['OBJTYPE'] == 'GTrpl'))
    ID = np.vstack((group['DIAM'] != -99., group['DIAM_LEDA'] != -99.)).T
    IZ = group['Z'] != -99.
    IS = group['SEP'] == 0.

    mask1 = IG * np.any(ID, axis=1)      # objtype=G and any diameter
    mask2 = IG * np.all(ID, axis=1)      # objtype=G and both diameters
    mask3 = IG * np.all(ID, axis=1) * IZ # objtype=G, both diameters, and a redshift
    mask4 = np.all(ID, axis=1) * IZ      # both diameters and a redshift
    mask5 = np.all(ID, axis=1) * IS      # both diameters and separation=0 (usually PGC is a minimum)
    mask6 = np.all(ID, axis=1)           # both diameters
    mask7 = np.any(ID, axis=1) * IS      # either diameter and separation=0
    mask8 = IS                           # separation=0

    if keep_all_mergers:
        mask0 = IM # objtype={GPair,GTrpl}
        allmasks = (mask0, mask1, mask2, mask3, mask4, mask5, mask6, mask7, mask8)
    else:
        allmasks = (mask1, mask2, mask3, mask4, mask5, mask6, mask7, mask8)

    for mask in allmasks:
        keep = np.where(mask)[0]
        drop = np.where(~mask)[0]
        if len(keep) == 1:
            keep, drop = np.where(mask)[0], np.where(~mask)[0]
            return keep, drop

    print('Warning: cases 1-8 failed; choosing by prefix.')
    print(group['OBJNAME', 'OBJTYPE', 'RA', 'DEC', 'DIAM', 'DIAM_LEDA', 'Z', 'PGC', 'SEP'])
    prefer_prefix = ['NGC', 'UGC', 'IC', 'MCG', 'CGCG', 'I', 'II', 'III', 'IV', 'V', 'VI', 'VII', 'VIII']
    prefix = np.array(list(zip(*np.char.split(group['OBJNAME'].value, ' ').tolist()))[0])
    mask = np.array([pre in prefer_prefix for pre in prefix])
    keep = np.where(mask)[0]
    drop = np.where(~mask)[0]
    if len(keep) == 1:
        keep, drop = np.where(mask)[0], np.where(~mask)[0]
        return keep, drop

    print('Warning: choosing by prefix failed; choosing the first object.')
    print(group['OBJNAME', 'OBJTYPE', 'RA', 'DEC', 'DIAM', 'DIAM_LEDA', 'Z', 'PGC', 'SEP'])
    indx = np.arange(len(group))
    return indx[:1], indx[1:]


def resolve_close(cat, refcat, maxsep=1., keep_all=False, allow_vetos=False,
                  keep_all_mergers=False, trim=True, verbose=False):
    """Resolve close objects.

    maxsep in arcsec

    """
    VETO = [
        '2MASX J15134005+2607307',  # overlaps with 3C 315, but the coordinates for 3C 315 are wrong
    ]

    allmatches = match_radec(cat['RA'].value, cat['DEC'].value,
                             refcat['RA'].value, refcat['DEC'].value,
                             maxsep/3600., indexlist=True, notself=False)
    nobj = 0
    allindx_cat, allindx_refcat = [], []
    for iobj, onematch in enumerate(allmatches):
        nmatch = len(onematch)
        if nmatch > 1:
            nobj += nmatch
            allindx_cat.append(iobj)
            allindx_refcat.append(onematch)

    if verbose:
        maxname = len(max(refcat['OBJNAME'], key=len))
        maxtyp = len(max(refcat['OBJTYPE'], key=len))

    refcat['GROUP_ID'] = np.zeros(len(refcat), np.int32) - 99
    refcat['PRIMARY'] = np.ones(len(refcat), bool)
    refcat['NGROUP'] = np.ones(len(refcat), np.int16)
    refcat['SEPARATION'] = np.zeros(len(refcat), 'f4')
    refcat['DONE'] = np.zeros(len(refcat), bool)

    for igroup, (indx_cat, indx_refcat) in enumerate(zip(allindx_cat, allindx_refcat)):
        if verbose and (igroup % 500 == 0):
            print(f'Working on group {igroup+1:,d}/{len(allindx_refcat):,d}')
        indx_cat = np.array(indx_cat)
        indx_refcat = np.array(indx_refcat)
        if np.all(refcat['DONE'][indx_refcat]):
            continue

        group = refcat[indx_refcat]
        dtheta = arcsec_between(cat[indx_cat]['RA'], cat[indx_cat]['DEC'], # [arcsec]
                                 group['RA'], group['DEC'])
        group['SEP'] = dtheta.astype('f4')
        ngroup = len(group)

        refcat['GROUP_ID'][indx_refcat] = igroup
        refcat['NGROUP'][indx_refcat] = ngroup
        refcat['SEPARATION'][indx_refcat] = dtheta
        refcat['DONE'][indx_refcat] = True

        if keep_all:
            primary = np.arange(ngroup)
            drop = np.array([])
        else:
            primary, drop = choose_primary(group, verbose=verbose, keep_all_mergers=keep_all_mergers)
            refcat['PRIMARY'][indx_refcat[drop]] = False

        #if verbose and (np.any(group['OBJTYPE'] == 'GPair') or np.any(group['OBJTYPE'] == 'GTrpl')):
        if verbose:
            for ii in primary:
                print('Keep: '+'{name: <{W}}'.format(name=group[ii]["OBJNAME"], W=maxname)+': ' + \
                      '{typ: <{W}}'.format(typ=group[ii]["OBJTYPE"], W=maxtyp)+', ' + \
                      f'PGC={group[ii]["PGC"]}, z={group[ii]["Z"]:.5f}, D={group[ii]["DIAM"]:.2f}, ' + \
                      f'D(LEDA)={group[ii]["DIAM_LEDA"]:.2f} arcmin, (ra,dec)={group[ii]["RA"]:.6f},' + \
                      f'{group[ii]["DEC"]:.6f}')
            for ii in drop:
                print('Drop: '+'{name: <{W}}'.format(name=group[ii]["OBJNAME"], W=maxname)+': ' + \
                      '{typ: <{W}}'.format(typ=group[ii]["OBJTYPE"], W=maxtyp)+', ' + \
                      f'PGC={group[ii]["PGC"]}, z={group[ii]["Z"]:.5f}, D={group[ii]["DIAM"]:.2f}, ' + \
                      f'D(LEDA)={group[ii]["DIAM_LEDA"]:.2f} arcmin, sep={group[ii]["SEP"]:.3f} arcsec')

        # check for vetos
        if allow_vetos:
            for veto in VETO:
                I = np.where(np.isin(refcat[indx_refcat]['OBJNAME'], veto))[0]
                if len(I) == 1:
                    if refcat[indx_refcat[I]]['PRIMARY'] == False:
                        print(f'Restoring vetoed object {refcat[indx_refcat[I[0]]]["OBJNAME"]}')
                        refcat['PRIMARY'][indx_refcat[I]] = True
        if verbose:
            print()

    ugroup = np.unique(refcat["GROUP_ID"])
    ugroup = ugroup[ugroup != -99]

    print(f'Found {len(ugroup):,d} group(s) with ' + \
          f'({np.sum(refcat["GROUP_ID"] != -99):,d}/{len(refcat):,d} ' + \
          f'unique objects) within {maxsep:.1f} arcsec.')

    #check = refcat[refcat['GROUP_ID'] != -99]
    #check = check[np.argsort(check['GROUP_ID'])]
    #drop = refcat[(refcat['GROUP_ID'] != -99) * (refcat['PRIMARY'] == False)]
    #prefix = np.array([objname.split(' ')[0] for objname in drop['OBJNAME']])

    if trim:
        out = refcat[refcat['PRIMARY']]
        out.remove_columns(['GROUP_ID', 'PRIMARY', 'NGROUP', 'SEPARATION', 'DONE'])
        return out
    else:
        return refcat


def qa_skypatch(primary=None, group=None, racol='RA', deccol='DEC', suffix='group',
                pngsuffix=None, objname=None, racenter=None, deccenter=None,
                add_title=True, width_arcmin=2., pngdir='.', jpgdir='.', verbose=False,
                overwrite=False):
    """Build QA which shows all the objects in a ~little patch of sky.

    primary - parent-style catalog
    group (optional) - parent-style catalog with additional systems in the field

    """
    import matplotlib.image as mpimg
    from matplotlib.patches import Circle
    from matplotlib.lines import Line2D
    from urllib.request import urlretrieve
    from astropy.wcs import WCS
    from astropy.io import fits


    if pngsuffix is None:
        pngsuffix = suffix

    def get_wcs(racenter, deccenter, width_arcmin=2., survey='ls'):
        pix = {'ls': 0.262, 'unwise': 2.75}
        pixscale = pix[survey]
        width = int(60. * width_arcmin / pixscale)
        hdr = fits.Header()
        hdr['NAXIS'] = 2
        hdr['NAXIS1'] = width
        hdr['NAXIS2'] = width
        hdr['CTYPE1'] = 'RA---TAN'
        hdr['CTYPE2'] = 'DEC--TAN'
        hdr['CRVAL1'] = racenter
        hdr['CRVAL2'] = deccenter
        hdr['CRPIX1'] = width/2+0.5
        hdr['CRPIX2'] = width/2+0.5
        hdr['CD1_1'] = -pixscale/3600.
        hdr['CD1_2'] = 0.0
        hdr['CD2_1'] = 0.0
        hdr['CD2_2'] = +pixscale/3600.
        wcs = WCS(hdr)
        width = wcs.pixel_shape[0]
        return wcs, width, pixscale


    def get_url(racenter, deccenter, width, layer):
        url = f'https://www.legacysurvey.org/viewer/jpeg-cutout?ra={racenter}&dec=' + \
            f'{deccenter}&width={width}&height={width}&layer={layer}'
        #print(url)
        return url

    bbox = dict(boxstyle='round', facecolor='k', alpha=0.5)
    ref_pixscale = 0.262

    if primary is None and group is None:
        raise ValueError('Must specify group *and/or* primary')

    if primary is not None and group is None:
        group = Table(primary)
    if primary is None and group is not None:
        primary = Table(group[0])

    if verbose:
        cols = ['OBJNAME', 'OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'MORPH', 'DIAM', 'DIAM_LEDA',
                'OBJTYPE', 'RA', 'DEC', 'RA_NED', 'DEC_NED', 'RA_HYPERLEDA', 'DEC_HYPERLEDA',
                'MAG', 'Z', 'PGC', 'PARENT_ROW']
        print(group[cols])
    N = len(group)

    if racenter is None and deccenter is None:
        racenter = primary[racol]
        deccenter = primary[deccol]
    if objname is None:
        objname = primary['OBJNAME']

    outname = objname.replace(' ', '_')
    pngfile = os.path.join(pngdir, f'{outname}-{pngsuffix}.png')
    if os.path.isfile(pngfile) and not overwrite:
        print(f'Output file {pngfile} exists; skipping.')
        return


    # check if the viewer cutout file exists
    surveys = ['ls', 'ls', 'unwise']
    layers = ['ls-dr9', 'ls-dr10', 'unwise-neo7']
    for survey, layer in zip(surveys, layers):
        jpgfile = os.path.join(jpgdir, f'{outname}-{suffix}-{layer}.jpeg')
        if os.path.isfile(jpgfile):
            surveys = [survey]
            layers = [layer]

    for survey, layer in zip(surveys, layers):
        wcs, width, pixscale = get_wcs(racenter, deccenter, survey=survey, width_arcmin=width_arcmin)
        jpgfile = os.path.join(jpgdir, f'{outname}-{suffix}-{layer}.jpeg')
        if os.path.isfile(jpgfile):
            img = mpimg.imread(jpgfile)
        else:
            urlretrieve(get_url(racenter, deccenter, width, layer=layer), jpgfile)
            img = mpimg.imread(jpgfile)
            if np.all(img == 32): # no data
                os.remove(jpgfile)
            else:
                break

    decsort = np.argsort(group[deccol])

    leg_colors = ['red']
    leg_lines = ['-']
    leg_labels = ['Adopted']

    #fig = plt.figure()
    #ax = fig.add_subplot()#projection=wcs)
    fig, ax = plt.subplots(figsize=(8, 8))
    ax.imshow(img, origin='lower')
    for imem, (mem, yoffset) in enumerate(zip(group[decsort], (np.arange(0, N)+0.5) * width / N)):

        label = f'{mem["OBJNAME"]} ({mem["OBJTYPE"]}): D={mem["DIAM"]:.3g}\nPGC {mem["PGC"]}: D(LEDA)={mem["DIAM_LEDA"]:.3g})'

        if imem % 2 == 0:
            xoffset = 0.2 * width
        else:
            xoffset = width - 0.2 * width

        if width-yoffset > width / 2:
            va = 'top'
        else:
            va = 'bottom'

        pix = wcs.wcs_world2pix(mem[racol], mem[deccol], 1)
        if np.abs(pix[1]-yoffset) < int(0.03*width):
            yoffset += int(0.03*width)

        ax.add_artist(Circle((pix[0], width-pix[1]), radius=4.*ref_pixscale/pixscale,
                             facecolor='none', edgecolor='red', lw=2, ls='-', alpha=0.9, clip_on=False))
        ax.annotate('', xy=(pix[0], width-pix[1]), xytext=(xoffset, width-yoffset),
                    annotation_clip=False, arrowprops=dict(
                        facecolor='red', width=3, headwidth=8, shrink=0.01, alpha=0.9))
        ax.annotate(label, xy=(xoffset, width-yoffset), xytext=(xoffset, width-yoffset),
                    va=va, ha='center', color='white', bbox=bbox, fontsize=9,
                    annotation_clip=False)

        if mem['RA_HYPERLEDA'] != -99. and mem['RA'] != mem['RA_HYPERLEDA']:
            hyper_pix = wcs.wcs_world2pix(mem['RA_HYPERLEDA'], mem['DEC_HYPERLEDA'], 1)
            ax.add_artist(Circle((hyper_pix[0], width-hyper_pix[1]), radius=8.*ref_pixscale/pixscale,
                                 facecolor='none', edgecolor='cyan', lw=1, ls='--', alpha=0.5, clip_on=False))
            ax.annotate('', xy=(hyper_pix[0], width-hyper_pix[1]), xytext=(xoffset, width-yoffset),
                        annotation_clip=False, arrowprops=dict(
                            facecolor='cyan', width=1, ls='--', headwidth=4, shrink=0.01, alpha=0.5))
            leg_colors += ['cyan']
            leg_lines += ['--']
            leg_labels += ['HyperLeda']

        if mem['RA_NED'] != -99. and mem['RA'] != mem['RA_NED']:
            ned_pix = wcs.wcs_world2pix(mem['RA_NED'], mem['DEC_NED'], 1)
            ax.add_artist(Circle((ned_pix[0], width-ned_pix[1]), radius=8.*ref_pixscale/pixscale,
                                 facecolor='none', edgecolor='yellow', lw=1, ls='--', alpha=0.5, clip_on=False))
            ax.annotate('', xy=(ned_pix[0], width-ned_pix[1]), xytext=(xoffset, width-yoffset),
                        annotation_clip=False, arrowprops=dict(
                            facecolor='yellow', width=1, ls='--', headwidth=4, shrink=0.01, alpha=0.5))
            leg_colors += ['yellow']
            leg_lines += ['--']
            leg_labels += ['NED']

        if mem['RA_LVD'] != -99. and mem['RA'] != mem['RA_LVD']:
            lvd_pix = wcs.wcs_world2pix(mem['RA_LVD'], mem['DEC_LVD'], 1)
            ax.add_artist(Circle((lvd_pix[0], width-lvd_pix[1]), radius=8.*ref_pixscale/pixscale,
                                 facecolor='none', edgecolor='blue', lw=1, ls='--', alpha=0.5, clip_on=False))
            ax.annotate('', xy=(lvd_pix[0], width-lvd_pix[1]), xytext=(xoffset, width-yoffset),
                        annotation_clip=False, arrowprops=dict(
                            facecolor='blue', width=1, ls='--', headwidth=4, shrink=0.01, alpha=0.5))
            leg_colors += ['blue']
            leg_lines += ['--']
            leg_labels += ['LVD']

    _, uindx = np.unique(leg_labels, return_index=True)
    ax.legend([Line2D([0], [0], color=col, linewidth=2, linestyle=ls)
               for col, ls in zip(np.array(leg_colors)[uindx], np.array(leg_lines)[uindx])],
              np.array(leg_labels)[uindx], loc='upper left', frameon=True, framealpha=0.7,
              fontsize=8)

    ax.invert_yaxis() # JPEG is flipped relative to my FITS WCS
    if add_title:
        ax.set_title(f'{objname} ({racenter:.8f},{deccenter:.8f})')
    ax.axis('off')
    fig.tight_layout()
    fig.savefig(pngfile, bbox_inches=0)#, dpi=200)
    plt.close()
    #print(f'Wrote {pngfile}')

    return pngfile


def drop_by_prefix(drop_prefix, allprefixes, pgc=None, diam=None, verbose=False):
    """Drop sources according to their name prefix. Most/all of these have
    been visually inspected. However, don't drop a source if it has PGC
    number and at least one diameter.

    """
    I = drop_prefix == allprefixes
    if pgc is not None and diam is not None:
        G = (pgc != -99) * (diam != -99.)
        I *= ~G
    I = np.where(I)[0]

    if verbose:
        print(f'Dropping {len(I):,d} object(s) with prefix {drop_prefix}.')
    return I


def resolve_multiples(fullcat, system='GTrpl', verbose=False, overwrite=False):
    """Remove (most) GTrpl and GPair systems, most of which are actually galaxy
    groups in NED. We want to keep the real pairs and triples, so we can make
    sure we have the individual galaxies making up the system (e.g., all the
    Arp systems).

    """
    from glob import glob
    from collections import Counter


    # Create a "working" catalog, but at the end copy over the results to
    # 'fullcat'.
    cols = ['OBJNAME', 'OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'MORPH', 'DIAM', 'DIAM_LEDA',
            'OBJTYPE', 'RA', 'DEC', 'RA_NED', 'DEC_NED', 'RA_HYPERLEDA', 'DEC_HYPERLEDA',
            'MAG', 'Z', 'PGC', 'PARENT_ROW']
    cat = fullcat.copy()#[cols]

    # Read and act on the "VI actions" file.
    actionsfile = os.path.join(sga_dir(), 'parent', 'vi', 'vi-actions.csv')
    actions = Table.read(actionsfile, format='csv', comment='#')

    for action in np.unique(actions['action']):
        match action:
            # drop from the sample
            case 'drop':
                obj = actions[action == actions['action']]['objname'].value
                I = np.isin(cat['OBJNAME'].value, obj)
                Ifull = np.isin(fullcat['OBJNAME'].value, obj)
                if (np.sum(I) != len(obj)) or (np.sum(Ifull) != len(obj)):
                    raise ValueError('Some objects not found!')
                if verbose:
                    print(f'Action: dropping {len(obj):,d} objects.')
                cat = cat[~I]
                fullcat = fullcat[~Ifull]
            # NED coordinates are wrong; adopt HyperLeda
            case 'hyperleda-coords':
                obj = actions[action == actions['action']]['objname'].value
                I = np.isin(cat['OBJNAME'].value, obj)
                Ifull = np.isin(fullcat['OBJNAME'].value, obj)
                if (np.sum(I) != len(obj)) or (np.sum(Ifull) != len(obj)):
                    raise ValueError('Some objects not found!')
                if verbose:
                    print(f'Action: adopting HyperLeda coordinates for {len(obj):,d} object(s).')
                for col in ['RA', 'DEC']:
                    cat[col][I] = cat[I][f'{col}_HYPERLEDA']
                    fullcat[col][Ifull] = fullcat[Ifull][f'{col}_HYPERLEDA']


    # Next, drop objects by prefix (after VI). Only consider objects (of the
    # given prefix) close to the input {GPair,GTrpl} system.
    T = cat[cat['OBJTYPE'] == system]
    allmatches = match_radec(T['RA'].value, T['DEC'].value, cat['RA'].value, cat['DEC'].value,
                             60./3600., indexlist=True, notself=False)
    cat = cat[np.unique(np.hstack(allmatches))]
    allmatches = match_radec(T['RA'].value, T['DEC'].value, cat['RA'].value, cat['DEC'].value,
                             60./3600., indexlist=True, notself=False)

    prefix = np.array(list(zip(*np.char.split(cat['OBJNAME'].value, ' ').tolist()))[0])

    diam = np.max((cat['DIAM'].value, cat['DIAM_LEDA'].value), axis=0)
    match system:
        case 'GTrpl':
            drop_prefixes = [
                'Mr20:[BFW2006]', 'Mr19:[BFW2006]', 'Mr18:[BFW2006]', '[BFW2006]',
                '[BJG2014]', '[H87]', '[PCM2000]', '[SPS2007]', '3C', 'AKARI', 'APMBGC',
                'CB-20.07763', 'HIDEEP', 'III', 'LCRS', 'LCSB', 
                'MLCG', 'PM2GC', '[IBG2003]', 'SCG', 'SSRS', 'VCC', 'VI', 'WAS',
                'FLASH', 'COSMOS', '[SAB2007]', 'DUKST', 'PPS2',
            ]
            ## keep: CGCG, ESO, UGC, NGC, SBS, some of WISEA, AM, 2MASX, III Zw,
            ## B2, MGC, IV Zw, LCSB, VI Zw, DUKST, 2MFGC, II Zw, I Zw, KUG, VII Zw,
            ## V Zw, APMUKS(BJ), VIII Zw, IRAS, IC, MCG, ARP, VV, CGMW
            #drop = []
            #for pre in ['LDCE', 'USGC', 'HDCE', 'UZC-CG',
            #            'NSCS', 'zCOSMOS', 'KTG', 'V1CG', '[YSS2008]',
            #            'LGG', 'LCLG', 'RSCG', 'HOLM', '[BWH2007]',
            #            '[ALB2009]', 'KTS', '[KLI2009]',
            #            'AGC', 'HIPASS', 'GALEXMSC', 'DEEP2', 'WBL',
            #            'PPS2', '[SAB2007]', 'VCC', 'WAS', 'CGPG',
            #            'SCG', 'FLASH', 
            #            'COMAi', 'PM2GC', ]:
            #for pre in ['WISEA', '2MASS', 'SDSS', '2dFGRS', 'GALEXASC']:
            #    I = np.where((pre == prefix) * (T['PGC'] == -99))[0]
            #    if verbose:
            #        print(f'Dropping {len(I):,d} object(s) with prefix {pre} and no PGC number.')
            #for pre in ['NGC', 'IC']:
            #    I = np.where((pre == prefix) * np.array(['group' in objname.lower() for objname in T['OBJNAME']]))[0]
            #    if verbose:
            #        print(f'Dropping {len(I):,d} object(s) with prefix {pre} and GROUP in their name.')

        case 'GPair':
            # These are mostly very faint and/or high-redshift, and ~none have
            # diamters larger than ~10 arcsec.
            drop_prefixes = [
                'GOODS', 'S-CANDELS', 'CANDELS', 'ACS-GC', 'WINGS', 'COSMOS',
                'zCOSMOS', 'ADBS', '[SAB2007]', 'Mr20:[BFW2006]', 'Mr19:[BFW2006]',
                'Mr18:[BFW2006]', 'EDCSN', 'GEMS', 'GEMS_N1566_05', 'GEMS_N1566_11',
                'GEMS_N3783_05', 'SSTSL2', '[BFH91]', '[MKB2002]', '[PSP2017]', '[vvd91a]',
                'AKARI', 'Bolocam', 'ABELL', 'FCCB', 'FOCA', 'GALEX', 'GALEXMSC',
                'GEMS_N7144_06', 'GIN', 'LQAC', 'LSBC', 'MESSIER', 'MaxBCG', 'NPM1G',
                'SCOSMOS160', 'Shadowy', 'SMM', 'TAFFY2', 'USGC', 'VLANEP', 'VPC',
                '[IBG2003]', 'Lock', 'MM', 'HDF:[LYF96]', 'HDF:[T2003]', 'CTS', 'PG',
                'UDF:[JBM2015]', 'HIZSS', 'HS', 'ISO_A2218_54']
        case _:
            drop_prefixes = None

    if drop_prefixes is not None:
        I = []
        for drop_prefix in drop_prefixes:
            I.append(drop_by_prefix(drop_prefix, prefix, pgc=cat['PGC'].value, diam=diam, verbose=False))
        if len(I) > 0:
            I = np.unique(np.hstack(I))
            if verbose:
                print(f'Removing {len(I):,d}/{len(cat):,d} {system} centrals and satellites based on prefix.')
                cat = cat[~np.isin(cat['PARENT_ROW'], cat[I]['PARENT_ROW'])]

    # Now remake the group catalogs.
    T = cat[cat['OBJTYPE'] == system]
    allmatches = match_radec(T['RA'].value, T['DEC'].value, cat['RA'].value, cat['DEC'].value,
                             60./3600., indexlist=True, notself=False)

    prefix = np.array(list(zip(*np.char.split(T['OBJNAME'].value, ' ').tolist()))[0])
    allprefix = np.array(list(zip(*np.char.split(cat['OBJNAME'].value, ' ').tolist()))[0])

    #SSTSL2 J121345.98+024833.9
    ## side hustle - inspect NED01/NED02 pairs
    #I = []
    #for iobj, objname in enumerate(T['OBJNAME']):
    #    J = np.where(np.logical_or(cat['OBJNAME'] == f'{objname} NED01', cat['OBJNAME'] == f'{objname} NED02'))[0]
    #    if len(J) > 1:
    #        I.append(iobj)
    #I = np.hstack(I)
    #T = T[I]

    C = Counter(prefix)
    Call = Counter(allprefix)
    C.most_common()
    Call.most_common()

    # np.flatnonzero(np.core.defchararray.find(cat['OBJNAME'].value, objname) != -1)

    # build QA of all the uncommon prefixes
    jpgdir = os.path.join(sga_dir(), 'parent', 'vi', f'viewer-{system}-uncommon-prefixes')
    if not os.path.isdir(jpgdir):
        os.makedirs(jpgdir)

    pngdir = os.path.join(sga_dir(), 'parent', 'vi', f'{system}-uncommon-prefixes')
    if not os.path.isdir(pngdir):
        os.makedirs(pngdir)

    if overwrite:
        flist = glob(os.path.join(pngdir, '*.png'))
        for onefile in flist:
            os.remove(onefile)

    #for pre, count in sorted(C.items()):
    testprefix = ['I'] # ['ARK', 'CoD', 'HARO', 'KAZ', 'NPM1G', 'Dwarf', 'PG', 'RB', 'TOLOLO']
    for pre, count in zip(testprefix, [1]*len(testprefix)):
        if (count > 0) * (count < 3):
        #if (count > 1) * (count < 3):
            print(f'Working on prefix {pre} (N={count})')
            I = np.where(pre == prefix)[0]
            for J in I:
                primary = T[J]
                group= cat[allmatches[J]]
                qa_skypatch(primary, group=group, suffix=system, pngdir=pngdir,
                            jpgdir=jpgdir, verbose=True, overwrite=overwrite)
            print()

    # fix problems here
    pdb.set_trace()


    #print(f'Removing {len(drop):,d}/{len(T):,d} GTrpl systems (most are galaxy groups).')
    #cat = cat[~np.isin(cat['OBJNAME'], T[drop]['OBJNAME'])]

    ## special-casing
    #match prim:
    #    case 'ESO 012-IG 019':
    #        # NED coordinates are bunk, HyperLeda is good; drop NED02 and keep the GTrpl.
    #        drop.append('ESO 012-IG 016 NED02')
    #        cat['RA'][indx_cat] = cat['RA_HYPERLEDA'][indx_cat]
    #        cat['DEC'][indx_cat] = cat['DEC_HYPERLEDA'][indx_cat]
    #    case _:
    #        drop.append(prim)

    # update fullcat


    return cat


def resolve_crossid_errors(fullcat, verbose=False, rebuild_file=False, build_qa=False):
    """Identify cross-identification errors.

    """
    from matplotlib.backends.backend_pdf import PdfPages
    from matplotlib.image import imread

    # convenience columns
    cols = ['OBJNAME', 'OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'MORPH', 'DIAM', 'DIAM_LEDA',
            'OBJTYPE', 'RA', 'DEC', 'RA_NED', 'DEC_NED', 'RA_HYPERLEDA', 'DEC_HYPERLEDA',
            'MAG', 'Z', 'PGC', 'PARENT_ROW']

    crossidfile = os.path.join(sga_dir(), 'parent', 'vi', 'vi-crossid-errors-1.5arcsec.csv')

    # Read or, optionally, rebuild the cross-id error file.
    if rebuild_file:
        # First, resolve 1-arcsec pairs **excluding** GPair and GTrpl systems.
        I = (fullcat['OBJTYPE'] != 'GPair') * (fullcat['OBJTYPE'] != 'GTrpl')

        cat = resolve_close(fullcat[I], fullcat[I], maxsep=1., allow_vetos=True, verbose=False)
        cat = vstack((cat, fullcat[~I]))
        cat = cat[np.argsort(cat['PARENT_ROW'])]

        # Next, algorithmically identify cross-identification errors.
        # Basically, NED typically associates the properties from HyperLeda
        # (PGC number, diameter, etc.) with the *system* rather than with the
        # appropriate galaxy (which is another entry in NED).
        print(f'Writing {crossidfile}')
        F = open(crossidfile, 'w')
        F.write('objname_ned_from,pgc_from,objname_ned_to,dtheta_arcsec,comment\n')

        #for system in ['GPair']:
        for system in ['GTrpl', 'GPair']:
            M = np.where((cat['OBJTYPE'] == system) * (cat['PGC'] != -99))[0]
            for m1 in M: # [1516:]:
                dtheta = arcsec_between(cat[m1]['RA_HYPERLEDA'], cat[m1]['DEC_HYPERLEDA'], cat['RA'], cat['DEC'])
                # Is there *another* source more than 1 arcsec away which is within
                # 1.5 arcsec of the *HyperLeda* coordinates with PGC==-99?
                m2 = np.where((dtheta < 1.5) * (cat['PGC'] == -99))[0]
                if len(m2) == 0:
                    continue
                elif len(m2) > 1:
                    #qa_skypatch(cat[m1], group=vstack((cat[m1], cat[m2])), pngdir='.', jpgdir='.', verbose=True, overwrite=True)
                    group = cat[m2]
                    group['SEP'] = arcsec_between(cat[m1]['RA_HYPERLEDA'], cat[m1]['DEC_HYPERLEDA'], group['RA'], group['DEC'])
                    primary, drop = choose_primary(group)
                    m2 = m2[primary]

                # If so, copy over the HyperLeda quantities over to the other
                # source.
                m2 = m2[0]

                # delta-theta between NED (old) and HyperLeda (adopted)
                dtheta_ned = arcsec_between(cat[m1]['RA'], cat[m1]['DEC'], cat[m1]['RA_HYPERLEDA'],
                                            cat[m1]['DEC_HYPERLEDA'])

                # objname_ned_from,objname_ned_to,dtheta_arcsec,comment\n')
                F.write(f'{cat[m1]["OBJNAME_NED"]},{cat[m1]["PGC"]},{cat[m2]["OBJNAME_NED"]},{dtheta_ned:.3f},{cat[m1]["OBJTYPE"]}\n')
                if verbose:
                    print(f'Writing {cat[m1]["OBJNAME"]} (PGC {cat[m1]["PGC"]}) --> {cat[m2]["OBJNAME"]} (PGC {cat[m2]["PGC"]})')

        F.close()

    # Update the input catalog.
    crossids = Table.read(crossidfile, format='csv', comment='#')
    print(f'Read {len(crossids):,d} rows from {crossidfile}')

    newcat = fullcat.copy()

    drop, dropcat = [], []
    for crossid in crossids:
        m1 = np.where(crossid['objname_ned_from'] == newcat['OBJNAME'])[0][0]
        m2 = np.where(crossid['objname_ned_to']== newcat['OBJNAME'])[0][0]
        drop.append(m1)
        dropcat.append(newcat[m1][cols])
        if verbose:
            print(f'Copying {newcat[m1]["OBJNAME"]} (PGC {newcat[m1]["PGC"]}, {newcat[m1]["OBJTYPE"]}) to ' + \
                  f'{newcat[m2]["OBJNAME"]} (PGC {newcat[m2]["PGC"]}, PGC {newcat[m2]["OBJTYPE"]})')
        for col in ['OBJNAME_HYPERLEDA', 'RA_HYPERLEDA', 'DEC_HYPERLEDA', 'DIAM', 'BA', 'PA',
                    'DIAM_LEDA', 'BA_LEDA', 'PA_LEDA', 'ROW_HYPERLEDA', 'PGC']:
            new = newcat[col][m1]
            old = newcat[col][m2]
            if new == '' and old == '':
                raise ValueError('Special case - write me')
            if 'DIAM' in col:
                new = np.max((new, old))
            if (new != '' or new != -99) and (old == '' or old == -99):
                if verbose:
                    print(f'  Replacing {col}: {old} --> {new}')
                # Do not create duplicate PGC or coordinate values...
                newcat[col][m2] = new
                newcat[col][m1] = old
            else:
                if verbose:
                    print(f'  Keeping {col}: {old} (ignoring {new})')
        for col in ['RA', 'DEC']:
            old = newcat[col][m2]
            new = newcat[f'{col}_HYPERLEDA'][m2]
            if verbose:
                print(f'  Replacing {col}: {old} --> {new}')
            newcat[col][m1] = new
        if verbose:
            print()

    drop = np.hstack(drop)
    dropcat = vstack(dropcat)

    print(f'Dropping {len(drop):,d} cross-id errors (all GTrpl and GPair) from the catalog.')
    newcat = newcat[np.delete(np.arange(len(newcat)), drop)]

    # Read and act on the "VI actions" file.
    actionsfile = os.path.join(sga_dir(), 'parent', 'vi', 'vi-actions.csv')
    actions = Table.read(actionsfile, format='csv', comment='#')

    for action in np.unique(actions['action']):
        match action:
            # drop from the sample
            case 'drop':
                obj = actions[action == actions['action']]['objname'].value
                I = np.isin(newcat['OBJNAME'].value, obj)
                if np.sum(I) != len(obj):
                    pdb.set_trace()
                    raise ValueError('Some objects not found!')
                if verbose:
                    print(f'Action: dropping {len(obj):,d} objects.')
                newcat = newcat[~I]
            # NED coordinates are wrong; adopt HyperLeda
            case 'hyperleda-coords':
                obj = actions[action == actions['action']]['objname'].value
                I = np.isin(newcat['OBJNAME'].value, obj)
                if np.sum(I) != len(obj):
                    pdb.set_trace()
                    raise ValueError('Some objects not found!')
                if verbose:
                    print(f'Action: adopting HyperLeda coordinates for {len(obj):,d} object(s).')
                for col in ['RA', 'DEC']:
                    newcat[col][I] = newcat[I][f'{col}_HYPERLEDA']


    # drop sources by prefix (based on VI in these merger fields)
    prefix = np.array(list(zip(*np.char.split(newcat['OBJNAME'].value, ' ').tolist()))[0])
    diam = np.max((newcat['DIAM'].value, newcat['DIAM_LEDA'].value), axis=0)
    pgc = newcat['PGC'].value

    drop_prefixes = ['WINGS', 'Mr20:[BFW2006]', 'Mr19:[BFW2006]', 'Mr18:[BFW2006]',
                     '[BFW2006]', ]
    I = []
    for drop_prefix in drop_prefixes:
        I.append(drop_by_prefix(drop_prefix, prefix, pgc=pgc, diam=diam, verbose=False))
    if len(I) > 0:
        I = np.unique(np.hstack(I))
        print(f'Removing {len(I):,d}/{len(newcat):,d} centrals and satellites based on prefix.')
        newcat = newcat[~np.isin(newcat['PARENT_ROW'], newcat[I]['PARENT_ROW'])]

    # resolve close sources / duplicates
    match_new = match_radec(dropcat['RA'].value, dropcat['DEC'].value, newcat['RA'].value,
                            newcat['DEC'].value, 75./3600., indexlist=True, notself=False)

    I = np.unique(np.hstack(match_new))
    dups = resolve_close(newcat[I], newcat[I], maxsep=1., allow_vetos=True, verbose=True, trim=False)
    I = np.isin(newcat['PARENT_ROW'], dups[dups['PRIMARY'] == False]['PARENT_ROW'])
    print(f'Dropping {np.sum(I):,d} close pairs.')
    newcat = newcat[~I]

    match_new = match_radec(dropcat['RA'].value, dropcat['DEC'].value, newcat['RA'].value,
                            newcat['DEC'].value, 75./3600., indexlist=True, notself=False)

    #ewcat[match_new[np.where(dropcat['OBJNAME'] == 'ESO 292-IG 010')[0][0]]][cols]
    #fullcat[match_full[np.where(dropcat['OBJNAME'] == 'APMUKS(BJ) B020550.65-13135 ID')[0][0]]][cols]
    #newcat[np.flatnonzero(np.core.defchararray.find(newcat['OBJNAME'].value, 'APMUKS') != -1)][cols]

    # Build QA.
    if build_qa:
        match_full = match_radec(dropcat['RA'].value, dropcat['DEC'].value, fullcat['RA'].value,
                                 fullcat['DEC'].value, 75./3600., indexlist=True, notself=False)

        jpgdir = os.path.join(sga_dir(), 'parent', 'vi', f'viewer-crossid-errors')
        if not os.path.isdir(jpgdir):
            os.makedirs(jpgdir)
        pngdir = os.path.join(sga_dir(), 'parent', 'vi', f'crossid-errors')
        if not os.path.isdir(pngdir):
            os.makedirs(pngdir)

        # make multiple pdfs
        nperpdf = 48
        npdf = int(np.ceil(len(crossids) / nperpdf))

        #for ii in range(npdf):
        for ii in np.arange(5, 9):
            ss = ii * nperpdf
            ee = (ii + 1) * nperpdf

            pdffile = os.path.join(sga_dir(), 'parent', 'vi', f'vi-crossid-errors-{ss:04}-{ee-1:04}.pdf')

            pngfile = []
            for iobj, drop in enumerate(dropcat[ss:ee]):
                #width = crossid['dtheta_arcsec'] / 60. # [arcmin]
                objname = f'{drop["OBJNAME"]} {drop["OBJTYPE"]}'
                racenter, deccenter = drop['RA'], drop['DEC']
                pngfile.append(
                    qa_skypatch(group=fullcat[match_full[iobj+ss]], pngsuffix='original',
                                add_title=False,
                                objname=objname, racenter=racenter, deccenter=deccenter,
                                pngdir=pngdir, jpgdir=jpgdir, verbose=True, overwrite=True)
                    )
                pngfile.append(
                    qa_skypatch(group=newcat[match_new[iobj+ss]], pngsuffix='fixed',
                                add_title=False,
                                objname=objname, racenter=racenter, deccenter=deccenter,
                                pngdir=pngdir, jpgdir=jpgdir, verbose=False, overwrite=True))
                print()

            pngfile = np.array(pngfile)

            ncol = 4
            nrow = 6
            nperpage = ncol * nrow
            npage = int(np.ceil(len(pngfile) / nperpage))
            allindx = np.arange(len(pngfile))

            pdf = PdfPages(pdffile)

            for ipage in range(npage):
                indx = allindx[ipage*nperpage:(ipage+1)*nperpage]
                fig, ax = plt.subplots(nrow, ncol, figsize=(2*ncol, 2*nrow))
                for iax, xx in enumerate(ax.flat):
                    if iax < len(indx):
                        xx.imshow(imread(pngfile[indx[iax]]), interpolation='None')
                    xx.axis('off')
                #for xx in np.arange(iax, nperpage):
                fig.subplots_adjust(wspace=0., hspace=0., bottom=0.05, top=0.95, left=0.05, right=0.95)
                pdf.savefig(fig, dpi=150)
                plt.close()

            pdf.close()
            print(f'Wrote {pdffile}')

        pdb.set_trace()

        #newcat[match_new[np.where(dropcat['OBJNAME'] == 'CGCG 039-044')[0][0]]][cols]

    return newcat


def build_parent(mindiam=15./60., verbose=False, overwrite=False):
    """Build the parent catalog.

    mindiam in arcmin

    """
    if True:
        catfile = os.path.join(sga_dir(), 'parent', f'SGA2024-parent-nocuts.fits')
        allcat = Table(fitsio.read(catfile))#, rows=np.arange(10000)))
        print(f'Read {len(allcat):,d} objects from {catfile}')

        # First, resolve cross-identification errors in NED/HyperLeda.
        print('Identifying cross-identification errors.')
        cat = resolve_crossid_errors(allcat, verbose=False, rebuild_file=False, build_qa=True)
        pdb.set_trace()

        cat.write(os.path.join(sga_dir(), 'parent', 'junk-parent.fits'), overwrite=True)
    else:
        print('HACK!!!!!!')
        catfile = os.path.join(sga_dir(), 'parent', 'junk-parent.fits')
        print(f'Reading {catfile}')
        if False:
            rows = fitsio.read(catfile, columns='PARENT_ROW')
            I = np.where(np.isin(rows, [6,3183437,4912507,3183438,3,467708,3178270,3178271,2255,3264786,2383404,3960635,2383405,911,19219,3237549]))[0]
            cat = Table(fitsio.read(catfile, rows=I))
        else:
            cat = Table(fitsio.read(catfile))

    #bb = cat[np.isin(cat['OBJNAME'], ['WISEA J212344.58+250427.7', '3C 433'])]
    #bb['RA'][0] = bb['RA_HYPERLEDA'][0]
    #bb['DEC'][0] = bb['DEC_HYPERLEDA'][0]

    # Next, analyze GTrpl and GPair systems.
    cat = resolve_multiples(cat, 'GPair', verbose=verbose, overwrite=overwrite)

    #cat = resolve_multiples(cat, 'GTrpl', verbose=verbose)
    pdb.set_trace()

    # First, resolve 1-arcsec GPair pairs. These are almost all cross-ID
    # errors in NED, where...
    I = cat['OBJTYPE'] == 'GTrpl'
    trimcat = resolve_close(cat[I], cat, maxsep=1., allow_vetos=True, verbose=verbose)

    pdb.set_trace()

    # First, resolve 1-arcsec pairs:
    I = allcat['OBJTYPE'] == 'GPair'
    cat = resolve_close(allcat[I], allcat, maxsep=1., keep_all_mergers=True,
                        allow_vetos=True, verbose=verbose)
    pdb.set_trace()

    #trimcat = resolve_close(trimcat, trimcat, maxsep=3., verbose=True)

    # Re-resolve 1-arcsec pairs because in 'resolve_multiples' we fix the
    # coordinates of some sources, which leads to new overlaps.
    cat = resolve_close(cat, cat, maxsep=1., keep_all_mergers=False, verbose=verbose)
    pdb.set_trace()

    # Resolve galaxy mergers (pairs, triples) as well as close (potentially
    # duplicate) objects.
    for objtype in ['GPair', 'GTrpl']:
        I = allcat['OBJTYPE'] == objtype
        print(f'Resolving {np.sum(I):,d} {objtype}.')
        resolve_close(allcat[I], allcat, maxsep=60., verbose=True)

    pdb.set_trace()

    c_cat = SkyCoord(ra=allcat['RA']*u.deg, dec=allcat['DEC']*u.deg)
    indx_cat, sep2d, _ = match_coordinates_sky(c_cat, c_cat, nthneighbor=2)
    #dd = allcat[(sep2d.arcsec < 1.) * (allcat['DIAM'] > 1.)]
    dd = allcat[(sep2d.arcsec < 1.)]
    dd = dd[np.argsort(dd['RA'])]

    pdb.set_trace()

    #W = np.where(allcat['DIAM_LEDA'] > 10.)[0]
    W = np.where((allcat['DIAM_LEDA'] > 5.) * (allcat['DIAM_LEDA'] < 10.))[0]
    srt = np.argsort(allcat['DIAM_LEDA'][W])[::-1]
    bb = allcat[W[srt]]['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_LVD', 'DIAM', 'DIAM_LEDA', 'DIAM_REF', 'MORPH']

    # compare diameters
    pngfile = os.path.join(qadir, 'qa-diamleda.png')
    I = np.where((allcat['DIAM'] > 0.) * (allcat['DIAM_LEDA'] > mindiam))[0]
    print(len(I))
    lim = (-1.7, 3.)
    #lim = (np.log10(mindiam)-0.1, 3.)



    import corner
    from SGA.qa import plot_style
    sns, colors = plot_style(talk=True, font_scale=1.2)

    fig, ax = plt.subplots(figsize=(7, 7))
    xx = np.log10(allcat['DIAM_LEDA'][I])
    yy = np.log10(allcat['DIAM'][I])
    J = ['IRAS' in morph for morph in allcat['MORPH'][I]]
    corner.hist2d(xx, yy, levels=[0.5, 0.75, 0.95, 0.995],
                  bins=100, smooth=True, color=colors[0], ax=ax, # mpl.cm.get_cmap('viridis'),
                  plot_density=True, fill_contours=True, range=(lim, lim),
                  data_kwargs={'color': colors[0], 'alpha': 0.4, 'ms': 4},
                  contour_kwargs={'colors': 'k'},)
    ax.scatter(xx[J], yy[J], s=15, marker='x', color=colors[2])
    #ax.scatter(xx, yy, s=10)
    ax.set_xlabel(r'$\log_{10}$ (Diameter) [HyperLeda]')
    ax.set_ylabel(r'$\log_{10}$ (Diameter) [archive]')
    #ax.set_xlim(lim)
    #ax.set_ylim(lim)
    ax.plot(lim, lim, color='k', lw=2)
    fig.tight_layout()
    print(f'Writing {pngfile}')
    fig.savefig(pngfile)#, bbox_inches='tight')
    plt.close(fig)

    I = np.where(allcat['DIAM'] > mindiam)[0]
    print(f'Trimming to {len(I):,d}/{len(allcat):,d} ({100.*len(I)/len(allcat):.1f}%) ' + \
          f'objects with DIAM>{60.*mindiam:.1f} arcsec.')
    cat = allcat[I]

    srt = np.argsort(cat['DIAM'])[::-1]
    cat[srt]['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_LVD', 'DIAM', 'DIAM_REF', 'MORPH']

    pdb.set_trace()


def qa_parent(nocuts=False, sky=False, size_mag=False):
    """QA of the parent sample.

    """
    from SGA.qa import fig_sky, fig_size_mag

    if nocuts:
        catfile = os.path.join(sga_dir(), 'parent', f'SGA2024-parent-nocuts.fits')
    else:
        pass

    cat = Table(fitsio.read(catfile))#, rows=np.arange(10000)))
    print(f'Read {len(cat):,d} objects from {catfile}')

    ###########
    # need to move this to the merge script
    cat['RA'] = cat['RA_LVD'].value
    cat['DEC'] = cat['DEC_LVD'].value
    I = np.where((cat['RA'] < 0.) * (cat['RA_NED']))[0]
    if len(I) > 0:
        cat['RA'][I] = cat[I]['RA_NED'].value
        cat['DEC'][I] = cat[I]['DEC_NED'].value
    I = np.where((cat['RA'] < 0.) * (cat['RA_HYPERLEDA']))[0]
    if len(I) > 0:
        cat['RA'][I] = cat[I]['RA_HYPERLEDA'].value
        cat['DEC'][I] = cat[I]['DEC_HYPERLEDA'].value
    I = np.sum(cat['RA'] < 0.)
    ###########

    if sky:
        png = os.path.join(qadir, 'qa-sky-parent-nocuts.png')
        #I = cat['DIAM'] > 0.2
        fig_sky(cat, racolumn='RA', deccolumn='DEC', pngfile=png,
                clip_lo=0., clip_hi=300., mloc=50.)

    if size_mag:
        png = os.path.join(qadir, 'qa-sizemag-parent-nocuts.png')
        fig_size_mag(cat, pngfile=png)
    pdb.set_trace()


def build_parent_nocuts(verbose=True):
    """Merge the external catalogs from SGA2024-query-ned.

    """
    def readit(catalog, version, bycoord=False):
        if bycoord:
            suffix = 'bycoord'
        else:
            suffix = 'byname'
        datafile = os.path.join(sga_dir(), 'parent', 'external', f'NED{suffix}-{catalog}_{version}.fits')
        data = Table(fitsio.read(datafile))
        print(f'Read {len(data):,d} objects from {datafile}')
        return data


    def populate_parent(input_cat, input_basic, verbose=False):
        parent = parent_datamodel(len(input_cat))
        for col in parent.columns:
            if col in input_cat.columns:
                if verbose:
                    print(f'Populating {col}')
                parent[col] = input_cat[col]
            if col in input_basic.columns:
                if verbose:
                    print(f'Populating {col}')
                parent[col] = input_basic[col]
        return parent


    print('#####')
    print('Input data:')
    custom = read_custom_external()
    lvd = read_lvd()
    nedlvs = read_nedlvs()
    hyper = read_hyperleda()
    print()

    ned_lvd = readit('LVD', version_lvd())
    ned_nedlvs = readit('NEDLVS', version_nedlvs())

    ned_hyper = readit('HyperLeda', version_hyperleda())
    ned_hyper_coords = readit('HyperLeda-coords', f'{version_hyperleda()}', bycoord=True)
    ned_hyper_coords.remove_columns(['INPUT_POSITION', 'INPUT_RA', 'INPUT_DEC', 'SEPARATION'])

    ned_hyper = vstack((ned_hyper, ned_hyper_coords))
    del ned_hyper_coords

    nobj_ned_lvd = len(ned_lvd)
    nobj_ned_nedlvs = len(ned_nedlvs)
    nobj_ned_hyper = len(ned_hyper)

    hyper_noned = hyper[~np.isin(hyper['ROW'], ned_hyper['ROW'])]

    # [0] Preprocess the data.

    # ned_nedlvs - 21 objects are duplicates, apparently because of
    # cross-identification problems in NED. Keep just the first one of each
    # occurrance here (=10 unique objects)
    print()
    print('#####')
    print('ned_nedlvs:')
    col = 'OBJNAME_NED'
    rr, cc = np.unique(ned_nedlvs[col], return_counts=True)
    dd = ned_nedlvs[np.isin(ned_nedlvs[col], rr[cc>1].value)]
    dd = dd[np.argsort(dd[col])]
    basic_dd = get_basic_geometry(dd, galaxy_column='OBJNAME_NED', verbose=verbose)
    toss = []
    for objname in np.unique(dd[col]):
        I = np.where(ned_nedlvs[col] == objname)[0]
        J = np.where(basic_dd['GALAXY'] == objname)[0]
        this = np.where(basic_dd[J]['DIAM'] > 0.)[0]
        if len(this) == 0:
            toss.append(np.delete(I, np.argmin(ned_nedlvs[I]['ROW'])))
        else:
            this = this[np.argsort(ned_nedlvs[I][this]['ROW'])]
            toss.append(np.delete(I, this[0]))
        #I = I[np.argsort(ned_nedlvs[col][I])]
        #toss.append(I[1:]) # keep the zeroth match
    toss = np.hstack(toss)
    print(f'Removing {len(toss):,d}/{len(ned_nedlvs):,d} ({100.*len(toss)/len(ned_nedlvs):.1f}%) {col} duplicates.')
    ned_nedlvs = ned_nedlvs[np.delete(np.arange(len(ned_nedlvs)), toss)]

    c_nedlvs = SkyCoord(ra=ned_nedlvs['RA']*u.deg, dec=ned_nedlvs['DEC']*u.deg)
    indx_nedlvs, sep2d, _ = match_coordinates_sky(c_nedlvs, c_nedlvs, nthneighbor=2)
    dd = ned_nedlvs[sep2d.arcsec == 0.]
    dd = dd[np.argsort(dd['RA'])]
    basic_dd = get_basic_geometry(dd, galaxy_column='OBJNAME_NED', verbose=verbose)
    radecs = np.char.add(np.round(dd['RA'], 10).astype(str), np.round(dd['DEC'], 10).astype(str))
    ref_radecs = np.char.add(np.round(ned_nedlvs['RA'], 10).astype(str), np.round(ned_nedlvs['DEC'], 10).astype(str))
    toss = []
    for radec in np.unique(radecs):
        I = np.where(radec == ref_radecs)[0]
        J = np.where(radec == radecs)[0]
        this = np.where(basic_dd[J]['DIAM'] > 0.)[0]
        if len(this) == 0:
            toss.append(np.delete(I, np.argmin(ned_nedlvs[I]['ROW'])))
        else:
            this = this[np.argsort(ned_nedlvs[I][this]['ROW'])]
            toss.append(np.delete(I, this[0]))
    toss = np.hstack(toss)
    print(f'Removing {len(toss):,d}/{len(ned_nedlvs):,d} ({100.*len(toss)/len(ned_nedlvs):.1f}%) coordinate duplicates.')
    ned_nedlvs = ned_nedlvs[np.delete(np.arange(len(ned_nedlvs)), toss)]

    #from pydl.pydlutils.spheregroup import spheregroup
    #ingroup, group_mult, firstgroup, nextgroup = spheregroup(ned_nedlvs['RA'], ned_nedlvs['DEC'], 1.5/60.)

    # Toss out non-galaxies in ned_nedlvs. But note:
    # * Need to make sure the individual members of the GGroup systems are
    #   in the final parent sample.
    # https://ned.ipac.caltech.edu/help/ui/nearposn-list_objecttypes?popup=1
    toss = np.where(np.isin(ned_nedlvs['OBJTYPE'], ['QSO', 'Q_Lens', 'G_Lens', '*',
                                                    'Other', 'GGroup'#,
                                                    #'GPair', 'GTrpl'
                                                    ]))[0]
    print(f'Removing {len(toss):,d}/{len(ned_nedlvs):,d} ({100.*len(toss)/len(ned_nedlvs):.1f}%) non-galaxies.')
    ned_nedlvs = ned_nedlvs[np.delete(np.arange(len(ned_nedlvs)), toss)]

    ## ned_hyper - 1 object (WINGSJ125256.27-152110.4) is a duplicate. As
    ## the primary object, it's PGC4777821, but as the alternate object,
    ## it's also [CZ2003]1631C-0295:095 = PGC6729485. In addition, remove
    ## the ~2500 objects not in NED and the ~11k objects resolve to the same
    ## object in NED; choose the first one.
    #warn = np.array(['WARNING' in objnote for objnote in ned_hyper['OBJECT_NOTE']])
    #print(f'Removing {np.sum(warn):,d}/{len(ned_hyper):,d} objects with NED warnings from ned_hyper.')
    #ned_hyper = ned_hyper[~warn]

    #col = 'OBJNAME'
    #rr, cc = np.unique(ned_hyper[col], return_counts=True)
    #dd = ned_hyper[np.isin(ned_hyper[col], rr[cc>1].value)]
    #dd = dd[np.argsort(dd[col])]
    #toss = []
    #for objname in np.unique(dd[col]):
    #    I = np.where(ned_hyper[col] == objname)[0]
    #    I = I[np.argsort(ned_hyper[col][I])]
    #    toss.append(I[1:]) # keep the zeroth match
    #toss = np.hstack(toss)
    #print(f'Removing {len(toss):,d}/{len(ned_hyper):,d} {col} duplicates from ned_hyper.')
    #ned_hyper = ned_hyper[np.delete(np.arange(len(ned_hyper)), toss)]
    print()
    print('ned_hyper:')

    col = 'OBJNAME_NED'
    rr, cc = np.unique(ned_hyper[col], return_counts=True)
    dd = ned_hyper[np.isin(ned_hyper[col], rr[cc>1].value)]
    dd = dd[np.argsort(dd[col])]
    basic_dd = get_basic_geometry(dd, galaxy_column='OBJNAME_NED', verbose=verbose)
    toss = []
    for objname in np.unique(dd[col]):
        I = np.where(ned_hyper[col] == objname)[0]
        J = np.where(basic_dd['GALAXY'] == objname)[0]
        this = np.where(basic_dd[J]['DIAM'] > 0.)[0]
        if len(this) == 0:
            toss.append(np.delete(I, np.argmin(ned_hyper[I]['ROW'])))
        else:
            this = this[np.argsort(ned_hyper[I][this]['ROW'])]
            toss.append(np.delete(I, this[0]))
    toss = np.hstack(toss)
    print(f'Removing {len(toss):,d}/{len(ned_hyper):,d} ({100.*len(toss)/len(ned_hyper):.1f}%) {col} OBJNAME_NED duplicates.')
    ned_hyper = ned_hyper[np.delete(np.arange(len(ned_hyper)), toss)]

    c_hyper = SkyCoord(ra=ned_hyper['RA']*u.deg, dec=ned_hyper['DEC']*u.deg)
    indx_hyper, sep2d, _ = match_coordinates_sky(c_hyper, c_hyper, nthneighbor=2)
    dd = ned_hyper[sep2d.arcsec == 0.]
    dd = dd[np.argsort(dd['RA'])]
    basic_dd = get_basic_geometry(dd, galaxy_column='OBJNAME_NED', verbose=verbose)
    radecs = np.char.add(np.round(dd['RA'], 10).astype(str), np.round(dd['DEC'], 10).astype(str))
    ref_radecs = np.char.add(np.round(ned_hyper['RA'], 10).astype(str), np.round(ned_hyper['DEC'], 10).astype(str))
    toss = []
    for radec in np.unique(radecs):
        I = np.where(radec == ref_radecs)[0]
        J = np.where(radec == radecs)[0]
        this = np.where(basic_dd[J]['DIAM'] > 0.)[0]
        if len(this) == 0:
            toss.append(np.delete(I, np.argmin(ned_hyper[I]['ROW'])))
        else:
            this = this[np.argsort(ned_hyper[I][this]['ROW'])]
            toss.append(np.delete(I, this[0]))
    toss = np.hstack(toss)
    print(f'Removing {len(toss):,d}/{len(ned_hyper):,d} ({100.*len(toss)/len(ned_hyper):.1f}%) {col} coordinate duplicates.')
    ned_hyper = ned_hyper[np.delete(np.arange(len(ned_hyper)), toss)]

    # Toss out non-galaxies in ned_hyper. But note:
    # * Need to make sure the individual members of the GGroup systems are
    #   in the final parent sample.
    # * Some objects classified as point sources (*) have SDSS redshifts,
    #   so the classification is wrong (e.g., GAMA743045=SDSSJ141614.97-005648.2)
    # * Also throw out VIRGO01, which incorrectly maps to 'Virgo I Dwarf'.

    # https://ned.ipac.caltech.edu/help/ui/nearposn-list_objecttypes?popup=1
    toss = np.where(np.isin(ned_hyper['OBJTYPE'], ['PofG', '!V*', '!PN', '**', 'GClstr', 'WD*',
                                                   'Red*', '!HII', 'C*', 'PN', '*Ass', 'Blue*',
                                                   '!**', 'SN', '!*', 'Other', 'SNR', '*Cl',
                                                   '!WD*', 'GGroup',
                                                   #'GPair', 'GTrpl',
                                                   'V*', '*', 'HII', 'Nova', 'Neb', 'RfN', '!V*', '!C*',
                                                   'QSO', 'Q_Lens', 'G_Lens']))[0]
    toss = np.hstack((toss, np.where(ned_hyper['OBJNAME'] == 'VIRGO01')[0]))
    print(f'Removing {len(toss):,d}/{len(ned_hyper):,d} ({100.*len(toss)/len(ned_hyper):.1f}%) non-galaxies.')
    ned_hyper = ned_hyper[np.delete(np.arange(len(ned_hyper)), toss)]

    # check
    print()
    print('After basic cuts:')
    for name, cat, norig in zip(['ned_lvd', 'ned_nedlvs', 'ned_hyper'],
                                [ned_lvd, ned_nedlvs, ned_hyper],
                                [nobj_ned_lvd, nobj_ned_nedlvs, nobj_ned_hyper]):
        nobj = len(cat)
        print(f'{name}: {nobj:,d}/{norig:,d} objects')
        for col in ['OBJNAME', 'OBJNAME_NED', 'ROW']:
            assert(len(np.unique(cat[col])) == nobj)
            #rr, cc = np.unique(cat[col], return_counts=True)
            ##print(rr[cc>1])
            #bb = cat[np.isin(cat[col], rr[cc>1].value)]
            #bb = bb[np.argsort(bb[col])]

    # [1] - Match HyperLeda{-altname} to NEDLVS using OBJNAME_NED.
    print()
    print('#####')

    keys = np.array(ned_nedlvs.colnames)
    keys = keys[~np.isin(keys, ['OBJNAME', 'ROW'])]

    out1 = join(ned_hyper, ned_nedlvs, keys=keys, table_names=['HYPERLEDA', 'NEDLVS'])
    out1.rename_columns(['RA', 'DEC', 'Z'], ['RA_NED', 'DEC_NED', 'Z_NED'])

    print(f'Matched {len(out1):,d}/{len(ned_hyper):,d} ({100.*len(out1)/len(ned_hyper):.1f}%) ned_hyper and ' + \
          f'{len(out1):,d}/{len(ned_nedlvs):,d} ({100.*len(out1)/len(ned_nedlvs):.1f}%) ned_nedlvs objects using OBJNAME_NED.')

    basic_out1 = get_basic_geometry(out1, galaxy_column='OBJNAME_NED', verbose=verbose)

    #indx_out, indx_hyper = match(out1['ROW_HYPERLEDA'], hyper['ROW'])
    #out1['OBJNAME_HYPERLEDA'][indx_out] = hyper['OBJNAME'][indx_hyper]
    #out1 = out1[np.argsort(out1['ROW_HYPERLEDA'])]

    parent1 = populate_parent(out1, basic_out1, verbose=verbose)

    indx_parent, indx_hyper = match(parent1['ROW_HYPERLEDA'], hyper['ROW'])
    if verbose:
        for col in ['RA_HYPERLEDA', 'DEC_HYPERLEDA', 'Z_HYPERLEDA']:
            print(f'Populating {col}')
    parent1['RA_HYPERLEDA'][indx_parent] = hyper['RA'][indx_hyper]
    parent1['DEC_HYPERLEDA'][indx_parent] = hyper['DEC'][indx_hyper]
    I = np.where(~np.isnan(hyper['V'][indx_hyper]))[0]
    parent1['Z_HYPERLEDA'][indx_parent[I]] = hyper['V'][indx_hyper[I]] / 2.99e5

    indx_parent, indx_nedlvs = match(parent1['ROW_NEDLVS'], nedlvs['ROW'])
    if verbose:
        for col in ['RA_NEDLVS', 'DEC_NEDLVS', 'Z_NEDLVS']:
            print(f'Populating {col}')
    parent1['RA_NEDLVS'][indx_parent] = nedlvs['RA'][indx_nedlvs]
    parent1['DEC_NEDLVS'][indx_parent] = nedlvs['DEC'][indx_nedlvs]
    I = np.where(~np.isnan(nedlvs['Z'][indx_nedlvs]))[0]
    parent1['Z_NEDLVS'][indx_parent[I]] = nedlvs['Z'][indx_nedlvs[I]]

    for col in ['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_NEDLVS', 'ROW_HYPERLEDA', 'ROW_NEDLVS']:
        assert(len(np.unique(parent1[col])) == len(parent1))
    print()
    print(f'Parent 1: N={len(parent1):,d}')
    #pdb.set_trace()

    # [2] - Add as many of the remaining ned_hyper objects as possible. Special
    # case VIRGO1, which incorrectly matches (in NED) to 'Virgo I Dwarf' rather
    # than 'Virgo I'.
    print()
    print('#####')
    miss_hyper = ned_hyper[np.logical_and(~np.isin(ned_hyper['ROW'], parent1['ROW_HYPERLEDA']),
                                          (ned_hyper['OBJNAME'] != 'VIRGO1'))]
    miss_hyper.rename_columns(['OBJNAME', 'ROW'], ['OBJNAME_HYPERLEDA', 'ROW_HYPERLEDA'])
    miss_hyper.rename_columns(['RA', 'DEC', 'Z'], ['RA_NED', 'DEC_NED', 'Z_NED'])

    print(f'Adding the remaining {len(miss_hyper):,d} objects from ned_hyper which did not name-match ned_nedlvs.')
    basic_miss_hyper = get_basic_geometry(miss_hyper, galaxy_column='OBJNAME_NED', verbose=verbose)

    parent2 = populate_parent(miss_hyper, basic_miss_hyper, verbose=verbose)

    indx_parent, indx_hyper = match(parent2['ROW_HYPERLEDA'], hyper['ROW'])
    if verbose:
        for col in ['RA_HYPERLEDA', 'DEC_HYPERLEDA', 'Z_HYPERLEDA']:
            print(f'Populating {col}')
    parent2['RA_HYPERLEDA'][indx_parent] = hyper['RA'][indx_hyper]
    parent2['DEC_HYPERLEDA'][indx_parent] = hyper['DEC'][indx_hyper]
    I = np.where(~np.isnan(hyper['V'][indx_hyper]))[0]
    parent2['Z_HYPERLEDA'][indx_parent[I]] = hyper['V'][indx_hyper[I]] / 2.99e5

    print()
    print(f'Parent 2: N={len(parent2):,d}')
    #pdb.set_trace()

    # [3] - Add the rest of the ned_nedlvs objects, being careful about exact
    # duplicates.
    print()
    print('#####')

    parent = vstack((parent1, parent2))

    miss_nedlvs = ned_nedlvs[~np.isin(ned_nedlvs['ROW'], parent['ROW_NEDLVS'])]
    miss_nedlvs.rename_columns(['OBJNAME', 'ROW'], ['OBJNAME_NEDLVS', 'ROW_NEDLVS'])
    miss_nedlvs.rename_columns(['RA', 'DEC', 'Z'], ['RA_NED', 'DEC_NED', 'Z_NED'])
    print(f'Analyzing the remaining {len(miss_nedlvs):,d} ned_nedlvs objects.')

    c_parent = SkyCoord(ra=parent['RA_NED']*u.deg, dec=parent['DEC_NED']*u.deg)
    c_nedlvs = SkyCoord(ra=miss_nedlvs['RA_NED']*u.deg, dec=miss_nedlvs['DEC_NED']*u.deg)
    indx_dup_nedlvs, sep2d, _ = c_parent.match_to_catalog_sky(c_nedlvs)
    indx_dup_parent = np.where(sep2d.arcsec == 0.)[0]
    indx_dup_nedlvs = indx_dup_nedlvs[indx_dup_parent]

    #dup_parent = parent[indx_dup_parent]
    #dup_parent['OBJNAME_HYPERLEDA', 'OBJNAME_NED', 'OBJNAME_NEDLVS', 'RA_NED', 'DEC_NED'][:10]
    #miss_nedlvs[indx_dup_nedlvs]['OBJNAME_NEDLVS', 'OBJNAME_NED', 'RA_NED', 'DEC_NED'][:10]

    print(f'Removing {len(indx_dup_nedlvs):,d}/{len(miss_nedlvs):,d} ({100.*len(indx_dup_nedlvs)/len(miss_nedlvs):.1f}%) ' + \
          f'ned_nedlvs duplicates (sep=0.0 arcsec) already in parent sample.')
    #parent = parent[np.delete(np.arange(len(parent)), indx_dup_parent)]
    miss_nedlvs = miss_nedlvs[np.delete(np.arange(len(miss_nedlvs)), indx_dup_nedlvs)]

    basic_miss_nedlvs = get_basic_geometry(miss_nedlvs, galaxy_column='OBJNAME_NED', verbose=verbose)

    parent3 = populate_parent(miss_nedlvs, basic_miss_nedlvs, verbose=verbose)

    indx_parent, indx_nedlvs = match(parent3['ROW_NEDLVS'], nedlvs['ROW'])
    if verbose:
        for col in ['RA_NEDLVS', 'DEC_NEDLVS', 'Z_NEDLVS']:
            print(f'Populating {col}')
    parent3['RA_NEDLVS'][indx_parent] = nedlvs['RA'][indx_nedlvs]
    parent3['DEC_NEDLVS'][indx_parent] = nedlvs['DEC'][indx_nedlvs]
    I = np.where(~np.isnan(nedlvs['Z'][indx_nedlvs]))[0]
    parent3['Z_NEDLVS'][indx_parent[I]] = nedlvs['Z'][indx_nedlvs[I]]

    print()
    print(f'Parent 3: N={len(parent3):,d}')
    #pdb.set_trace()

    parent = vstack((parent, parent3))

    # [4] - Add any outstanding hyper objects with measured diameters.
    print()
    print('#####')

    miss_hyper = hyper_noned[~np.isin(hyper_noned['ROW'], parent['ROW_HYPERLEDA'])]
    miss_hyper.rename_columns(['OBJNAME', 'ROW'], ['OBJNAME_HYPERLEDA', 'ROW_HYPERLEDA'])
    miss_hyper.rename_columns(['RA', 'DEC'], ['RA_HYPERLEDA', 'DEC_HYPERLEDA'])
    miss_hyper['Z_HYPERLEDA'] = np.zeros(len(miss_hyper)) - 99.
    I = np.where(~np.isnan(miss_hyper['V']))[0]
    miss_hyper['Z_HYPERLEDA'][I] = hyper['V'][I] / 2.99e5

    # http://atlas.obs-hp.fr/hyperleda/leda/param/celpos.html
    #I = np.where((0.1*10**miss_hyper['LOGD25'] > 1.) * (miss_hyper['F_ASTROM'] < 1))[0]
    I = np.where(miss_hyper['LOGD25'] > 0.)[0]
    print(f'Adding {len(I):,d}/{len(miss_hyper):,d} HyperLeda objects with measured diameters ' + \
          'not in ned_hyper and not already in our catalog.')
    miss_hyper = miss_hyper[I]
    basic_miss_hyper = get_basic_geometry(miss_hyper, galaxy_column='OBJNAME_HYPERLEDA', verbose=verbose)

    parent4 = populate_parent(miss_hyper, basic_miss_hyper, verbose=verbose)

    print()
    print(f'Parent 4: N={len(parent4):,d}.')
    #pdb.set_trace()

    parent = vstack((parent, parent4))

    # [5] Add LVD.
    print()
    print('#####')
    print(f'Analyzing {len(lvd):,d} LVD objects, of which {len(ned_lvd):,d} ({100.*len(ned_lvd)/len(lvd):.1f}%) are in ned_lvd.')

    # ned_lvd - already in parent sample
    I = np.where(parent['OBJNAME_NED'] != '')[0]
    indx_parent, indx_lvd = match(parent[I]['OBJNAME_NED'], ned_lvd['OBJNAME_NED'])

    nexisting = len(indx_parent)
    if verbose:
        print(f'Populating ROW_LVD')
    parent['ROW_LVD'][I[indx_parent]] = ned_lvd['ROW'][indx_lvd]
    print(f'Matched {len(indx_lvd):,d}/{len(lvd):,d} ({100.*len(indx_lvd)/len(lvd):.1f}%) ' + \
          'ned_lvd objects to the /existing/ parent sample using OBJNAME_NED.')

    indx_parent2, indx_lvd2 = match(parent['ROW_LVD'][I[indx_parent]], lvd['ROW'])
    if verbose:
        for col in ['OBJNAME_LVD', 'RA_LVD', 'DEC_LVD']:
            print(f'Populating {col}')
    parent['OBJNAME_LVD'][I[indx_parent[indx_parent2]]] = lvd['OBJNAME'][indx_lvd2]
    parent['RA_LVD'][I[indx_parent[indx_parent2]]] = lvd['RA'][indx_lvd2]
    parent['DEC_LVD'][I[indx_parent[indx_parent2]]] = lvd['DEC'][indx_lvd2]
    #parent[I[indx_parent[indx_parent2]]]['OBJNAME_NED', 'OBJNAME_LVD', 'RA_NED', 'DEC_NED', 'RA_LVD', 'DEC_LVD', 'ROW_LVD', 'PGC']

    # ned_lvd - not in parent sample (new)
    miss_lvd = ned_lvd[~np.isin(ned_lvd['ROW'], parent['ROW_LVD'])]
    miss_lvd.rename_columns(['OBJNAME', 'ROW'], ['OBJNAME_LVD', 'ROW_LVD'])
    miss_lvd.rename_columns(['RA', 'DEC', 'Z'], ['RA_NED', 'DEC_NED', 'Z_NED'])
    print(f'Adding {len(miss_lvd):,d}/{len(lvd):,d} ({100.*len(miss_lvd)/len(lvd):.1f}%) ' + \
          '/new/ ned_lvd objects to the parent sample.')

    basic_miss_lvd = get_basic_geometry(miss_lvd, galaxy_column='OBJNAME_LVD', verbose=verbose)

    parent5a = populate_parent(miss_lvd, basic_miss_lvd, verbose=verbose)

    # LVD - not in parent sample (new)
    miss_lvd = lvd[np.logical_and(~np.isin(lvd['ROW'], parent['ROW_LVD']),
                                  ~np.isin(lvd['ROW'], parent5a['ROW_LVD']))]
    miss_lvd.rename_columns(['OBJNAME', 'ROW'], ['OBJNAME_LVD', 'ROW_LVD'])
    miss_lvd.rename_columns(['RA', 'DEC'], ['RA_LVD', 'DEC_LVD'])
    print(f'Adding {len(miss_lvd):,d}/{len(lvd):,d} ({100.*len(miss_lvd)/len(lvd):.1f}%) ' + \
          'new LVD objects to the parent sample.')

    basic_miss_lvd = get_basic_geometry(miss_lvd, galaxy_column='OBJNAME_LVD', verbose=verbose)

    parent5b = populate_parent(miss_lvd, basic_miss_lvd, verbose=verbose)

    parent5 = vstack((parent5a, parent5b))

    # Fill in a bit more info.
    indx_parent, indx_lvd = match(parent5['ROW_LVD'], lvd['ROW'])
    print(f'Matching {len(indx_parent):,d} objects to the original LVD catalog.')
    if verbose:
        for col in ['OBJNAME_LVD (replacing the NED-friendly names)', 'RA_LVD', 'DEC_LVD']:
            print(f'Populating {col}')
    parent5['OBJNAME_LVD'][indx_parent] = lvd['OBJNAME'][indx_lvd] # replace the NED-friendly names
    parent5['RA_LVD'][indx_parent] = lvd['RA'][indx_lvd]
    parent5['DEC_LVD'][indx_parent] = lvd['DEC'][indx_lvd]

    print()
    print(f'Parent 5: N={len(parent5)+nexisting:,d}')
    #pdb.set_trace()

    # [6] include the custom-added objects
    print()
    print('#####')
    print(f'Adding {len(custom):,d} more objects from the custom catalog.')

    custom.rename_column('ROW', 'ROW_CUSTOM')
    basic_custom = get_basic_geometry(custom, galaxy_column='OBJNAME', verbose=verbose)
    parent6 = populate_parent(custom, basic_custom, verbose=verbose)

    print()
    print(f'Parent 6: N={len(parent6):,d}')

    # [7] build the final sample
    parent = vstack((parent, parent5, parent6))

    # sort, check for uniqueness, and then write out
    srt = np.lexsort((parent['ROW_HYPERLEDA'].value, parent['ROW_NEDLVS'].value,
                      parent['ROW_LVD'].value, parent['ROW_CUSTOM'].value))
    parent = parent[srt]

    for col in ['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_NEDLVS', 'OBJNAME_LVD']:
        I = parent[col] != ''
        assert(len(parent[I]) == len(np.unique(parent[col][I])))

    I = parent['PGC'] > 0
    assert(len(parent[I]) == len(np.unique(parent['PGC'][I])))

    #pgc, count = np.unique(parent['PGC'][I], return_counts=True)
    #bb = parent[np.isin(parent['PGC'], pgc[count>1].value)]['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_NEDLVS',
    #                                                        'OBJNAME_LVD', 'RA_NED', 'DEC_NED', 'PGC', 'ROW_HYPERLEDA', 'ROW_NEDLVS', 'ROW_LVD']
    #bb = bb[np.argsort(bb['PGC'])]

    for col in ['ROW_HYPERLEDA', 'ROW_NEDLVS', 'ROW_LVD', 'ROW_CUSTOM']:
        I = parent[col] != -99
        assert(len(parent[I]) == len(np.unique(parent[col][I])))

    print()
    print('#####')
    print(f'Final parent sample: N={len(parent):,d}')

    # Populate OBJNAME, RA, DEC, and Z
    print()
    for dataset in ['NED', 'LVD', 'NEDLVS', 'HYPERLEDA']:
        I = np.where((parent['RA'] == -99.) * (parent[f'RA_{dataset}'] != -99.))[0]
        if len(I) > 0:
            print(f'Adopting {len(I):,d}/{len(parent):,d} ({100.*len(I)/len(parent):.1f}%) RA,DEC values from {dataset}.')
            parent['RA'][I] = parent[I][f'RA_{dataset}']
            parent['DEC'][I] = parent[I][f'DEC_{dataset}']
        I = np.where((parent['OBJNAME'] == '') * (parent[f'OBJNAME_{dataset}'] != ''))[0]
        if len(I) > 0:
            print(f'Adopting {len(I):,d}/{len(parent):,d} ({100.*len(I)/len(parent):.1f}%) OBJNAMEs from {dataset}.')
            # Roughly 4300 objects have "SDSSJ" names rather than "SDSS J".
            # Standardize that here so we match NED-LVS and so that we're
            # NED-friendly.
            if dataset == 'HYPERLEDA':
                parent['OBJNAME'][I] = nedfriendly_hyperleda(parent[I][f'OBJNAME_{dataset}'])
            else:
                parent['OBJNAME'][I] = parent[I][f'OBJNAME_{dataset}']

    for dataset in ['NEDLVS', 'NED', 'HYPERLEDA']:
        I = np.where((parent['Z'] == -99.) * (parent[f'Z_{dataset}'] != -99.))[0]
        if len(I) > 0:
            print(f'Adopting {len(I):,d}/{len(parent):,d} ({100.*len(I)/len(parent):.1f}%) Z values from {dataset}.')
            parent['Z'][I] = parent[I][f'Z_{dataset}']
    print()

    # reset and then prioritize the diameters
    basic_hyper = get_basic_geometry(hyper, galaxy_column='ROW', verbose=verbose)
    basic_ned_hyper = get_basic_geometry(ned_hyper, galaxy_column='ROW', verbose=verbose)
    basic_ned_nedlvs = get_basic_geometry(ned_nedlvs, galaxy_column='ROW', verbose=verbose)
    basic_lvd = get_basic_geometry(lvd, galaxy_column='ROW', verbose=verbose)
    basic_custom = get_basic_geometry(custom, galaxy_column='ROW_CUSTOM', verbose=verbose)

    for col in ['DIAM', 'BA', 'PA', 'MAG']:
        parent[col] = -99.
        parent[f'{col}_REF'] = ''
    parent['BAND'] = ''

    print()
    for basic, row, dataset in zip((basic_custom, basic_lvd, basic_ned_hyper, basic_ned_nedlvs, basic_hyper),
                                   ('ROW_CUSTOM', 'ROW_LVD', 'ROW_HYPERLEDA', 'ROW_NEDLVS', 'ROW_HYPERLEDA'),
                                   ('CUSTOM', 'LVD', 'NED-HyperLeda', 'NEDLVS', 'HyperLeda')):
        for col in ['DIAM', 'BA', 'PA', 'MAG']:
            I = np.where((parent[col] == -99.) * (parent[row] != -99))[0]
            if len(I) > 0:
                indx_parent, indx_basic = match(parent[I][row], basic['GALAXY'])
                G = np.where(basic[indx_basic][col] != -99.)[0]
                if len(G) > 0:
                    print(f'Populating parent with {len(G):,d}/{len(I):,d} {col}s from {dataset}.')
                    parent[col][I[indx_parent[G]]] = basic[indx_basic[G]][col]
                    parent[f'{col}_REF'][I[indx_parent[G]]] = basic[indx_basic[G]][f'{col}_REF']
                    if col == 'MAG':
                        parent['BAND'][I[indx_parent[G]]] = basic[indx_basic[G]]['BAND']
                    #parent[I[indx_parent[G]]]['OBJNAME_NED', 'OBJNAME_HYPERLEDA', 'OBJNAME_NEDLVS', 'OBJNAME_LVD', 'DIAM', 'DIAM_REF', 'BA', 'BA_REF', 'PA', 'PA_REF']
        print()

        # special columns for HyperLeda geometry
        if dataset == 'HyperLeda':
            I = np.where(parent[row] != -99)[0]
            indx_parent, indx_basic = match(parent[I][row], basic['GALAXY']) # 'GALAXY' here is actually 'ROW'
            for col in ['DIAM', 'BA', 'PA']:
                parent[f'{col}_LEDA'][I[indx_parent]] = basic[indx_basic][col]

    # final statistics
    nobj = len(parent)
    for col in ['DIAM', 'BA', 'PA', 'MAG']:
        N = parent[col] != -99.
        refs = np.unique(parent[N][f'{col}_REF'])
        print(f'N({col}) = {np.sum(N):,d}/{nobj:,d} ({100.*np.sum(N)/nobj:.1f}%)')
        for ref in refs:
            R = parent[N][f'{col}_REF'] == ref
            print(f'  N({ref}) = {np.sum(R):,d}/{np.sum(N):,d} ({100.*np.sum(R)/np.sum(N):.1f}%)')

    parent['PARENT_ROW'] = np.arange(len(parent))

    outfile = os.path.join(sga_dir(), 'parent', f'SGA2024-parent-nocuts.fits')
    print(f'Writing {len(parent):,d} objects to {outfile}')
    parent.write(outfile, overwrite=True)


#def domatch_coord(cat, refcat, rank=0):
#    """Match a catalog and a reference catalog based coordinates.
#
#    """
#    import astropy.units as u
#    from astropy.coordinates import SkyCoord
#    from SGA.util import match
#
#    print('Rank {rank:03d}: Matching based on coordinates.')
#    c_cat = SkyCoord(cat['RA']*u.deg, cat['DEC']*u.deg)
#    c_refcat = SkyCoord(refcat['RA']*u.deg, refcat['DEC']*u.deg)
#    indx_refcat, sep2d, _ = c_cat.match_to_catalog_sky(c_refcat)
#    print(f'Rank {rank:03d}: Mean and max separation between {len(cat)} galaxies is ' \
#          f'{np.mean(sep2d.arcsec):.3f}+/-{np.std(sep2d.arcsec):.3f}, {np.max(sep2d.arcsec):.3f} arcsec.')
#
#    srt = np.argsort(sep2d)[::-1]
#    refcat[indx_refcat[srt]][:10]
#    cat[srt][:10]
#
#    info = hstack((refcat[indx_refcat]['PGC', 'OBJNAME', 'RA', 'DEC', 'LOGD25'], cat['GALAXY', 'RA', 'DEC', 'RHALF', 'SURFACE_BRIGHTNESS_RHALF']))
#    info['SEPARCSEC'] = sep2d.arcsec.astype('f4')


def read_cat(catalog):
    """Wrapper to read one of the known external catalogs.

    """
    match catalog.lower():
        case 'hyperleda':
            cat = read_hyperleda()
        case 'wxsc':
            cat = read_wxsc()
        case 'nedlvs':
            cat = read_nedlvs()
        case 'lvd':
            cat = read_lvd()
        case _:
            raise ValueError(f'Unrecognized catalog name {catalog}')

    return cat


#def _get_ccds(args):
#    """Wrapper for the multiprocessing."""
#    return get_ccds(*args)


def get_ccds(allccds, onegal, width_pixels, pixscale=PIXSCALE, return_ccds=False):
    """Quickly get the CCDs touching this custom brick.  This code is mostly taken
    from legacypipe.runbrick.stage_tims.

    """
    from SGA.coadds import custom_brickname
    from legacypipe.survey import wcs_for_brick, BrickDuck, ccds_touching_wcs

    brickname = f'custom-{custom_brickname(onegal["RA"], onegal["DEC"])}'
    brick = BrickDuck(onegal['RA'], onegal['DEC'], brickname)

    targetwcs = wcs_for_brick(brick, W=float(width_pixels), H=float(width_pixels), pixscale=pixscale)
    I = ccds_touching_wcs(targetwcs, allccds)
    #ccds = survey.ccds_touching_wcs(targetwcs)
    #print(len(I))

    # no CCDs within width_pixels
    if len(I) == 0:
        if return_ccds:
            return Table(), Table()
        else:
            return Table()

    ccds = allccds[I]

    onegal['NCCD'] = len(ccds)
    onegal['FILTERS'] = ''.join(sorted(set(ccds.filter)))

    if return_ccds:
        # convert to an astropy Table so we can vstack
        _ccds = ccds.to_dict()
        ccds = Table()
        for key in _ccds.keys():
            ccds[key.upper()] = _ccds[key]

        ccds = ccds['RA', 'DEC', 'CAMERA', 'EXPNUM', 'PLVER', 'CCDNAME', 'FILTER']
        #ccds['GALAXY'] = [galaxy]
        ccds['ROW'] = onegal['ROW']

        return ccds, Table(onegal)
    else:
        return Table(onegal)


def in_footprint(cat, allccds, radius=1., width_pixels=152, bands=BANDS, comm=None, mp=1):
    """Find which objects are in the given survey footprint based on positional
    matching with a very generous (1 deg) search radius.

    radius in degrees

    """
    from SGA.util import weighted_partition, match_to

    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size

    cat['NCCD'] = np.zeros(len(cat), int)
    cat['FILTERS'] = np.zeros(len(cat), '<U4')

    if rank == 0:
        t0 = time.time()

        # I, which is a list of len(cat), is the variable-length of indices into
        # allccds of the matches, or None if no match (which we filter out).
        indx_ccds = match_radec(cat['RA'], cat['DEC'], allccds.ra, allccds.dec,
                                radius, indexlist=True)

        indx_cat = []
        nccdperobj = []
        for icat, val in enumerate(indx_ccds):
            if val is not None:
                indx_cat.append(icat)
                nccdperobj.append(len(indx_ccds[icat]))
        print(f'Rank {rank:03d}: Found {len(indx_cat):,d}/{len(cat):,d} objects with at least one CCD within {radius} deg.')

        groups = weighted_partition(nccdperobj, size)
    else:
        indx_cat = []
        indx_ccds = []
        groups = [np.array([])]

    # broadcast the work to the other ranks
    if comm:
        indx_cat = comm.bcast(indx_cat, root=0)
        indx_ccds = comm.bcast(indx_ccds, root=0)
        groups = comm.bcast(groups, root=0)

    # now perform a more refined search for each matching object

    fcat = []
    #ccds = []
    for icat, indx in enumerate(groups[rank]):
        catindx = indx_cat[indx]
        onegal = cat[catindx]
        if icat % len(groups[rank]) == 0 or icat+1 == len(groups[rank]):
            print(f'Rank {rank:03d}: Working on galaxy: {icat+1:,d}/{len(groups[rank])}')
        #print(f'Rank {rank:03d} working on galaxy: {onegal["GALAXY"]}')
        one_fcat = get_ccds(allccds[indx_ccds[catindx]], onegal, width_pixels,
                            pixscale=PIXSCALE, return_ccds=False)
        fcat.append(one_fcat)

    if len(fcat) > 0:
        fcat = vstack(fcat)

    #mpargs = []
    #for icat, catindx in enumerate(M):
    #    mpargs.append([icat, allccds[I[catindx]], cat[catindx], width_pixels, PIXSCALE])
    #
    #if mp > 1:
    #    import multiprocessing
    #    with multiprocessing.Pool(mp) as P:
    #        out = P.map(_get_ccds, mpargs)
    #else:
    #    out = [_get_ccds(_mpargs) for _mpargs in mpargs]
    #out = list(zip(*out))
    #
    #ccds = out[0]
    #fcat = out[1]
    #if len(ccds) > 0:
    #    ccds = vstack(ccds)
    #    fcat = vstack(fcat)
    #print(f'Final sample: {len(fcat):,d}/{len(indx_cat):,d} objects and {len(ccds):,d} CCDs.')
    #return fcat, ccds

    if comm:
        fcat = comm.gather(fcat, root=0)

    # sort and return
    if rank == 0:
        fcat = vstack(fcat)

        print(f'Rank {rank:03d}: Final sample: {len(fcat):,d}/{len(indx_cat):,d} objects.')

        fcat = fcat[match_to(fcat['ROW'], cat['ROW'])]
        print(f'Rank {rank:03d}: Total time: {(time.time()-t0)/60.:.3f} min')
        return fcat


def main():
    """Main wrapper

    """
    import argparse

    regions = ['north', 'south']
    catalogs = ['HyperLeda', 'NEDLVS', 'WXSC', 'LVD'] # 'HECATE', 'Z0MGS']

    parser = argparse.ArgumentParser()
    parser.add_argument('--build-parent-nocuts', action='store_true', help='Merge the catalogs retrieved by SGA2024-query-ned into the parent-nocuts catalog.')
    parser.add_argument('--build-parent', action='store_true', help='Merge the catalogs retrieved by SGA2024-query-ned.')
    parser.add_argument('--qa-parent', action='store_true', help='Build QA.')
    parser.add_argument('--mindiam', default=15./60., type=float, help='Minimum diameter for selecting parent sample.')
    parser.add_argument('--region', choices=regions, type=str, nargs='*', help='Region to pass to --in-footprint.')
    parser.add_argument('--catalog', choices=catalogs, type=str, help='External catalog to pass to --in-footprint.')
    parser.add_argument('--in-footprint', action='store_true', help='Match the various external catalogs to the CCDs files.')
    parser.add_argument('--verbose', action='store_true', help='Be verbose.')
    parser.add_argument('--overwrite', action='store_true', help='Overwrite existing files.')
    parser.add_argument('--mp', default=1, type=int, help='Number of multiprocessing processes per MPI rank.')
    args = parser.parse_args()

    # https://docs.nersc.gov/development/languages/python/parallel-python/#use-the-spawn-start-method
    if args.mp > 1 and 'NERSC_HOST' in os.environ:
        import multiprocessing
        multiprocessing.set_start_method('spawn')

    try:
        from mpi4py import MPI
        from mpi4py.util import pkl5
        #comm = MPI.COMM_WORLD
        comm = pkl5.Intracomm(MPI.COMM_WORLD)
    except ImportError:
        comm = None

    if comm is None:
        rank, size = 0, 1
    else:
        rank, size = comm.rank, comm.size


    if args.build_parent_nocuts:
        build_parent_nocuts()

    if args.build_parent:
        build_parent(mindiam=args.mindiam, overwrite=args.overwrite, verbose=args.verbose)

    if args.qa_parent:
        qa_parent(nocuts=True, size_mag=True)
        #qa_parent(nocuts=True, sky=True)



    #basedir = sga_dir()
    #footdir = os.path.join(basedir, 'parent', 'in-footprint')
    #if args.in_footprint:
    #    from legacypipe.runs import get_survey
    #
    #    if not os.path.isdir(footdir):
    #        os.makedirs(footdir)
    #
    #    for region in np.atleast_1d(args.region):
    #        survey = get_survey(region)#, allbands=BANDS)
    #        _ = survey.get_ccds_readonly()
    #
    #        for catalog in np.atleast_1d(args.catalog):
    #            outfile = os.path.join(footdir, f'{catalog}-{region}.fits')
    #            qafile = os.path.join(footdir, f'qa-{catalog}-{region}.png')
    #            if rank == 0:
    #                print(f'Rank {rank:03d}: Working on region={region} and catalog={catalog}')
    #                fullcat = read_cat(catalog)
    #            else:
    #                fullcat = Table()
    #
    #            if comm:
    #                fullcat = comm.bcast(fullcat, root=0)
    #
    #            if not os.path.isfile(outfile) or args.overwrite:
    #                cat = in_footprint(fullcat, allccds=survey.ccds, bands=None, comm=comm, mp=1)#=args.mp)
    #                # write out
    #                if rank == 0:
    #                    nccds = np.sum(cat['NCCD'])
    #                    print(f'Rank {rank:03d}: Writing {len(cat):,d} objects with {nccds:,d} CCDs to {outfile}')
    #                    #print(f'Writing {len(cat):,d} objects and {len(ccds):,d} CCDs to {outfile}')
    #                    fitsio.write(outfile, cat.as_array(), extname='CATALOG', clobber=True)
    #                    #fitsio.write(outfile, ccds.as_array(), extname='CCDS')
    #            else:
    #                if rank == 0:
    #                    cat = Table(fitsio.read(outfile, ext='CATALOG'))
    #                    #ccds = Table(fitsio.read(outfile, ext='CCDS'))
    #                    print(f'Rank {rank:03d}: Read {len(cat):,d} objects from {outfile}')
    #
    #            if rank == 0:
    #                # simple QA
    #                import matplotlib.pyplot as plt
    #                import seaborn as sns
    #
    #                if len(fullcat) < 1e3:
    #                    s = 20
    #                    markerscale = 1
    #                else:
    #                    s = 1
    #                    markerscale = 10
    #                fig, ax = plt.subplots(figsize=(8, 6))
    #                ax.scatter(fullcat['RA'], fullcat['DEC'], s=s, color='gray')
    #                for bands in sorted(set(cat['FILTERS'])):
    #                    I = cat['FILTERS'] == bands
    #                    ax.scatter(cat['RA'][I], cat['DEC'][I], s=s, alpha=0.7, label=f'{bands} (N={np.sum(I):,d})')
    #                ax.set_xlabel('RA')
    #                ax.set_ylabel('Dec')
    #                ax.set_xlim(360., 0.)
    #                ax.set_ylim(-90., 90.)
    #                #ax.invert_xaxis()
    #                ax.legend(fontsize=10, ncols=2, markerscale=markerscale, loc='lower left')
    #                fig.tight_layout()
    #                fig.savefig(qafile)
    #                print(f'Rank {rank:03d}: Wrote {qafile}')
    #

    #if args.merge:
    #    # What's missing?
    #
    #    #region = 'south'
    #    region = 'north'
    #    cat = Table(fitsio.read(os.path.join(footdir, f'LVD-{region}.fits'), 'CATALOG'))
    #    cat = add_pgc(cat, 'lvd')
    #
    #    cat = Table(fitsio.read(os.path.join(footdir, f'NEDLVS-{region}.fits'), 'CATALOG'))
    #    refcat = Table(fitsio.read(os.path.join(footdir, f'HyperLeda-{region}.fits'), 'CATALOG'))
    #
    #    #info = domatch_pgc(cat, refcat)
    #    info = domatch_coord(cat, refcat)
    #
    #    cat = read_lvd()
    #    cc = cat[cat['PGC'] == 0]['NAME', 'RA', 'DEC', 'CONFIRMED_REAL', 'REF_STRUCTURE']
    #    for oo in cc:
    #        print(f'{oo["NAME"]} {oo["RA"]}d {oo["DEC"]}d 0.5')
    #
    #    ned = read_nedlvs()
    #    I = [indx for indx, gg in enumerate(ned['OBJNAME']) if 'bootes' in gg.lower()]


if __name__ == '__main__':
    main()
