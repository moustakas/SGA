#!/usr/bin/env python

"""Query NED with an external SGA2024 sample (e.g., HyperLeda).

SGA2024-query-ned --catalog HyperLeda --query-data
SGA2024-query-ned --catalog HyperLeda --gather-data

"""
import os, re, time
from glob import glob
import numpy as np
import fitsio
from astropy.table import Table, vstack, hstack, join
import astropy.units as u
from astropy.coordinates import SkyCoord


def readone(queryfile):
    """Read one file.

    """
    skip_head = 17
    skip_foot = 4
    data_start = skip_head + 1

    with open(queryfile, 'r') as F:
        D = F.readlines()
    D = [line.replace('\n', '') for line in D]

    nodata = 'No input names recognized by the NED name interpreter'
    if nodata in D[-4]:
        print(f'No data in {queryfile}')
        return Table()
    else:
        print(f'Reading {queryfile}')
        #pass

    #D14 = D[14].split('|')
    #D15 = D[15].split('|')
    #D16 = D[16].split('|')
    #D17 = D[17].split('|')
    #for ii in range(30):
    #    txt14 = f'{D14[ii][:30]:30}'.replace(' ', '.')
    #    txt15 = f'{D15[ii][:30]:30}'.replace(' ', '.')
    #    txt16 = f'{D16[ii][:30]:30}'.replace(' ', '.')
    #    txt17 = f'{D17[ii][:30]:30}'.replace(' ', '.')
    #    print(f'{txt14}  {txt15}  {txt16}  {txt17}')

    rawdata = D[data_start:-skip_foot]
    # the header is different(!!) if only 1 object is in the file
    if len(rawdata) == 0:
        rawdata = D[data_start-1:-skip_foot]
    nned = len(rawdata)

    #if nned != len(racat[ss:ee]):
    #    print('Missing some objects')
    #    raise ValueError()

    # pack the data into a Table
    cols = ['ROW', 'OBJECT_NOTE', 'OBJNAME', 'ESSENTIAL_NOTE',
            'OBJNAME_NED','RA', 'DEC', 'OBJTYPE', 'Z',
            'BASIC_MAG', 'BASIC_DMAJOR', 'BASIC_DMINOR', 'BASIC_MORPH', 'MORPH',
            'SDSS_R', 'RC3_B', 'TWOMASS_K',
            'SDSS_DIAM_R', 'SDSS_BA_R', 'SDSS_PA_R',
            'TWOMASS_DIAM_K', 'TWOMASS_BA_K', 'TWOMASS_PA_K',
            'RC3_DIAM_B', 'RC3_BA_B', 'RC3_PA_B',
            'ESO_DIAM_B', 'ESO_BA_B', 'ESO_PA_B',
            'ROW2']

    data = []
    for irow, row in enumerate(rawdata):
        #print(irow, len(row.split('|')))
        row = row.replace('|b|', 'b') # special case the essential note for PGC006041
        row = row.replace('|S(87GB) - S(87GB[BWE91])|', 'S(87GB) - S(87GB[BWE91])') # PGC2822076
        row = row.replace('comp|SB?c', 'comp; SB?c') # ESO 108-IG 021
        row = row.replace('Spiral;|HSB', 'Spiral; HSBc') # PGC1372311
        row = row.split('|')
        row = [val.strip() for val in row]
        if len(row) != len(cols):
            print(f'Problem parsing row {irow} of {queryfile}')
            raise ValueError()
        data.append(row)
    nobj = len(data)

    data = list(zip(*data))

    temp = Table()
    for icol, col in enumerate(cols):
        #print(col, data[icol])
        #if len(data[icol]) != nobj:
        #    raise ValueError()
        try:
            temp[col] = data[icol]
        except:
            raise ValueError()

    if not np.all(temp['ROW'] == temp['ROW2']):
        raise ValueError()

    # clean up the columns
    out = Table()
    #out['ROW'] = temp['ROW'].astype(int)
    out['OBJNAME'] = temp['OBJNAME']
    out['OBJNAME_NED'] = temp['OBJNAME_NED']
    out['OBJTYPE'] = temp['OBJTYPE']
    out['OBJECT_NOTE'] = temp['OBJECT_NOTE']
    out['ESSENTIAL_NOTE'] = temp['ESSENTIAL_NOTE']

    # coordinates
    ra = np.zeros(nobj, 'f8') - 99.
    dec = np.zeros(nobj, 'f8') - 99.
    for iobj, (strra, strdec) in enumerate(temp.iterrows('RA', 'DEC')):
        # no coordinates if NED doesn't resolve the name
        if strra != '' and strdec != '':
            cc = SkyCoord(strra, strdec)
            ra[iobj] = cc.ra.value
            dec[iobj] = cc.dec.value
    out['RA'] = ra
    out['DEC'] = dec

    for col in ['Z', 'BASIC_MAG', 'BASIC_DMAJOR', 'BASIC_DMINOR',
                'SDSS_R', 'RC3_B', 'TWOMASS_K',
                'SDSS_DIAM_R', 'SDSS_BA_R', 'SDSS_PA_R',
                'TWOMASS_DIAM_K', 'TWOMASS_BA_K', 'TWOMASS_PA_K',
                'RC3_DIAM_B', 'RC3_BA_B', 'RC3_PA_B',
                'ESO_DIAM_B', 'ESO_BA_B', 'ESO_PA_B']:
        val = np.zeros(nobj, 'f4') - 99.
        for iobj, strval in enumerate(temp[col]):
            if strval != '':
                try:
                    vval = re.sub('[^0-9,.]', '', strval)
                    if vval != '': # e.g., [MLF2006]J072039.75+710950.8 has 'R' and PGC021687 has 'B'
                        # special case for PGC019437 and possibly others
                        if vval[-1] == '.':
                            vval = vval[:-1]
                        val[iobj] = np.float32(vval)
                except:
                    raise ValueError()
                if 'DIAM' in col:
                    val[iobj] /= 60. # [arcsec --> arcmin]
        out[col] = val
    #temp[temp.colnames[14:20]]
    #out[out.colnames[14:20]]

    return out


def build_url(objnames, linebreak='%0D%0A'):

    from urllib.parse import quote # make the galaxy name http-safe

    uplist = '+' + ''.join([f'{quote(obj)}{linebreak}' for obj in objnames])

    url = f'https://ned.ipac.caltech.edu/cgi-bin/gmd?uplist={uplist}&delimiter=bar' + \
        '&NO_LINKS=1&nondb=row_count&nondb=user_name_msg&nondb=user_objname' + \
        '&crosid=objname&position=ra%2Cdec&enotes=objnote&position=pretype' + \
        '&position=z&gadata=magnit&gadata=sizemaj&gadata=sizemin&gadata=morphol' + \
        '&attdat_CON=M&gphotoms_CON=5861&gphotoms_CON=22&gphotoms_CON=1495' + \
        '&diamdat_CON=117&diamdat_CON=2&diamdat_CON=32&diamdat_CON=15&attdat=attned' + \
        '&gphotoms=q_value&diamdat=ned_maj_dia&diamdat=ned_min_dia&diamdat=ned_pa'

    return url


def main():
    """Main wrapper.

    """
    import argparse
    from urllib.request import urlretrieve
    from urllib.error import HTTPError

    from SGA.io import sga_dir, get_raslice
    from SGA.util import get_basic_geometry, match

    catalogs = ['HyperLeda', 'HyperLeda-altname', 'NEDLVS', 'LVD']

    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--catalog', default='HyperLeda', choices=catalogs, type=str, help='External catalog to read.')
    parser.add_argument('--query-data', action='store_true', help='Query for basic data.')
    parser.add_argument('--gather-data', action='store_true', help='Gather the query-data output.')
    parser.add_argument('--merge', action='store_true', help='Merge all the results.')
    parser.add_argument('--overwrite', action='store_true', help='Overwrite any existing output files.')
    args = parser.parse_args()

    if args.merge:
        from SGA.io import (read_hyperleda, read_nedlvs, read_lvd, version_hyperleda, nedfriendly_hyperleda,
                            altnames_hyperleda, version_lvd, version_nedlvs, nedfriendly_lvd)

        def readit(catalog, version):
            datafile = os.path.join(sga_dir(), 'parent', 'external', f'NED-{catalog}_{version}.fits')
            data = Table(fitsio.read(datafile))
            print(f'Read {len(data):,d} objects from {datafile}')
            return data

        hyper = read_hyperleda()
        nedlvs = read_nedlvs()
        lvd = read_lvd()

        ned_lvd = readit('LVD', version_lvd())
        ned_nedlvs = readit('NEDLVS', version_nedlvs())
        ned_hyper = readit('HyperLeda', version_hyperleda())
        ned_hyperalt = readit('HyperLeda-altname', f'{version_hyperleda()}-altname')
        #assert(np.all(nedlvs['OBJNAME'] == nedlvs['OBJNAME_NED']))
        #assert(len(nedlvs) == len(np.unique(nedlvs['OBJNAME_NED'])))

        # [1] - NED flags the following objects as duplicates and most of them
        # indeed appear to be errors in HyperLeda cross-identification. Remove
        # them here but deal with them later.
        warn = ['WARNING' in objnote for objnote in ned_hyper['OBJECT_NOTE']]
        warnalt = ['WARNING' in objnote for objnote in ned_hyperalt['OBJECT_NOTE']]
        cross = vstack((ned_hyper[warn], ned_hyperalt[warnalt]))
        crossfile = os.path.join(sga_dir(), 'parent', 'external', f'HyperLeda_{version_hyperleda()}-crossID-errors.fits')
        cross.write(crossfile, overwrite=True)
        print(f'Wrote {len(cross):,d} objects to {crossfile}')

        ned_hyper = ned_hyper[~np.isin(ned_hyper['ROW'], cross['ROW'])]
        ned_hyperalt = ned_hyperalt[~np.isin(ned_hyperalt['ROW'], cross['ROW'])]

        # [2] - Match HyperLeda{-altname} to NEDLVS using OBJNAME_NED.
        keys = np.array(ned_nedlvs.colnames)
        keys = keys[~np.isin(keys, ['OBJNAME', 'ROW'])]

        _out1 = join(ned_hyper, ned_nedlvs, keys=keys, table_names=['HYPERLEDA', 'NEDLVS'])
        _out2 = join(ned_hyperalt, ned_nedlvs, keys=keys, table_names=['HYPERLEDA', 'NEDLVS'])
        out1 = vstack((_out1, _out2))

        # WINGSJ125256.27-152110.4 is a duplicate:
        #  As the primary object, it's PGC4777821 - http://atlas.obs-hp.fr/hyperleda/ledacat.cgi?o=WINGS%20J125256.27-152110.4
        #  But as the alternate object, it's also [CZ2003]1631C-0295:095=PGC6729485 - http://atlas.obs-hp.fr/hyperleda/ledacat.cgi?o=%5BCZ2003%5D1631C-0295%3A095
        # Here, we drop the second object.
        _, uindx = np.unique(out1['OBJNAME_HYPERLEDA'], return_index=True)
        out1 = out1[uindx]
        indx_out, indx_hyper = match(out1['ROW_HYPERLEDA'], hyper['ROW'])
        out1['OBJNAME_HYPERLEDA'][indx_out] = hyper['OBJNAME'][indx_hyper]
        out1 = out1[np.argsort(out1['ROW_HYPERLEDA'])]

        # [2] - Try to match more of the HyperLeda sample to NEDLVS using
        # coordinates. Ignore the crossID errors from above because
        # many/most/all of those are real and we want to pick them up.
        miss_hyper = hyper[~np.isin(hyper['ROW'], out1['ROW_HYPERLEDA'])]
        miss_nedlvs = ned_nedlvs[~np.isin(ned_nedlvs['ROW'], out1['ROW_NEDLVS'])]

        c_hyper = SkyCoord(ra=miss_hyper['RA']*u.deg, dec=miss_hyper['DEC']*u.deg)
        c_nedlvs = SkyCoord(ra=miss_nedlvs['RA']*u.deg, dec=miss_nedlvs['DEC']*u.deg)
        rad = 1.5 * u.arcsec
        indx_nedlvs, indx_hyper, d2d, _ = c_hyper.search_around_sky(c_nedlvs, rad)

        out2 = hstack((miss_hyper[indx_hyper]['PGC','OBJNAME','ALTNAMES','ROW'],
                       miss_nedlvs[indx_nedlvs]['OBJNAME','OBJNAME_NED','RA','DEC','ROW']),
                      table_names=['HYPERLEDA','NEDLVS'])
        out2 = out2[np.argsort(out2['ROW_HYPERLEDA'])]

        # [3] At this point we have ~290k NEDLVS and 3.3M HyperLeda objects
        # with no match to within 1.5 arcsec.
        miss_hyper = hyper[~np.isin(hyper['ROW'], np.hstack((out1['ROW_HYPERLEDA'], out2['ROW_HYPERLEDA'])))]
        miss_nedlvs = ned_nedlvs[~np.isin(ned_nedlvs['ROW'], np.hstack((out1['ROW_NEDLVS'], out2['ROW_NEDLVS'])))]

        c_hyper = SkyCoord(ra=miss_hyper['RA']*u.deg, dec=miss_hyper['DEC']*u.deg)
        c_nedlvs = SkyCoord(ra=miss_nedlvs['RA']*u.deg, dec=miss_nedlvs['DEC']*u.deg)
        _, sep2d, _ = c_hyper.match_to_catalog_sky(c_nedlvs)

        I = np.where(~np.isnan(miss_hyper['LOGD25']) * (0.1*10**miss_hyper['LOGD25'] > 0.))[0]



        import matplotlib.pyplot as plt
        plt.clf()
        plt.hist(np.log10(sep2d.arcsec), bins=100, log=True);
        plt.savefig('junk.png')


        import pdb ; pdb.set_trace()

        return

    # query stuff below here

    match args.catalog.lower():
        case 'hyperleda':
            from SGA.io import read_hyperleda, version_hyperleda, nedfriendly_hyperleda
            version = version_hyperleda()
            cat = read_hyperleda()#rows=np.arange(500))

        case 'hyperleda-altname':
            # for non-matching sources, use ALTNAME
            from SGA.io import read_hyperleda, version_hyperleda, altnames_hyperleda
            version = version_hyperleda()
            cat = read_hyperleda()

            nedfile = os.path.join(sga_dir(), 'parent', 'external', f'NED-HyperLeda_{version}.fits')
            ned = Table(fitsio.read(nedfile))
            print(f'Read {len(ned):,d} objects from {nedfile}')

            alt = cat[~np.isin(cat['ROW'], ned['ROW'])] # no match
            alt = alt[alt['ALTNAMES'] != '']
            altname = altnames_hyperleda(alt)

            alt.rename_column('OBJNAME', 'OBJNAME_ORIG')
            alt['OBJNAME'] = altname

            # 2489 duplicates!!
            _, uindx = np.unique(alt['OBJNAME'], return_index=True)
            alt = alt[uindx]
            print(f'Trimmed to {len(alt):,d} objects with unique altnames')

            cat = alt
            version = f'{version_hyperleda()}-altname'

        case 'nedlvs':
            from SGA.io import read_nedlvs, version_nedlvs
            version = version_nedlvs()
            cat = read_nedlvs()#rows=np.arange(1000))

        case 'lvd':
            from SGA.io import read_lvd, version_lvd, nedfriendly_lvd
            version = version_lvd()
            cat = read_lvd()

        case _:
            raise NotImplementedError
    ngal = len(cat)

    workdir = os.path.join(sga_dir(), 'parent', 'external', f'nedquery-{args.catalog.lower()}')

    if args.query_data:
        # https://ned.ipac.caltech.edu/forms/gmd.html
        nperquery = 250 # number of galaxies per query

        logdir = os.path.join(workdir, 'logs')
        if not os.path.isdir(logdir):
            os.makedirs(logdir)

        # sort into RA slices
        raslices = np.array([get_raslice(ra) for ra in cat['RA'].value])
        uraslices = np.unique(raslices)
        print(f'Dividing the sample into {len(uraslices)} unique RA slices.')

        for uraslice in uraslices:#[181:]:
            racat = cat[uraslice == raslices]
            nobj = len(racat)
            nquery = int(np.ceil(nobj / nperquery)) # number of queries
            print(f'RA slice {uraslice} will require {nquery:,d} queries of {nperquery} objects per query.')

            querydir = os.path.join(workdir, uraslice)
            if not os.path.isdir(querydir):
                os.makedirs(querydir)

            logfile = os.path.join(logdir, f'raslice{uraslice}.log')
            if os.path.isfile(logfile) and args.overwrite:
                os.remove(logfile)

            for iquery in range(nquery):
                queryfile = os.path.join(querydir, f'raslice{uraslice}-query{iquery:05d}.txt')

                ss = iquery * nperquery
                ee = (iquery + 1) * nperquery
                if ee > ngal:
                    ee = ngal

                racat1 = racat[ss:ee]

                if not os.path.isfile(queryfile) or args.overwrite:
                    if iquery % 1 == 0:
                        print(f'Executing query {iquery:05d}/{nquery-1:05d}')

                    objnames = racat1['OBJNAME'].value
                    # NED-friendly names!
                    if args.catalog.lower() == 'hyperleda':
                        objnames = nedfriendly_hyperleda(objnames, racat1['PGC'].value)
                    if args.catalog.lower() == 'lvd':
                        objnames = nedfriendly_lvd(objnames)

                    url = build_url(objnames)
                    try:
                        urlretrieve(url, queryfile)
                        # update the log
                        with open(logfile, 'a') as L:
                            L.write(f'{url}\n')
                    except HTTPError as err:
                        # http request was too long for the NED server, so split
                        # the request in two
                        if err.code == 414:
                            print(f'Too many objects in query {queryfile}; splitting in two.')
                            for suffix, _objnames in zip(['a', 'b'], np.array_split(objnames, 2)):
                                _queryfile = queryfile.replace('.txt', f'{suffix}.txt')
                                url = build_url(_objnames)
                                urlretrieve(url, _queryfile)
                                with open(logfile, 'a') as L:
                                    L.write(f'{url}\n')
                        else:
                            print(url)
                            raise HTTPError(f'Problem with {queryfile}')
                    #time.sleep(0.2)
                else:
                    print(f'Skipping existing query {iquery:05d}/{nquery-1:05d}')


    if args.gather_data:
        allfiles = sorted(glob(os.path.join(workdir, '???', 'raslice???-query*.txt')))
        #allfiles = glob(os.path.join(workdir, '176', 'raslice176-query*.txt'))

        print(f'Merging {len(allfiles):,d} query files.')

        data = []
        for onefile in allfiles:
            data1 = readone(onefile)
            data.append(data1)
        data = vstack(data)

        #np.sum(np.logical_or.reduce((data['RC3_DIAM_B'] > 0., data['TWOMASS_DIAM_K'] > 0., data['SDSS_PA_R'] > 0.)))
        #np.sum(np.logical_or.reduce((data['RC3_PA_B'] > 0., data['TWOMASS_PA_K'] > 0., data['SDSS_PA_R'] > 0.)))

        if args.catalog.lower() == 'lvd':
            from SGA.io import nedfriendly_lvd

            keep = data['OBJECT_NOTE'] != 'ERROR: Input name not recognized by NED' # = Bootes V
            data = data[keep]

            miss = cat[~np.isin(nedfriendly_lvd(cat['OBJNAME'].value), data['OBJNAME'])]

            indx_cat, indx_data = match(nedfriendly_lvd(cat['OBJNAME']), data['OBJNAME'])
            cat = cat[indx_cat]
            data = data[indx_data]

            assert(np.all(nedfriendly_lvd(cat['OBJNAME']) == data['OBJNAME']))
            data['ROW'] = cat['ROW']
            data = data[np.argsort(data['ROW'])]


        if args.catalog.lower() == 'nedlvs':
            assert(len(data) == len(np.unique(data['OBJNAME'])))
            indx_cat, indx_data = match(cat['OBJNAME'], data['OBJNAME'])
            srt = np.argsort(indx_cat)
            data = data[indx_data[srt]]
            assert(np.all(cat['OBJNAME'] == data['OBJNAME']))
            data['ROW'] = cat['ROW']

            # 1139 objects have NED 'resolve' issues. Throw those out here.
            diff = data[data['OBJNAME'] != data['OBJNAME_NED']]['OBJNAME', 'OBJNAME_NED', 'OBJECT_NOTE', 'Z']
            diff = join(diff, cat['OBJNAME', 'RA', 'DEC', 'Z'], keys='OBJNAME', table_names=['QUERY', 'LVS'])
            #diff[diff['OBJECT_NOTE'] != '']
            diff.write('NEDLVS-resolve-issues.fits', overwrite=True)

            I = np.where(data['OBJNAME'] == data['OBJNAME_NED'])[0]
            print(f'Keeping {len(I):,d}/{len(data):,d} objects where OBJNAME==OBJNAME_NED')
            data = data[I]


        if args.catalog.lower() == 'hyperleda':
            from SGA.io import nedfriendly_hyperleda

            # Any objects with OBJECT_NOTE "ERROR: Input name not recognized by NED"
            # or which are not in 'data', were not resolved by NED.
            keep = data['OBJECT_NOTE'] != 'ERROR: Input name not recognized by NED'
            print(f'Keeping {np.sum(keep):,d}/{len(data):,d} objects resolved by NED.')
            out = data[keep]

            # Remove three duplicates that come from recasting the PGC names in
            # nedfriendly_hyperleda.
            #gg, cc = np.unique(out['OBJNAME'], return_counts=True)
            #print(out[np.isin(out['OBJNAME'], gg[cc>1])])
            _, uindx = np.unique(out['OBJNAME'], return_index=True)
            out = out[uindx]

            # There are 2747 OBJNAME_NED duplicates that will need to be
            # resolved by-hand later! In particular, those with a "'WARNING:
            # Same object as in entry...'" warning can be identified with
            # OBJNAME_NED=='', and there are 1601 of those.
            # gg, cc =np.unique(out['OBJNAME_NED'], return_counts=True)
            # print(gg[cc>1])

            # Now, in order to match back to the original catalog we need the
            # PGC number, which we don't have for 'out'! But making the
            # NED-friendly names leads to 4 duplicates...
            objnames_ned = nedfriendly_hyperleda(cat['OBJNAME'], cat['PGC'])
            #gg, cc = np.unique(objnames_ned, return_counts=True)
            #cat[np.isin(objnames_ned, gg[cc>1])]['PGC', 'OBJNAME', 'ALTNAMES', 'RA', 'DEC']

            out['PGC'] = np.zeros(len(out), dtype=cat['PGC'].dtype)
            out['ROW'] = np.zeros(len(out), dtype=cat['ROW'].dtype)

            I = cat['PGC'] <= 73197
            indx1_cat, indx1_out = match(objnames_ned[I], out['OBJNAME'])
            out['PGC'][indx1_out] = cat[I][indx1_cat]['PGC']
            out['ROW'][indx1_out] = cat[I][indx1_cat]['ROW']

            indx2_cat, indx2_out = match(objnames_ned[~I], out['OBJNAME'])
            out['PGC'][indx2_out] = cat[~I][indx2_cat]['PGC']
            out['ROW'][indx2_out] = cat[~I][indx2_cat]['ROW']
            assert(len(out) == len(np.unique(out['ROW'])))

            out = out[np.argsort(out['ROW'])]
            data = out


        if args.catalog.lower() == 'hyperleda-altname':
            from SGA.io import altnames_hyperleda

            keep = ( (data['OBJECT_NOTE'] != 'ERROR: Input name not recognized by NED') *
                     (data['OBJECT_NOTE'] != 'ERROR: Input name (32) too long - max 30 characters') )
            print(f'Keeping {np.sum(keep):,d}/{len(data):,d} objects resolved by NED.')
            out = data[keep]

            altname = altnames_hyperleda(cat)

            out['PGC'] = np.zeros(len(out), dtype=cat['PGC'].dtype)
            out['ROW'] = np.zeros(len(out), dtype=cat['ROW'].dtype)

            indx_cat, indx_out = match(altname, out['OBJNAME'])
            out['PGC'][indx_out] = cat[indx_cat]['PGC']
            out['ROW'][indx_out] = cat[indx_cat]['ROW']
            assert(len(out) == len(np.unique(out['ROW'])))

            out = out[np.argsort(out['ROW'])]
            data = out


        outfile = os.path.join(sga_dir(), 'parent', 'external', f'NED-{args.catalog}_{version}.fits')
        print(f'Writing {len(data):,d} objects to {outfile}')
        data.write(outfile, overwrite=True)


if __name__ == '__main__':
    main()
